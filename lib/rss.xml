<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Congzhi's Notes Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://congzhi.wiki/</link><image><url>https://congzhi.wiki/lib/media/favicon.png</url><title>Congzhi's Notes Vault</title><link>https://congzhi.wiki/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 20 Aug 2025 11:05:48 GMT</lastBuildDate><atom:link href="https://congzhi.wiki/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 20 Aug 2025 11:04:24 GMT</pubDate><copyright><![CDATA[Congzhi]]></copyright><ttl>60</ttl><dc:creator>Congzhi</dc:creator><item><title><![CDATA[3. Building System and Makefile]]></title><description><![CDATA[ 
 <br>Source: <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=DtGrdB8wQ_8&amp;t=14s" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=DtGrdB8wQ_8&amp;t=14s" target="_blank">Makefiles: 95% of what you need to know</a><br>what is a building system do?<br>
<br>自动地进行文件的编译和链接，将各个文件变成一个可执行文件。通过构建系统，你可以直观地体会到整个项目是如何编译并链接的，各个文件之间的联系是什么。
<br>应当只完成必要的：也就是说当文件（依赖项）有修改时，重新编译这些修改过的文件并链接。可以省去全部编译所产生的时间。也就是说你不需要重新编译所有的代码，用 shell bash 就需要整个的进行编译。当项目非常大时，你可能需要数个小时进行编译，而用makefile，你只需要编译需要的
<br>同时，build system 应当有一定的规约（一种编程语言，比如用shell script）使得整个构建系统易于上手
<br>一个简单的makefile.   前面的都是变量，直到 all: <br>CC=gcc # complier variable
INCDIRS=-I # 
OPT=-O0 # optimization variable
CFLAGS=-Wall -Wextra -g $(INCDIRS) $(OPT)

CFLAGS=x.c y.c
OBJECTS=x.o y.o

BINARY=bin

all: $(BINARY)

$(BINARY): $(OBJECTS)
		$(CC) -o $@ $^
%.o:%.c # make feature, % is a wildcard (repersenting everything)
		$(CC) $(CFLAGS) -c -o $@ $^
clean:
		rm -rf $(BINARY) $(OBJECTS)
<br>build rule: all: $(BINARY) 就是一个build rule（这里的target和dependencies有何联系？）<br>
all directive, 当你用make all就会转到makefile中的 （默认情况下只用 make 会转到第一个 rule）<br>上面的makefile的意思就是说当你在终端 make all 的时候，你就会转到 $(BINARY): $(OBJECTS)。但是BINARY depends on OBJECTS，也就是你要构建bin你就需要先得到所有的 .o 文件。如果没有，就会执行 %.o:%.c (.o 文件 依赖 .c 文件) @ 表示 LHS (.o) ^表示RHS (.c)<br>target: dependencies 如果所有依赖项都满足就执行 command
		command
<br>即使你有 x.o 如果你修改了源文件，make 一个新的 x.o 将会被编译(build system的第二个事情，只做需要的事情)<br>需要先介绍不用variable会怎么样。<br>用variable之后makefile才会做需要做的事情吗？<br>如果头文件修改，makefile不会察觉到有什么修改了（因为.h没有依赖项<br>第三个例子： featureful makefile---我们将头文件都放在include文件夹中，将库放到lib文件夹中。我们将include作为依赖项，当我们修改.h文件时，makefile就能够察觉到有东西被修改了<br>BINARY=bin
CODEDIR=. lib
INCDIRS=. ./include/ # can be listed

CC=gcc
OPT=-O0
# generate files that encode make rules for the .h dependencies
DEPFLAGS=-MP -MD # let make work with compiler?是这样么
CFLAGS=-Wall -Wextra -g $(foreach D, $(INCDIRS),-I$(D) $(OPT) $(DEPFLAGS))

CFILES=$(foreach D,$(CODEDIRS),$(wildcard $(D)/*.c))

OBJECTS=$(patsubst %.c,%.o,%(CFILES))
DEPFILES=$(patsubst %.c,%.d,%(CFILES))

all: $(BINARY)

$(BINARY): $(OBJECT)
		$(CC) -o $@ $^

%.o:%.c # make feature, % is a wildcard (repersenting everything)
		$(CC) $(CFLAGS) -c -o $@ $&lt; # only want .c dependency here thus $&lt;
clean:
		rm -rf $(BINARY) $(OBJECTS) $(DEPFILES)

distribute: clean
		tar zcvf dist.tgz *

diff
# include the dependency
-include $(DEPFILES)
<br>依赖项 (dependencies)<br># GCC flags first
target [targets...]: [components...]
	[ command 1]
	# ...
	[ command n]
<br>special commands<br>-
@
+
<br>macros and variables<br>MACRO1 = 12
COMPILE = gcc *.c

gcc:
	$(COMPILE)
<br>multi line commands<br>
在makefile中，每行命令都默认允许在一个独立的shell终端中，也就是说在 Badlisting 中<br>
如果需要多个命令共享同一个上下文（例如切换目录后运行操作），需要将它们合并到一个 Shell 会话中，通常使用反斜杠 \ 或 &amp;&amp;<br>
分号用于在同一行中分隔多个命令。例如：<br>Badlisting:
	cd dir
	ls
Goodlisting:
	cd dir;\ # same as cd dir &amp;&amp; ls
	ls
<br>makefile 也可以用于 testing<br><br>makefile的好处就是只编译修改了的.c/.cpp文件并生成修改后的.o文件。节省编译时间。一个标准的makefile可能是这样的：<br>CC = gcc
INCDIRS = -I.
OPT = -O0
CFLAGS = -Wall -Wextra -g $(INCDIRS) $(OPT)

CFILES = 
OBJECTS = 
BINARY = 

all: $(BINARY)

clean:


random:
	date
	sl
	mkdir useless
	cd useless ;\
	cd..
<br>all: # all 是干嘛的？

help: #这些是按照顺序执行的么

<br>CC = GCC
all:
	$(CC) file.c -o proj
debug:
debug: CC += -g -DDEBUG
<br><br>在构建系列的<br>头文件：存放声明和内联化的函数/变量（constexpr/consteval...）declarations<br>因为内联后的符号没有定义<br>These files contain declarations and inline functions/variables. For example, constexpr and consteval declarations are often found in header files. Declarations inform the compiler about the existence and type of functions or variables without providing the full implementation.<br>Source Files (源文件): These files contain definitions. A definition is a specific type of declaration that provides complete information about an entity, including its implementation.<br>foo.h<br>int foo(int n); //A declaration, no function body.
extern int e; // A non-defining declaration.
<br>foo.cpp<br>#include "foo.h"

int foo(int n) // A definition, we have the function body after this.
{
    // Function implementation goes here.
}
int a; // Definition without initialization
int b = 10; // Definition with initialization
static int c = 10; // A file-scope static definition
extern int d = 10;
<br>other.cpp<br>int e = 10; // 
<br>源文件中存放definitions， definition is a specific type of declaration. Definition gives a set of informations about the entity but no verse visa<br>ProjectName/
├── include/              # Header file
│   ├── main.h
│   └── utils.h
├── src/                  # Source file
│   ├── main.cpp
│   └── utils.cpp
├── lib/                  # Libraries
│   └── mylib.a
├── build/                # Object files
│   └── (object files, executables)
├── tests/                # Testing code
│   └── test_main.cpp
├── CMakeLists.txt        # CMake build file
├── Makefile              # Makefile build file
└── README.md             # Intro to the project

<br>这节课用建立 shell 文件构建代码<br>gcc ....
gcc ....

gcc x.o y.o -o bin
<br>用上面的方法的劣势就是每次构建都需要整个的编译，而不是选择性编译<br>下节课用 makefile]]></description><link>https://congzhi.wiki/building-and-version-control/3.-building-system-and-makefile.html</link><guid isPermaLink="false">Building and Version Control/3. Building System and Makefile.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:57:51 GMT</pubDate></item><item><title><![CDATA[4. Release Build and Debug Build]]></title><description><![CDATA[ 
 <br>release build and debug build<br>#ifdef DEBUG
printf("Debugging info...\n");
#endif

#ifdef NDEBUG
printf("This version is for releasing\n");
#endif
]]></description><link>https://congzhi.wiki/building-and-version-control/4.-release-build-and-debug-build.html</link><guid isPermaLink="false">Building and Version Control/4. Release Build and Debug Build.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 29 Mar 2025 15:12:43 GMT</pubDate></item><item><title><![CDATA[5. CMake]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/building-and-version-control/5.-cmake.html</link><guid isPermaLink="false">Building and Version Control/5. CMake.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 15 Mar 2025 09:27:52 GMT</pubDate></item><item><title><![CDATA[Building and Version Control]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br>一当 <a data-href="Building and Version Control" href="https://congzhi.wiki/building-and-version-control/building-and-version-control.html" class="internal-link" target="_self" rel="noopener nofollow">Building and Version Control</a> 系列完成， <a data-href="进程的一生——从出生到死亡 (Abandoned)" href="https://congzhi.wiki/some-notes/进程的一生——从出生到死亡-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">进程的一生——从出生到死亡 (Abandoned)</a> 将被彻底废除。其所有关于进程如何出生的内容将会在本系列中重制，其他内容请参阅操作系统系列。<br>在这个系列中，我会系统地探讨探讨 C/C++ 的代码构建方面的知识。我们会聊到一段文本（高级语言）是如何变成二进制的 01序列（机器语言）在 CPU 上运行的。同时，我们牵扯到一些 Git 版本控制的简单知识。<br>系列目录如下：<br>Building and Version Control

<br><a data-href="1. Compilation" href="https://congzhi.wiki/building-and-version-control/1.-compilation.html" class="internal-link" target="_self" rel="noopener nofollow">1. Compilation</a>
<br><a data-href="2. Linking" href="https://congzhi.wiki/building-and-version-control/2.-linking.html" class="internal-link" target="_self" rel="noopener nofollow">2. Linking</a>
<br><a data-href="3. Building System and Makefile" href="https://congzhi.wiki/building-and-version-control/3.-building-system-and-makefile.html" class="internal-link" target="_self" rel="noopener nofollow">3. Building System and Makefile</a>
<br><a data-href="5. CMake" href="https://congzhi.wiki/building-and-version-control/5.-cmake.html" class="internal-link" target="_self" rel="noopener nofollow">5. CMake</a>
<br><a data-href="4. Release Build and Debug Build" href="https://congzhi.wiki/building-and-version-control/4.-release-build-and-debug-build.html" class="internal-link" target="_self" rel="noopener nofollow">4. Release Build and Debug Build</a>
<br><a data-href="Git and Github - Part I" href="https://congzhi.wiki/building-and-version-control/git-and-github-part-i.html" class="internal-link" target="_self" rel="noopener nofollow">Git and Github - Part I</a>
<br><a data-href="Git and Github - Part II" href="https://congzhi.wiki/building-and-version-control/git-and-github-part-ii.html" class="internal-link" target="_self" rel="noopener nofollow">Git and Github - Part II</a>

<br><br><br>任何问题都可以通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/building-and-version-control/building-and-version-control.html</link><guid isPermaLink="false">Building and Version Control/Building and Version Control.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:26:36 GMT</pubDate></item><item><title><![CDATA[Git and Github - Part I]]></title><description><![CDATA[ 
 <br><br><br>为什么每次在 git push 之前必须先 git pull 一下？<br>git status
git add .
git commit -m "commit"
git pull origin main
git push origin main
<br>这是最早困惑我的问题。但回过头来想一想并不难理解为什么要这么做。因为 git pull 操作相当于 git fetch 和 git merge 两步操作的合并。如下：<br>git pull origin main
# is the same as:
git fetch origin main
git merge
<br>因为 Git 是一个版本控制系统，所以在 git push 之前需要先拉取远程更新并解决冲突。什么意思呢？就是说在你 git pull 一个新版本时，需要先检查当前版本中的文件是否有修改，确定需要保留什么、更新什么，解决冲突后再提交修改，最后通过 git push 上传到远程仓库。<br>从云端拉取当前版本信息的操作就是 git fetch 这时候会生成一个 .lock 文件，以免别人对版本做出修改（也就是 Git 操作的原子性）。之后你用 git merge 修改需要修改的，保留需要保留的。然后生成一个新版本并上传云端，即 git push 。<br><br><br><br>Git 是一种开源的分布式版本控制系统。虽然说我们有必要学习一下什么是版本控制系统，但是版本控制在我们生活中太常见了，到处都是。通过版本控制，你就能很方便地回溯到以前的某个版本。<br>版本控制系统分为两种，集中式版本控制系统（比如 SVN）和分布式版本控制系统（如 Git）。集中式版本控制系统的所有版本历史都存储在一个中央服务器上。而分布式版本控制系统中，每个开发者在其存储库中都包含项目的完整版本历史。Git 就是一个开源免费的分布式版本控制系统。<br>Git 的特点就是并不依赖一个中央服务器，而且有一个远程仓库用于共享代码。<br><br>要使用版本控制系统，你需要不停的创建新的版本。下面，我们来了解一下如何创建一个版本。<br><br>要创建版本，你需要先初始化一个 Git repo 。当你运行 git init 后，你的当前目录就会生成一个隐藏的 .git 文件夹。这个文件夹存储着所有版本控制所需要的元数据（如提交历史、分支、标签和配置等），通过这些元数据，Git 就能跟踪当前 repo 都做了哪些改动，实现版本控制。<br>初始化 Git repo 后，当前目录就会称为工作区(working directory)，你可以在当前的工作区中修改文件。然后通过 git add 和 git commit 对修改的文件进行提交，纳入版本控制。<br>如果重复运行 git init 就会彻底地重置 repo 。<br>在最开始的时候我创建了一个 LearnGit 的文件夹，并创建了一个 file1.cpp 文件。<br>du@DVM:~/LearnGit$ ls
file1.cpp
<br>使用 git init ，一个 repo 就初始化好了。<br>du@DVM:~/LearnGit$ git init
hint: Using 'master' as the name for the initial branch. This default branch name
hint: is subject to change. To configure the initial branch name to use in all
hint: of your new repositories, which will suppress this warning, call:
hint: 
hint:   git config --global init.defaultBranch &lt;name&gt;
hint: 
hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
hint: 'development'. The just-created branch can be renamed via this command:
hint: 
hint:   git branch -m &lt;name&gt;
Initialized empty Git repository in /home/du/LearnGit/.git/
<br>加入 -A 参数查看当前文件夹下的内容，我们确实得到了一个 .git 文件，现在 Git 会跟踪你对对当前 repo 做出的所有修改。<br>du@DVM:~/LearnGit$ ls -A
file1.cpp  .git
<br><br>在版本控制系统中，每一次提交都会记录作者的信息，而且在团队开发中，明确每个人的更改记录能够有助于问题出现时能够快速解决造成问题的人。所以在提交版本前，你需要先配置你的邮箱和姓名。<br>你可以加上 --global 配置全局的个人信息。（不需要初始化 repo）<br>git config --global user.name "Your Name"
git config --global user.email "email@example.com"

git config --global --list
<br>也可以仅仅配置当前 repo 的个人信息，这样在该 repo 中的提交会使用此专用信息。（需要先初始化 repo ）<br>git config --local user.name "Your Name"
git config --local user.email "email@example.com"
git config --local --list 
<br><br>使用 git status 可以显示自上次 commit 后当前 repo 中所有的修改。它提示我们这里的 file1.cpp 文件还尚未暂存(untracked files)。<br>du@DVM:~/LearnGit$ git status
On branch master

No commits yet

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        file1.cpp

nothing added to commit but untracked files present (use "git add" to track)
<br><br>git add 的作用是将哪些文件暂存到暂存区。我们使用 git add file1.cpp 将文件 file1.cpp 暂存到暂存区里面。我们再创建一个 file2.cpp，查看一下 status ：<br># git add . 就是把当前文件夹中的所有文件都加入到暂存区里面，这个时候工作区只有 file1.cpp。相当于 git add file1.cpp
du@DVM:~/LearnGit$ git add .
du@DVM:~/LearnGit$ git status
On branch master

No commits yet

Changes to be committed:
  (use "git rm --cached &lt;file&gt;..." to unstage)
        new file:   file1.cpp

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        file2.cpp
<br>我们可以见得 file1.cpp 已经放到暂存区了，而 file2.cpp 仍然在工作区。我们提到了工作区、暂存区，这两个区到底是什么？<br><br>Git 工作流牵扯到四个区域：工作区、暂存区、版本库和远程仓库。我们先解释一下前三者。我们编辑文件的地方就是工作区，所有的改动最早就发生在这里，但 Git 不会跟踪这些修改。你也在上面看到了 Untreacked files ，这里反应的就是工作区，相当于草稿纸。<br>文件的工作完成后，我们将其加入到暂存区，暂存区是一个检查点，用于选择性地提交文件，相当于待提交清单。到了后面，一旦暂存区提交到版本库中，这些变动就会永久地保存为一个版本快照，即一个全新的版本。版本库就相当于一个存档册。<br><br>相当于 git add 的逆向操作，将文件从暂存区移回工作区。<br><br>修改完成后，一天的工作可能到此结束。我们需要提交暂存区中的文件。我们使用 git commit -m "message" 来带有附加信息地提交已暂存文件。至此，一个版本诞生了！<br>du@DVM:~/LearnGit$ git commit -m "Version1"
[master (root-commit) 1b6d4d0] Version1
 1 file changed, 6 insertions(+)
 create mode 100644 file1.cpp
du@DVM:~/LearnGit$ git status
On branch master
Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        file2.cpp

nothing added to commit but untracked files present (use "git add" to track)
<br>你可以用 git log 查看已提交的版本信息：<br>du@DVM:~/LearnGit$ git log
commit 1b6d4d07b9d608fb7082ce804a8b3b4cbad6d56d (HEAD -&gt; master)
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:06:09 2025 +0800

    Version1
<br>如果提交后发现文件漏了或者是信息填写错误，我们可以使用 --amend 进行补救。假如文件 file2.cpp 本应一并提交的，但忘记添加到暂存区里面了，我们就可以：<br>du@DVM:~/LearnGit$ git add file2.cpp
du@DVM:~/LearnGit$ git commit --amend --no-edit # No change to commit messages
<br>如果你发现原先提交的提交信息写错了，我们仍然使用 --amend 。如：git commit --amend -m "correct messages"。这里我不做修改。<br>现在，我们有一个版本了，我们再来多创建几个版本。<br><br>在第二个版本中，我们对两个文件都进行了修改。加入暂存区、提交。第二个版本也创建好了。<br>第三个版本中，我们添加一个 file3.cpp 文件，然后对所有的文件都进行修改。加入暂存区、提交。第三个版本也创建好了。<br>du@DVM:~/LearnGit$ git log
commit 8971aa2d544055f35807f30e7ca1916717d25fd4 (HEAD -&gt; master)
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:31:18 2025 +0800

    Version3

commit 795279eb96afbd381e598a5b4e880b6341af9d50
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:28:47 2025 +0800

    Version2

commit d4ae88f7c9b6ac9c0e789c3a3a7812ed12e4c7f6
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:06:09 2025 +0800

    Version1
<br>如果这个时候，我们发现版本三有大问题，需要回溯到版本二重新修改提交。既然我们在用的是版本控制系统，回溯版本肯定没有任何问题。<br><br>在我们使用 git log 时，每个 commit 都会对应一个提交哈希。要回溯版本，你只需要复制哈希值并使用 git checkout -- &lt;Hash&gt; 回溯到旧版本。<br>du@DVM:~/LearnGit$ git log --all
commit 8971aa2d544055f35807f30e7ca1916717d25fd4 (master)
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:31:18 2025 +0800

    Version3

commit 795279eb96afbd381e598a5b4e880b6341af9d50 (HEAD)
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:28:47 2025 +0800

    Version2

commit d4ae88f7c9b6ac9c0e789c3a3a7812ed12e4c7f6
Author: YourName &lt;email@example.com&gt;
Date:   Sat Mar 29 22:06:09 2025 +0800

    Version1
<br>这里，你会发现 HEAD 移动到 Version2 commit 后面了，这个 HEAD 是干嘛的？HEAD 是一个指向某个提交版本或指向分支的指针（如果你只有一个分支，一般就用main/master来指代最新的版本，你由此会看到 HEAD -&gt; master）。<br>在 Version2 ，我们添加 file4.cpp 、保存到暂存区然后提交（附加 "new Version3"）。这样，我们就有一个分支了 (branch)。<br>du@DVM:~/LearnGit$ git log --all --graph
* commit 197d5496fb2680446e9a034c759a8ddc32bddb8f (HEAD)
| Author: YourName &lt;email@example.com&gt;
| Date:   Sat Mar 29 22:40:45 2025 +0800
| 
|     new Version3
|   
| * commit 8971aa2d544055f35807f30e7ca1916717d25fd4 (master)
|/  Author: YourName &lt;email@example.com&gt;
|   Date:   Sat Mar 29 22:31:18 2025 +0800
|   
|       Version3
| 
* commit 795279eb96afbd381e598a5b4e880b6341af9d50
| Author: YourName &lt;email@example.com&gt;
| Date:   Sat Mar 29 22:28:47 2025 +0800
| 
|     Version2
| 
* commit d4ae88f7c9b6ac9c0e789c3a3a7812ed12e4c7f6
  Author: YourName &lt;email@example.com&gt;
  Date:   Sat Mar 29 22:06:09 2025 +0800
  
      Version1
<br>在回退到旧版本后，用 git log 并不会显示旧版本之后版本的信息。你需要使用 git log --all 来显示所有版本的信息。<br><br>git log --all --graph<br>如果有多个分支，那么只要使用 git checkout BranchName 就可以了。<br><br>.gitignore&nbsp;是除了 .git 文件外另一个重要的配置文件。.git 用于告知 Git 跟踪哪些文件或目录，而 .gitignore 就是告知 Git 哪些文件或目录应当被忽略。它的核心作用就是让 Git 明确排除那些不需要被跟踪的文件（比如临时文件、日志、编译产物等），避免垃圾污染仓库。<br><br>用 rm -rf .git 永久移除当前 repo 下的 .git。<br><br>git status
git config --global alias.s "status"
git s
]]></description><link>https://congzhi.wiki/building-and-version-control/git-and-github-part-i.html</link><guid isPermaLink="false">Building and Version Control/Git and Github - Part I.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 15 Jun 2025 16:02:54 GMT</pubDate></item><item><title><![CDATA[Git and Github - Part II]]></title><description><![CDATA[ 
 <br><br>在<a data-tooltip-position="top" aria-label="Git and Github - Part I" data-href="Git and Github - Part I" href="https://congzhi.wiki/building-and-version-control/git-and-github-part-i.html" class="internal-link" target="_self" rel="noopener nofollow">上节课</a>中，我们简单的学习了一下 Git 的版本控制，但那一切都是在 local repo 上进行的。本节，我们将会引入远程仓库(remote repo)，便于在不同机器上共享代码，同步更新，让项目具有更好的备份功能。<br>首先，你需要拥有一个 Github 账户。然后在你的 Github 上新建一个 repo，便于管理，你可以起一个和本地 repo 一样的名字。创建好后，Github 一般会提供一些命令供你在本地初始化并推送到远程仓库的命令行命令，你只需要将当前文件路径换成本地仓库的文件路径然后使用 Github 提供的命令创建远程-本地的仓库连接就可以了。<br>congzhi@ ~ git remote add origin https://github.com/UserName/Repo.git
git branch -M main
git push -u origin main
Enumerating objects: 28, done.
Counting objects: 100% (28/28), done.
Delta compression using up to 10 threads
Compressing objects: 100% (23/23), done.
Writing objects: 100% (28/28), 6.70 KiB | 6.70 MiB/s, done.
Total 28 (delta 3), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (3/3), done.
To https://github.com/UserName/Repo.git
 * [new branch]      main -&gt; main
branch 'main' set up to track 'origin/main'.
<br>下面，我们来解释这些指令的含义。<br><br><br>与远程仓库有关的操作我们都用 git remote 来进行。本来我们的远程仓库和本地仓库毫无关联，使用 git remote add，我们就可以在本地添加远程仓库。上面，我们使用：<br>git remote add origin https://github.com/UserName/Repo.git
<br>这里的 origin 指的是远程仓库的默认名称，在后面，我们还看到了一串 URL，这是 Github 提供的 URL。当你运行这行命令后，本地仓库和远程仓库就建立连接了。<br>如果你想查看当前本地仓库相关联的远程仓库，你可以用 git remote 查看当前仓库已添加的远程仓库，如果你想要看更详细的信息，你可以用 git remote --verbose。<br><br>与之相对的，你可以用 git remote remove origin 来取消关联。（origin 是远程仓库的默认名称）<br><br>有了关联的远程仓库，我们就可以将本地仓库内容上传到远程仓库，这一过程称为 push，即推送。同时，我们也可以把远程仓库的内容下载到本地仓库，这一过程称为 pull，即拉取。<br><br>如果本地仓库有相关的改动，我们就可以用 git push origin main 命令来同步远程仓库。我们前面提到过 origin 指的是远程仓库的名称，而 main 是我们当前的分支名。 所以这条命令的作用就是将 main 分支同步推送到远程名叫 origin 的仓库（每次推送一个分支的 commits ）。<br>在推送前，你需要确保提交 (git commit) 所有的修改，因为 git push 只会推送分支的 commits ，而不会推送 changes。<br>你可以使用下面的命令来将本地分支与远程分支相关联，以后， git push 不需要指定 origin 的仓库名和 main 的分支名。<br>git push origin main --set-upstream
git push # same as git push origin main
<br>上节课中，我们学到，如果上次提交有问题，我们可以用 git commit --amend 来补救，然而这样就会产生一个新的提交分支。这时，如果你 git push，那么 Git 就会报错。为了解决，你可以用 git push origin main --force 覆盖掉远程提交。如果是合作项目，你可以先拉取最新提交后在推送。<br><br>当你换了一台新设备，或者作为新人参与到一个团队项目中，你就需要先用 git clone 将远程仓库下载到本地。<br>git clone url a-folder-name
<br><br>如果远程仓库有更新，你可以用 git pull 来获取最新的更新。同样的，Git 需要知道你要从哪个远程仓库获取哪个分支的更新。<br>git pull origin main
<br>git pull origin main --set-upstream
git pull
<br>这条命令相当于：<br>git fetch origin main # git fetch
git merge origin/main
<br>]]></description><link>https://congzhi.wiki/building-and-version-control/git-and-github-part-ii.html</link><guid isPermaLink="false">Building and Version Control/Git and Github - Part II.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 16 Jun 2025 17:32:02 GMT</pubDate></item><item><title><![CDATA[Computer Architecture]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br>Computer architecture has been one of the topics I am most curious about in CS. This series will be updated periodically in the future. My study materials may include the following:<br>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@CMUCompArch" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@CMUCompArch" target="_blank">CMUCompArch</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@IntelTechnology" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@IntelTechnology" target="_blank">Intel Technology</a>
<br><a data-tooltip-position="top" aria-label="https://en.wikichip.org/wiki/WikiChip" rel="noopener nofollow" class="external-link" href="https://en.wikichip.org/wiki/WikiChip" target="_blank">WikiChip - Chips &amp; Simi</a>
<br><a data-tooltip-position="top" aria-label="https://cpu.retromuseum.org/index.html" rel="noopener nofollow" class="external-link" href="https://cpu.retromuseum.org/index.html" target="_blank">Honux 的 CPU 博物馆</a>
<br>Currently, these notes are under this topic:<br>
<br><a data-href="Modern CPU Evolution (ENG)" href="https://congzhi.wiki/computer-architecture/modern-cpu-evolution-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Modern CPU Evolution (ENG)</a>
<br><a data-href="NUMA Systems" href="https://congzhi.wiki/computer-architecture/numa-systems.html" class="internal-link" target="_self" rel="noopener nofollow">NUMA Systems</a>
<br>If you have any questions about the contents, fell free to contact me at <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a>.]]></description><link>https://congzhi.wiki/computer-architecture/computer-architecture.html</link><guid isPermaLink="false">Computer Architecture/Computer Architecture.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 09 Aug 2025 12:50:39 GMT</pubDate></item><item><title><![CDATA[Modern CPU Evolution (ENG)]]></title><description><![CDATA[ 
 <br>Notes on <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=wGSSUSeaLgA" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=wGSSUSeaLgA" target="_blank">Unlocking Modern CPU Power</a>, very interesting talk, highly recommended!<br><br>We got this two CPUs die shot a decade of server processor advancements. We serve here the Intel Xeon E5-2600 v2 from 2013 and Intel Xeon Sapphire Rapids from 2023.<br><br>Back in the day, server CPUs were always monolithic, meaning all cores were build on a single die. In 2013, Intel released the Ivy Bridge-EP Xeon E5-2600 v2 processors for high performance servers. This processor features:<br>
<br>Up to 12 cores (10 cores version here)
<br>30MB of L3 cache (positioned in the center of the cores)
<br>Manufactured on a 22nm process
<br>This image below shows a 10-core variant of this CPU, all cores are integrated on a single die:<br><img alt="IDF13: Intel Announces The Ivy Bridge-EP Xeon E5-2600 V2 Processors For ..." src="https://th.bing.com/th/id/R.670894858090d2dfb61080790606b731?rik=3b5Hls7UPIkAPQ&amp;riu=http%3a%2f%2fcdn.wccftech.com%2fwp-content%2fuploads%2f2013%2f09%2fIvy-Bridge-EP-Die-Shot.jpg&amp;ehk=LCfiS0UCljQfbcggzKvBg8R0UdUSIYizlH2SL5OxcWA%3d&amp;risl=1&amp;pid=ImgRaw&amp;r=0" referrerpolicy="no-referrer"><br><br>In 2023, the process node technology has improved a lot, but the fundamental microarchitecture on a single die hasn't changed significantly. However, CPU architecture itself has transformed--moving away from monolithic designs toward chiplet-based structures.<br>The uArch is changing. We now have four-die (tile) structure, where each die contains 15 cores, totaling 56 cores across the entire processor. The L3 cache is on top of the whole die.<br><img alt="Intel &quot;Sapphire Rapids&quot; Xeon 4-tile MCM Annotated | TechPowerUp" src="https://www.techpowerup.com/img/Blh3tbyMprN5HklS.jpg" referrerpolicy="no-referrer"><br><br>With the advancement in manufacturing processes, transistor density has increased significantly, leading a higher performance while maintaining similar die size(2013 - one die; 2023 multiple die in a package).<br>We now have much larger caches. In 2024, the AMD server (EPYC IV) processor features 1GB of shared cache and 96MB of L2 cache (1MB per core), while 10 years ago, we only had 30MB of L3 cache. (Make-up for slow memory)<br><br>In this section, we'll talk about how your to write better code with a little bit of understanding of modern CPU. First things first, let's dive into the superscalar stuff.<br><br>All modern CPUs are superscalar, meaning multiple independent instructions can be executed within a single clock cycle. For example:<br>for(size_t i =; i &lt; N; ++i) {
	x = a[i] + b[i];
	y = a[i] - b[i];
	a[i] += 10;
	b[i] &lt;&lt;= 3;
}
<br>This loop contains multiple operations that modern CPUs can execute in parallel within the backend of a CPU. A typical CPU features multiple types of ALUs, allowing diverse computations to occur simultaneously. Provided there are no dependencies between these instructions, the CPU can dispatch different instructions to various ALUs (execution units), optimizing execution efficiency.<br>If a task is memory-bound, meaning its performance is primarily limited by memory access speed rather than CPU computation, then superscalar execution has minimal impact.<br>Luckily, CPU caches have our back!<br><br>Typically, modern CPU have three layers of cache within the processor: L1, L2 and L3. These caches help reduce memory latency and increase data access speed. They provide a prefetching mechanisms, allowing the CPU to predict and load data ahead of time, especially when memory access is sequential. If memory access is random, the prefetch prediction could be defeated. (Vectors are the best most of the times)<br><br>Inside a CPU, we have four stages to execute an instruction——fetch, decode, execute, and write back. Each of these stages have several sub-stages that enable CPU pipelining, allowing multiple instructions to be processed in different execution sub-stages simultaneously.<br>But when the CPU encounters a branch instruction, it must determine which part of the program to jump to, or the pipeline will stall. Modern CPUs have branch predictors and speculative execution to prevent pipeline stalls, and normally, you get fairly accurate predictions, making the pipeline run smoothly.<br>But if the branch prediction is wrong, the CPU then must flush the pipeline, discarding incorrectly fetched instructions.This results in a performance penalty, also called a misbranching penalty. The longer the pipeline is, the greater the penalty will be.<br>To avoid this penalty, we need to write code that makes branch predictable. Or if you cannot ensure which branch would be taken, then you should make every possible code path is branchless. (Avoiding condition instruction, condition data is fine)<br>Branchless code often executes more instructions than branch-based code because it always computes both possible outcomes, regardless of whether they are needed. However, this isn't necessarily a drawback in modern CPUs (no misbranching penalty + superscalar execution). ]]></description><link>https://congzhi.wiki/computer-architecture/modern-cpu-evolution-(eng).html</link><guid isPermaLink="false">Computer Architecture/Modern CPU Evolution (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 02 Jun 2025 00:57:14 GMT</pubDate><enclosure url="https://th.bing.com/th/id/R.670894858090d2dfb61080790606b731?rik=3b5Hls7UPIkAPQ&amp;riu=http%3a%2f%2fcdn.wccftech.com%2fwp-content%2fuploads%2f2013%2f09%2fIvy-Bridge-EP-Die-Shot.jpg&amp;ehk=LCfiS0UCljQfbcggzKvBg8R0UdUSIYizlH2SL5OxcWA%3d&amp;risl=1&amp;pid=ImgRaw&amp;r=0" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://th.bing.com/th/id/R.670894858090d2dfb61080790606b731?rik=3b5Hls7UPIkAPQ&amp;riu=http%3a%2f%2fcdn.wccftech.com%2fwp-content%2fuploads%2f2013%2f09%2fIvy-Bridge-EP-Die-Shot.jpg&amp;ehk=LCfiS0UCljQfbcggzKvBg8R0UdUSIYizlH2SL5OxcWA%3d&amp;risl=1&amp;pid=ImgRaw&amp;r=0"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[NUMA Systems]]></title><description><![CDATA[ 
 <br>Explore for more: <br>
<br><a data-href="Modern CPU Evolution (ENG)" href="https://congzhi.wiki/computer-architecture/modern-cpu-evolution-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Modern CPU Evolution (ENG)</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=f0ZKBusa4CI&amp;t=6s" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=f0ZKBusa4CI&amp;t=6s" target="_blank">NUMA - Fedor Pikus CppNow</a>
<br><a data-tooltip-position="top" aria-label="https://frankdenneman.nl/category/numa/page/3/" rel="noopener nofollow" class="external-link" href="https://frankdenneman.nl/category/numa/page/3/" target="_blank">Frank Denneman's category about NUMA</a>
<br><br>在过去，所有的处理器都通过统一的总线去访问统一共享的内存和 I/O 资源，这种系统架构被称为基于总线的系统。在这个时期的主板上，除了 CPU 之外，还有南桥芯片和北桥芯片，其中北桥芯片负责内存控制、南桥芯片负责 I/O 控制。处理器会通过前端总线于北桥通信，从而访问内存。<br>在这种架构下，所有处理器核心访问内存的路径和速度是完全一致的，因此被称为统一内存架构（Uniform Memory Architecture, UMA）。采用 UMA 架构的处理器系统也被称为对称多处理器（Symmetric Multi-Processor, SMP）系统。<br>
<img alt="02-02-UMA 架构" src="http://frankdenneman.nl/wp-content/uploads/2016/07/02-02-UMA-Architecture-750x560.png" referrerpolicy="no-referrer"><br>
随着系统性能的提升和核心数量的增加，采用 UMA 架构可能会带来一些问题：<br>
<br>当 CPU 数量增加时，所有处理器共享同一内存总线，平均下来每个 CPU 能获得的可用带宽会下降。
<br>CPU 增多意味着总线长度和负载增加，在技术不变的情况下，这会导致总线延迟上升，影响整体性能。
<br>本来 CPU 和内存的速度差距就在不断拉大，总线延迟的增加只会让这一情况变得更糟，所以长期来看，UMA 肯定不可取。而且随着技术进一步发展，传统的南桥和北桥芯片的功能逐步被集成在一颗处理器芯片中，形成片上系统，即 SoC (System on Chip)。片上系统的一颗处理器可能有许多核心节点。而这些节点在 die 上的分布可能并不均匀。如果想要设计高速的 symmetric interconnected system 会很复杂，所有部件都将集成在一起，散热还是一个需要考虑的问题。<br>为了优化内存的访问速度并减少跨核心延迟的同时兼顾设计和散热问题，现代处理器架构通常采用非统一内存架构 (Non-Uniform Memory Architecture, NUMA)。<br>
<img alt="02 月 3 日NUMA_Local_and_Remote_Access" src="http://frankdenneman.nl/wp-content/uploads/2016/07/02-03-NUMA_Local_and_Remote_Access-750x295.png" referrerpolicy="no-referrer"><br>
在 NUMA 架构下，每个 CPU （或者每个 CPU package）都使用自己的内存控制器去访问自己的本地内存 (local memory)。从而，在 CPU 访问本地内存时，系统就能够提供低延迟-高带宽的性能。我们也把 SoC + 本地内存的这样一个 cluster 称为 NUMA node。这种架构通过引入拓扑感知的内存访问方式避免传统 UMA 架构中因总线导致的访问延迟。<br>NUMA 架构中，各个 NUMA 节点之间通过 CPU 总线或专用的互联协议高速互联。如果一个 NUMA 节点想要去访问其他的 NUMA 节点中的内存，那么跨 NUMA 节点的远程内存访问就会引入额外的延迟和带宽损耗，从而使得访问效率显著下降。<br>假如一个任务在 Node 0 上运行，这个任务要访问的资源在 Node 1 的内存里，就需要通过互连总线把数据从 Node 1 传输到 Node 0 中。对于内存密集型的任务，不同 NUMA 节点之间的数据交互会带来额外的通信开销，影响性能。（还可能造成处理器前端的 installation）<br><img alt="4.1 System Architecture | The-Art-of-Problem-Solving-in-Software ..." src="https://enhancedformysql.github.io/The-Art-of-Problem-Solving-in-Software-Engineering_How-to-Make-MySQL-Better/media/73a26c9835141996aa0470f756f95344.png" referrerpolicy="no-referrer"><br>所以在 NUMA 系统中，我们可以通过将任务绑定在一个 NUMA node 上来减少跨节点的访问，让代码和数据都在同一个 NUMA 节点上，这就是 NUMA-aware 的思想。在 Linux 中，系统提供了一些接口实现处理器亲和性和 NUMA binding：<br>
<br>lscpu
<br>numactl
<br>perl
<br><br>不同 NUMA 节点拥有的资源可能是不一样的，因为每个节点都拥有自己的 IO 控制器。假如网卡在 NUMA 节点 0 的 PCIe 插槽，那么它的 DMA 操作就会有限访问节点 0 的本地内存，如果网络处理线程运行在节点 1，那么就会跨节点远程内存访问，增加延迟。<br><br>NUMA 系统中，如果数据在 NUMA node-1 的 L3 cache 中，NUMA node-2 运行的线程需要使用数据，那么数据会从 NUMA node-1 中的 L3 cache 复制到 NUMA node-2 上的 L3 cache 还是转移？]]></description><link>https://congzhi.wiki/computer-architecture/numa-systems.html</link><guid isPermaLink="false">Computer Architecture/NUMA Systems.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 09 Aug 2025 13:05:56 GMT</pubDate><enclosure url="http://frankdenneman.nl/wp-content/uploads/2016/07/02-02-UMA-Architecture-750x560.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="http://frankdenneman.nl/wp-content/uploads/2016/07/02-02-UMA-Architecture-750x560.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Computer Networking A Top-Down Approach]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br>原计划对 Computer Networking: A Top-Down Approach 整本书进行翻译，翻译了三章翻译不动了。目前已弃坑......<br>这个系列并不建议阅读。本人不对任何翻译产生的错误信息负责。<br>若您对网络感兴趣，请移步 <a data-href="Networks" href="https://congzhi.wiki/networks/networks.html" class="internal-link" target="_self" rel="noopener nofollow">Networks</a> 。<br><br><br>三章的入口罗列在下：<br>Computer Networking: A Top-Down Approach

<br><a data-href="Chapter 1 Computer Networks and the Internet" href="https://congzhi.wiki/computer-networking-a-top-down-approach/chapter-1-computer-networks-and-the-internet.html" class="internal-link" target="_self" rel="noopener nofollow">Chapter 1 Computer Networks and the Internet</a>
<br><a data-href="Chapter 2 Application Layer" href="https://congzhi.wiki/computer-networking-a-top-down-approach/chapter-2-application-layer.html" class="internal-link" target="_self" rel="noopener nofollow">Chapter 2 Application Layer</a>
<br><a data-href="Chapter 3 Transport Layer" href="https://congzhi.wiki/computer-networking-a-top-down-approach/chapter-3-transport-layer.html" class="internal-link" target="_self" rel="noopener nofollow">Chapter 3 Transport Layer</a>
<br><a data-href="SMTP" href="https://congzhi.wiki/computer-networking-a-top-down-approach/smtp.html" class="internal-link" target="_self" rel="noopener nofollow">SMTP</a>

]]></description><link>https://congzhi.wiki/computer-networking-a-top-down-approach/computer-networking-a-top-down-approach.html</link><guid isPermaLink="false">Computer Networking A Top-Down Approach/Computer Networking A Top-Down Approach.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:26:56 GMT</pubDate></item><item><title><![CDATA[ASLR for Safety (ENG)]]></title><description><![CDATA[ 
 <br>Inspired by <a data-href="Malloc in glibc (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/malloc-in-glibc-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Malloc in glibc (ENG)</a><br>Actually, I really should put this in operating system section......<br><br>ASLR is a computer security technique. Its primary purpose is to protect against memory corruption vulnerabilities. It is primarily a security feature used to prevent exploitation by making it harder for attackers to predict memory locations. <br>With ASLR, every time you run your program, the memory mapping will differ. We will demonstrate this phenomenon using an example just a second.<br>In this brief note, we will continue using the example from the malloc note. If you compile that code and run it multiple times, you will observe that the heap memory layout varies with each execution. This variability is a direct result of ASLR.<br>du@DVM:~/cpp$ ./proc
Size of struct linked_block is 16 bytes.
The address of first block is: 0x584b0525b6b0
The address of second block is: 0x584b0525b6d0
du@DVM:~/cpp$ ./proc 
Size of struct linked_block is 16 bytes.
The address of first block is: 0x59697921b6b0
The address of second block is: 0x59697921b6d0
du@DVM:~/cpp$ ./proc 
Size of struct linked_block is 16 bytes.
The address of first block is: 0x5659369f46b0
The address of second block is: 0x5659369f46d0
du@DVM:~/cpp$ ./proc 
Size of struct linked_block is 16 bytes.
The address of first block is: 0x61b7291d06b0
The address of second block is: 0x61b7291d06d0
<br>Modern operating systems enable ASLR by default. This is why, each time you run your program, the memory mapping is different. And you can disable ASLR using this:<br>setarch $(uname -m) -R ./proc
<br><br>When running your program in GDB, you will notice that the memory addresses remain consistent across executions. For instance:<br>(gdb) run
Starting program: /home/du/cpp/proc 
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Size of struct linked_block is 16 bytes.
The address of first block is: 0x5555555596b0
The address of second block is: 0x5555555596d0
[Inferior 1 (process 12774) exited normally]
<br>This happens because ASLR is disabled by default in GDB. The reason for this is that GDB is designed for debugging. Consistent memory addresses are very important during debugging to allow for repeatable and predictable analysis of memory-related behaviors. <br>If you wish, you can enable ASLR in GDB using the following command:<br>set disable-randomization off
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/aslr-for-safety-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/ASLR for Safety (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:27:31 GMT</pubDate></item><item><title><![CDATA[Assert in C++ (NC)]]></title><description><![CDATA[ 
 <br>static_assert: a compile-time assert]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/assert-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Assert in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:27:43 GMT</pubDate></item><item><title><![CDATA[Branchless Programming Intro in C++ (ENG)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Know Your Hardware - Modern CPU Architecture" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/know-your-hardware-modern-cpu-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Know Your Hardware - Modern CPU Architecture</a><br><br><br>Every programmer working with C++ is, in some way, obsessed with performance. Unlike Java or Python, C++ doesn't provide too many abstractions, making such language a true systems programming language, which is all about performance.<br>To optimize performance, a Cpper not only needs to understand the language itself (efficient use of language and finding optimal algorithm), but also the underlying computing system (efficient use of hardware). This include how data moves in and out of memory, how cache works, and how CPU or even GPU executes calculations efficiently.<br>In today's topic, we will focus on branchless programming, to see how you can boost CPU executions with C++.<br><br><br>Inside a modern CPU, we of course have execution units, caches, decoder and microcode, besides that, we got branch prediction, out-of-order execution, speculative execution, register renaming, and multi-level instruction pipelines.<br>Back in the day, a CPU core could only perform one instructions at a time, making it impossible to do the optimization with hardware. But nowadays, the pipeline technique allows CPU core to do multiple things at the same time. Instructions can be fetched, decoded, executed, and written back in parallel stages.<br>When pipelining, branches become critical factors that can be slow down the CPU's instruction pipelining factory. This happens because the CPU must wait for a branch instruction to resolve before it can proceed down the correct path. While modern CPUs include a branch predictor in the front-end, allowing pipeline to continue speculatively without stalling, but again, if the guess is wrong, the CPU thus need to flushes the entire pipeline of the front-end and reload the correct branch, which could be painful (the back-end may need 20 instruction cycles to reload the instructions).<br>Most modern CPUs implement dynamic branch prediction using a table that records branch history. And the predictor consults this history to make educated guesses about which path a branch would take. While the predictor itself doesn't have infinite buffer capacity, its goal is to use recent execution outcomes to. optimize future decisions.<br>In general, the branch misprediction rate is below 10%, and when the branch behavior in your code follows a consistent pattern, the misprediction rate can dorp significantly - sometimes even less than 0.1%.<br>// Example of well-patterned branching:
for (size_t i = 0; i &lt; 10000; i++) {
	//...
}
<br><br><br>From different perspectives, the concept of a "branch" can be understood differently. Assuming we have the code down below:<br>if (x || y) {
	// Branch a
} else {
	// Branch b
}
<br>To programmers, x || y is seen as a single compound condition, and it's very natural that we may assume that CPU evaluates the entire logical expression as one unit. <br>However, from the perspective of the compiler and the CPU, this involves two distinct conditions: x and y. Here's how CPU unfolds the evaluation:<br>
<br>If x's true, the short-circuit behavior of || kicks in, and y isn't even been considered. The flow proceeds directly to the branch always.
<br>If x's false, only then will the machine evaluate the value of y. This is when the branch b been considered by the hardware.
<br>In this example, it's this short-circuit of the execution that causes branch misprediction. If we can evaluate x and y at the same time, the prediction would be unnecessary. So, what can we do? We can use integer or bitwise arithmetic logic on boolean expressions, which allows the CPU evaluate both operands unconditionally. For example:<br>if (bool(x) + bool(y)) {
	//...
} else {
	//...
}

// or

if (bool(x) | bool(y)) {
	//...
} else {
	//...
}

// or

auto flag = bool(x) + bool(y);
if (flag) {
	//...
} else {
	//...
}
<br>But you may ask: isn't the CPU doing a lot more job this way? Yes, it is evaluating both operands - but as discussed in <a data-href="Know Your Hardware - Modern CPU Architecture" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/know-your-hardware-modern-cpu-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Know Your Hardware - Modern CPU Architecture</a>, modern CPUs are built to handle multiple things at once. Thanks to instruction pipelining, out-of-order execution, evaluating simple expressions in parallel is trivially fast, you get those extra computation for free, but a pipeline stall IS a big deal.<br><br><br>In the last example, we reduced two branches into one. In the example down below, we can use some tricks to reduce one branch to zero, achieving a true branchless execution.<br>Instead of relying condition branching code:<br>auto sum += cond ? expr1 : expr2;
<br>We could restructure the code to this:<br>auto terms[2] = {expr2, expr1};
sum += term[bool(cond)]
<br>Here, we pre-compute both options and select the correct one based on the boolean value. This tricks avoids branching entirely and allows compiler to generate straight-line code with no jumps. However, not all branch substations lead to actual performance gains. Some optimization would not work, and result in what's known as spectacular pessimization. For example:<br>if (cond) {
	f1();
} else {
	f2();
}
<br>which can be written as:<br>auto fptr f[2] = {&amp;f2, &amp;f1};
(f[cond])();
<br>While this does eliminate the explicit branch, but it introduces new problems. The use of function pointers prevents the compiler from applying optimizations, such as inlining, and this may lead to cache misses due to indirection call. As a result, performance might degrade rather than improve.<br><br><br>So this is branchless programming, instead of introducing branches, you just do a little more work. (Yes, this is a trade-off)]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/branchless-programming-intro-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Branchless Programming Intro in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 24 Jul 2025 20:33:18 GMT</pubDate></item><item><title><![CDATA[C-Style Casts in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Integer Literal &amp; Float Literal in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/integer-literal-&amp;-float-literal-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Integer Literal &amp; Float Literal in C++</a><br><br>在 C 语言中，我们有许多的数据类型（int, float, char 等）。有时候，为了处理不同类型的数据，我们需要将一个数据从一种类型转换成另一种类型。比方说下面常见的运算类型：<br>#include &lt;iostream&gt;

int main(){
	int si = 80000;
	short s = si; // casting integer i to a short type variable	
	std::cout &lt;&lt; s &lt;&lt; std::endl;

	int i = -1;
	unsigned int ui = 10;
	if(i &gt; u){
		std::cout &lt;&lt; "What's going on??" &lt;&lt; std::endl;
	}
}
<br>你会得到这样的输出：<br>14464
What's going on??
<br>运行的结果告诉你 80000 = 14464，-1 比 10 要大。这是因为 int 类型变量有 4 字节，而 short 类型的变量只有 2 字节（最大存储范围是  -32768 - 32767）由于 80000 超出了 short 可存储的范围，所以发生了整数溢出。导致的结果就是 s 只存储了两字节的截断后的值。<br>00000000 00000001 00111010 00000000  int
                  00111010 00000000  short
<br>而当我们比较一个带符号 int 类型的 -1 和一个无符号 int 类型的 10 进行比较时，编译器会自动将带符号的 i_ 转换成无符号数来进行比较。-1 在机器中的二进制表示是下面这样的。如果被转换成一个无符号数，那么这个数字的大小就是 4294967295。远远比 10 要大。<br>11111111 11111111 11111111 11111111  (-1)
<br>奇奇怪怪对吧？这是因为参与运算的双方类型不一致，编译器帮你把类型自动转换了，这叫隐式类型转换 (Implicit casting)。在指针操作中，我们常用 void* 指针类型来存储任意类型的数据，在使用时转换成相应数据类型的指针，这种由程序员手动明确指定的类型转换叫显示类型转换 (Explicit casting)，也叫强制类型转换。<br>void* ptr = nullptr;
int i = 10;
ptr = &amp;i;
std::cout &lt;&lt; *(int*)ptr &lt;&lt; std::endl;
float f = 3.14;
ptr = &amp;f;
std::cout &lt;&lt; *(float*)ptr &lt;&lt; std::endl;
<br>虽然类型转换非常有用，但是如果操作不当，就会发生一些像上面演示中那些匪夷所思的事情。为了避免这些问题，C++ 提供了许多方法。<br><br>我们接着看一个例子，编译一下，观察输出结果。你会发现，输出的 pi_int 的值是 3 的，而且 355/113 也是输出了一个整数 3 。这是为什么？<br>#include &lt;iostream&gt;
int main(){
	int pi_int = 3.14159;
	std::cout &lt;&lt; pi_int &lt;&lt; std::endl; // 3
	std::cout &lt;&lt; 355/113 &lt;&lt; std::endl;// 3
	return 0;
}
<br>这两个问题都同样和 implicit narrowing conversion 有关。当编译器看到 int pi_int = 3.14159; 时，编译器会将右边的浮点数看成一个 double 类型的数。而我们想把这个浮点数存到一个 int 类型。由于类型不同，这里会发生 narrow conversion（窄转换）。将结果截断到整数个位，损失小数的精度。所以我们得到的结果为 3。<br>在执行 std::cout &lt;&lt; 355/113 &lt;&lt; std::endl; 时，编译器会先检查除数和被除数的类型。这两种类型都是 int 类型，所以编译器断定商数应该是 int 类型的，在运行这行代码时也会出现截断。要输出正确的值，我们可以将类型显式地告诉编译器或者显式地使用浮点字面量类型。如：<br>#include &lt;iostream&gt;
int main(){
    std::cout &lt;&lt; (double)355/(double)113 &lt;&lt; std::endl; // 3.14159
    
    // or, explicitly use the float literals
    std::cout &lt;&lt; 355.0/113.0 &lt;&lt; std::endl; // 3.14159
	return 0;
}
<br>为了类型安全，在 C++11 后还引入了列表初始化禁止这种 narrowing conversion 的发生。<br>int pi = 3.14; // okay
int pi{3.14};  // error
<br><br>对于有符号和无符号类型的比较，C++20 中的 &lt;utility&gt; 库中提供了如 std::cmp_greater() 这样的函数，用于安全地比较有符号和无符号整数，避免常见的类型转换问题。<br>#include &lt;iostream&gt;
#include &lt;utility&gt;

int main() {
    int i = -1;
    unsigned int u = 10;

    if (std::cmp_greater(i, u)) {
        std::cout &lt;&lt; "Correct comparison!" &lt;&lt; std::endl;
    } else {
        std::cout &lt;&lt; "Expected result!" &lt;&lt; std::endl;
    }
}
<br><a data-tooltip-position="top" aria-label="https://en.cppreference.com/w/cpp/utility/intcmp" rel="noopener nofollow" class="external-link" href="https://en.cppreference.com/w/cpp/utility/intcmp" target="_blank">std::cmp*</a><br><br>不同于 C 语言，C++ 会对语言进行一些封装以提供更好的安全性并支持 C++ 的语言特性。在 C++ 中，我们主要有四种显式类型转换方式：<br>
<br>static_cast：基本类型转换，对 C 语言中显示类型的封装。
<br>dynamic_cast：用于类类型之间的转换，通常用于运行时多态。
<br>reinterpret_cast：用于指针类型的转换，如 int* -&gt; char*。
<br>const_cast：去除变量的 const 修饰，使得 const 变量可修改。
<br>这几种转换类型在之后的 parts 中介绍。<br>当 C++ 编译器遇到 C 类型的类型转换时，它会将其解释为 C++ 的显式类型转换。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/c-style-casts-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/C-Style Casts in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 30 May 2025 13:30:28 GMT</pubDate></item><item><title><![CDATA[Call Stack in C++ (ENG)]]></title><description><![CDATA[ 
 <br>Learn more at: <a data-tooltip-position="top" aria-label="https://www.icourse163.org/learn/NJU-1001625001?tid=1472100484#/learn/content?type=detail&amp;id=1257458503&amp;cid=1291031865" rel="noopener nofollow" class="external-link" href="https://www.icourse163.org/learn/NJU-1001625001?tid=1472100484#/learn/content?type=detail&amp;id=1257458503&amp;cid=1291031865" target="_blank">计算机系统基础(一)：第七周<em></em>中国大学MOOC(慕课)</a>过程调用概述<br>
Wanna learn more about process? Check out: <a data-href="6. Processing The Processes" href="https://congzhi.wiki/congzhi's-os-series/6.-processing-the-processes.html" class="internal-link" target="_self" rel="noopener nofollow">6. Processing The Processes</a>.<br><br>The call stack, also known as the execution stack, and is often shortened to simply the "stack". Which is a fundamental component in how your program runs. It refers to a particular segment of memory within a process's virtual memory layout, used to manage function calls and their execution context. Before going deep into this topic, it's important to understand the role stack plays in a process's memory layout. And if you're unfamiliar with it, I've linked some resources above for you to check out.<br>To put this simply: your program runs on the stack. Say, when a function is called, the system sets aside some space in the stack memory for the function to do its necessary work. These chunks of space or memory are called "stack frame" or "function frame". The call stack's primary purpose is to manage how functions call one another and how parameters are passed between them.<br>Each thread has its own call stack, also referred to as stack memory.<br>Although this data structure is normally abstracted away by the high-level programming languages, understanding its structure and behavior is still quite useful, especially when exploring some advanced features like RVO (Return Value Optimization) and copy elision in C++. These optimization offered by compiler is often involving stack frames and function return mechanics.<br><br>Firstly, let's start by understanding how a function stack frame is formed and what it contains. In the code below, we define a function called add, which is then called inside the main function. So what happens during the execution? Since there are two functions involved, the system creates two stack frames (one for the main function and another for add). Because the stack is a LIFO data structure, you can imagine how these function frames are constructed and then destroyed in reverse order.<br>int add(int x, int y){
	return x + y;
}
int main(){
	int a = 32;
	int b = 64;
	int sum = add(a, b);
}
<br>Here's what happens step by step:<br>
<br>The main() function is called first, and its function frame is created.    
<br>The add() function is called by main(), and its function frame is pushed onto the stack.
<br>After the add() function finishes executing, its function frame is popped off the stack, and control returns to main().
<br>Once main() finishes, its stack frame is removed, and the program terminates.
<br><img alt="call_stack.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/call_stack.png"><br><br>We now understand how stack frames are pushed and popped. Next, let's delve into what is inside the stack frame. We'll compile the code above into assembly language to examine the details. For this purpose, I'll use GCC with the -m32 option here to generate 32-bit machine assembly code, because on a 64-bit machine, parameters are typically passed using registers rather than the stack.<br>Here's the assembly:<br>_Z3addii:
    pushl   %ebp
    movl    %esp, %ebp
    movl    8(%ebp), %edx
    movl    12(%ebp), %eax
    addl    %edx, %eax
    popl    %ebp
    ret
main:
    pushl   %ebp
    movl    %esp, %ebp

	subl    $16, %esp

	; Construct local variables in the stack frame
    movl    $32, -12(%ebp)
    movl    $64, -8(%ebp)

	; Push local variable as argument for function call
	pushl   -8(%ebp)
    pushl   -12(%ebp)

	; Call add() function
    call    _Z3addii
    addl    $8, %esp
    movl    %eax, -4(%ebp)   ; &lt;- Next instruction address
    movl    $0, %eax
    leave
    ret
<br>Okay, in assembly, there are two important registers help define the boundaries of a stack frame: the base pointer register and the stack pointer register.<br>
<br>The base pointer register (ebp in this case) always point to the bottom of the current stack frame.
<br>The stack pointer register (esp in this case) always points to the top of the stack frame..
<br>Because the stack grows from high memory addresses to low memory addresses, esp<br>Before calling add(), main() forms its stack frame using ebp and esp. It first saves the old base pointer using a pushl instruction, and then sets the new ebp to point to the old ebp, creating the base of the stack frame. After that, space is allocated on the stack for local variables. Then, the passing parameters are pushed, and add() is called.<br>When the call instruction is executed, a return address is automatically pushed into the stack frame. In the add() function, a new stack frame is formed using pushl %ebp and movl %esp, %ebp. After the calculation is done, the return value is passed back via eax. The frame is then cleaned up with popl %ebp.<br>After the frame is cleaned up, the return address is instantly passed to eip, which points to the next instruction address of call _Z3addii. The stack space allocated for passing parameters is then cleaned up. Finally, the return value 0 is set, and the stack is further cleaned up.<br><br>From the above, we have seen that passing parameters involves copying them to the stack (32-bit machine). If the object is large, this copying can be quite costly in terms of CPU time. To mitigate this, we can pass a pointer or a reference to the object instead, making the copy much smaller. You can consider references in C++ as syntactic sugar for pointers, they act the same way.<br>For large objects, it's always recommended to pass it by a reference. You might be concerned that directly operating on the original object could lead to unintended modifications. To avoid this, you can use the const keyword to ensure the object is not altered. This approach is exactly what we use in a copy constructor and copy assignment operator.<br><br>It is generally recommended to pass by const reference. On modern 64-bit machines, a pointer requires 8 bytes to store. Whether you use pass-by-pointer or pass-by-reference, you will always copy an 8-byte pointer to somewhere, typically a register (or the stack when there are many parameters).<br>When you pass a easy type no bigger than 8 bytes (e.g., int, which typically stores 4 bytes on most machines) by value, you only copy 4 bytes. However, if you use a pointer or reference, it would take 8 bytes. In this case, will pass this type no bigger than 8 bytes by value be better?<br>It looks so, because you copy 4 bytes must be two times faster than copy a 8 bytes pointer. But in most cases, these parameters will be passed to a register (the rule is in the next section). Copying a type no larger than 8 bytes to a register doesn't make much of a difference in terms of performance. But it's a good practice to do so.<br>It seems so, because copying 4 bytes must be two-times faster than copying an 8-byte pointer, right? However, in most cases, these parameters will be passed to a register. Copying a type no larger than 8 bytes to a register doesn't make much of a difference in terms of performance. But it's a good practice to do so.<br><br>On 64-bit machines, function parameters are usually passed through specific registers, and only when the number of parameters exceeds the available registers do parameters get passed on the stack. Here are the typical rules for x86-64 (System V ABI):<br>
<br>Integer and Pointer Parameters:<br>
Passed through registers: RDI, RSI, RDX, RCX, R8, R9 (for the first six parameters).<br>
Any additional parameters are passed on the stack.
<br>Floating-point Parameters:<br>
Passed through registers: XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7 (for the first eight parameters).<br>
Any additional parameters are passed on the stack.
<br>For custom complex types:<br>
<br>If passed by value and they do not fit into available registers, they are stored on the stack.
<br>If passed by reference (pointer), the reference itself is passed through a register if there are enough available.
<br>Following these rules ensures efficient and consistent parameter passing, helping to optimize function calls.]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/call-stack-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Call Stack in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 09 Aug 2025 13:07:31 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/call_stack.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/call_stack.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Character Literal & String Literal in C++]]></title><description><![CDATA[ 
 <br><a data-href="Integer Literal &amp; Float Literal in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/integer-literal-&amp;-float-literal-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Integer Literal &amp; Float Literal in C++</a><br><br>在我们了解了整数型字面量和浮点型字面量之后，这篇文档我们来学习什么是字符字面量和字符串字面量。首先，我们来看一看最常见的字符字面量和字符串字面量。<br><br><img alt="std_ascii.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/std_ascii.png"><br>普通字符字面量就是我们常用 char 类型来表示的标准 ASCII 字符。 char 类型只占 1 个字节，能表示的数值在 0 - 255 这 256 个数字，而用来表示标准的 ASCII 绰绰有余（不包含 EASCII ）。普通字符字面量在 C++ 中有这几种表示形式：<br>char a1 = 'a';      // basic character, ASCII value of 'a' is 97
char a2 = '\x61';   // using escape sequence, equals to '\141' as in octal
char a3 = 0x61;     // direct hexadecimal value, equals to a3 = 97;
<br>第一种形式适合用于表示那些在 ASCII 中可显示的字符，上图从 0x21(32) 到 0x7E(126) 都是可显示字符。而那些不可显示的字符我们通常用转义字符来表示，比如换行符 \n 和水平制表符 \t，你也可以用转移字符表示可显示的字符。因为每个 ASCII 字符在表中都有一个数字与之对应，你也可以直接使用这些数值来对 char 类型变量赋值。<br><br>"String is an array of characters"，不难理解，普通字符串字面量就是由普通字符字面量构成的字符序列。这些字符序列要在双引号 " " 中进行表示。里面的字符可以是标准 ASCII 字符和转义字符。普通字符串字面量的类型是 const char[N] 或 const char*。为了知道结尾的位置，编译器会在字符串末尾添加一个空字符 \0。<br>const char* str1 = "Hello, world"; // Same as const char[] str1 = "Hello, World!";
const char* str2 = "Line1\nLine2";
const char* str3 = "Hello,\0world"; // only prints "Hello,"
<br>在这个例子中的三个普通字符串字面量在内存中如下图所示：<br>
<img alt="string_literal.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/string_literal.png"><br>
C++11 引入了原始字符串字面量，这也是一种普通字符串字面量。和上面的用双引号表示方法不同的是，原始字符串字面量以 R"()" 的形式表示。允许字符串中包含反斜杠和引号而不用转义。我们接着用上面的例子举例：<br>const char* str1 = R"(Hello, world)";
const char* str2 = R"(Line1
Line2)";
const char* str3 = R"(Hello,\0world)"; // prints Hello,\0world
<br><br>Unicode 是一个字符集，旨在包含全世界上所有的字符和符号，它兼容 ASCII 。UFT(Unicode Transformation Format) 是一系列用于编码 Unicode 字符的标准。<br><br>UFT-8 字符字面量能够表示的字符和普通字符字面量能表示的字符是相同的（0x00 - 0x7F），都只能表示标准 ASCII 字符。所以UFT-8 字符字面量的一个字符只占用 1 字节。<br>UFT-8 表示字面量格式如下：<br>char a = u8'a'; // until C++20
char8_t a = u8'a'; // since C++20
<br>在 C++20 之前，我们用 char 类型来表示 UFT-8 字符字面量，因为 Unicode 兼容 ASCII 嘛。但是在 C++20 之后，我们用 char8_t 类型来表示 UFT-8 的字符字面量。为了保持统一。<br><br>// Until C++20
const char[] c = u8"Hello, world!";
const char[] c2 = u8R"(Hello, world!)";

// Since C++20
const char8_t[] c = u8"Hello, world!";
const char8_t[] c2 = u8R"(Hello, world!)";
<br>普通和 UFT-8 d 字符串字面量统称为narrow string literals.<br><br><br>虽然 char 类型表示的字符还能扩容到 256 个。但是对于世界上这么多字符符号，256 个完全是不够用的。为了表示比普通字符集更大的字符集，在 C++ 中，我们有宽字符或者叫长字符字面量。在不同的平台，宽字符大小的实现可能有所不同，通常是 2 字节（UFT-16）或是 4 字节（UFT-32）。字节数多了，能够表示的字符数量也就指数级别的增多。2 字节宽字符可以表示 65536 个字符，4 字节宽字符可以表示超过 400 万个字符。<br>宽字符用 wchar_t 类型来表示，我们有下面的例子：<br>wchar_t a = L'哈'; // Allocate 2/4 bytes of memory depending on the system
std::wcout &lt;&lt; a &lt;&lt; std::endl;
std::wcout &lt;&lt; L'β' &lt;&lt; std::endl;
<br>奇怪的是宽字符的前缀不为 W 而是 L，令人百思不得其解。<br><br>一切尽在不言中。<br>const wchar_t* c = L"Hello, world!";
const wchar_t* c2 = LR"(Hello, world!)";
<br><br>你现在知道了字符字面量和字符串字面量表示上的联系，我们最后再来看看 C++ 中的最后两种字符字面量：UFT-16 字符字面量和 UFT-32 字符字面量。<br><br>UFT-16 字符用 char16_t 类型表示，单个 char16_t 字符占用 2 字节内存，可以表示 65536 个不同的字符。UFT-32 字符用 char32_t 类型表示，单个 char32_t 字符占用 4 字节内存，可以表示 4,294,967,296 个不同的字符。<br>UFT-16 和 UFT-32 表示字面量的格式如下：<br>char16_t a = u'a';
char32_t a = U'a';
<br><br>// UFT-16 string literals
const char16_t* c = u"Hello, world!";
const char16_t* c2 = uR"(Hello, world!)";
// UFT-32 string literals
const char32_t* c = U"Hello, world!";
const char32_t* c2 = UR"(Hello, world!)";
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/character-literal-&amp;-string-literal-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Character Literal &amp; String Literal in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 27 May 2025 03:37:23 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/std_ascii.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/std_ascii.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Command Design Pattern (NC)]]></title><description><![CDATA[ 
 <br>行为模式 (behavoir pattern) 是一种主要关注对象如何交互的设计模式，将重点关注对象如何的行为，而不是对象的结构或创建方式。命令设计模式就属于行为型模式，简单来说，命令设计模式就是一种将对象的行为封装成一条条命令的设计模式，即 command design pattern。<br>在命令设计模式中，命令可以用于：<br>
<br>GUI 按钮和菜单，每个按钮绑定一个命令对象，点击执行命令（回调）
<br>将命令存入队列，按顺序执行（存储任务队列）
<br>存储命令历史，支持撤销和重做操作
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/command-design-pattern-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Command Design Pattern (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 31 May 2025 12:09:38 GMT</pubDate></item><item><title><![CDATA[Congzhi's C Plus Plus Series]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">🔙 Go Back Home</a><br><br><br>This series will document my C++ learning path from zero to hero. A big part of these notes are inspired by Mr. Mike, all the learning resources are listed below:<br>
这个系列用于记录我学习 C++ 的进化之路。参考资料来源如下：<br>
<br><a data-tooltip-position="top" aria-label="https://en.cppreference.com/w/" rel="noopener nofollow" class="external-link" href="https://en.cppreference.com/w/" target="_blank">cppreference.com</a>
<br><a data-tooltip-position="top" aria-label="https://www.learncpp.com/" rel="noopener nofollow" class="external-link" href="https://www.learncpp.com/" target="_blank">Learn C++</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@CppCon" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@CppCon" target="_blank">CppCon Talks</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@CppNow" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@CppNow" target="_blank">CppNow Talks</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@TheCherno/playlists" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@TheCherno/playlists" target="_blank">The Cherno's C++ Series</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/@MikeShah/playlists" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/@MikeShah/playlists" target="_blank">Mr. Mike Shah's C++ Series</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/playlist?list=PLvv0ScY6vfd9wBflF0f6ynlDQuaeKYzyc" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/playlist?list=PLvv0ScY6vfd9wBflF0f6ynlDQuaeKYzyc" target="_blank">Mr. Mike Shah's Design Pattern Series</a>
<br><a data-tooltip-position="top" aria-label="https://gameprogrammingpatterns.com/" rel="noopener nofollow" class="external-link" href="https://gameprogrammingpatterns.com/" target="_blank">Game Programming Patterns by Robert Nystorm</a>
<br>...
<br>参考的书籍有：<br>
<br>Effective C++ - Scott Mayer
<br>Effective Modern C++ - Scott Mayer
<br>一些好用的工具：<br>
<br><a data-tooltip-position="top" aria-label="https://godbolt.org/" rel="noopener nofollow" class="external-link" href="https://godbolt.org/" target="_blank">Compiler Explorer</a>
<br><a data-tooltip-position="top" aria-label="https://cppinsights.io/" rel="noopener nofollow" class="external-link" href="https://cppinsights.io/" target="_blank">C++ Insights</a>
<br><br><br>C++ 笔记将分为中文笔记、英文笔记和设计模式三部分，你可以根据英文关键字检索你感兴趣的内容。部分笔记可能比较简短，但对于理解这些 C++ 知识应当是足够的。目前，我所有的 C++ 笔记罗列如下：<br>Congzhi's C++ Series (86 notes in total)
C++ 中文笔记 (NC stands for Not Covered, 50 notes in total)：

<br><a data-href="Assert in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/assert-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Assert in C++ (NC)</a>
<br><a data-tooltip-position="top" aria-label="C-Style Casts in C++" data-href="C-Style Casts in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/c-style-casts-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Casting in C++ (Part I)</a>
<br><a data-tooltip-position="top" aria-label="Static Casting &amp; Reinterpret Casting in C++ (NC)" data-href="Static Casting &amp; Reinterpret Casting in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/static-casting-&amp;-reinterpret-casting-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Casting in C++ (Part II)</a>
<br><a data-tooltip-position="top" aria-label="Dynamic Casting in C++" data-href="Dynamic Casting in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/dynamic-casting-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Casting in C++ (Part III)</a>
<br><a data-href="Character Literal &amp; String Literal in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/character-literal-&amp;-string-literal-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Character Literal &amp; String Literal in C++</a>
<br><a data-href="Concurrency in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/concurrency-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Concurrency in C++</a>
<br><a data-href="Const in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/const-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Const in C++</a>
<br><a data-href="Copying and Copy Constructors in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/copying-and-copy-constructors-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Copying and Copy Constructors in C++</a>
<br><a data-href="Effective C++ - Prefer the Compiler to the Processor" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/effective-c++-prefer-the-compiler-to-the-processor.html" class="internal-link" target="_self" rel="noopener nofollow">Effective C++ - Prefer the Compiler to the Processor</a>
<br><a data-href="Enums in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/enums-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Enums in C++ (NC)</a>
<br><a data-href="Exception Control in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/exception-control-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Exception Control in C++ (NC)</a>
<br><a data-href="Extern Keyword in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/extern-keyword-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Extern Keyword in C++</a>
<br><a data-href="Function Pointers in C++ (Examples)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/function-pointers-in-c++-(examples).html" class="internal-link" target="_self" rel="noopener nofollow">Function Pointers in C++ (Examples)</a>
<br><a data-href="Functions in Standard Library (Examples)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/functions-in-standard-library-(examples).html" class="internal-link" target="_self" rel="noopener nofollow">Functions in Standard Library (Examples)</a>
<br><a data-href="Generics Programming in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/generics-programming-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Generics Programming in C++ (NC)</a>
<br><a data-href="Google C++ Style Guide" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/google-c++-style-guide.html" class="internal-link" target="_self" rel="noopener nofollow">Google C++ Style Guide</a>
<br><a data-href="GPU Programming in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/gpu-programming-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">GPU Programming in C++ (NC)</a>
<br><a data-href="Inheritance in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/inheritance-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Inheritance in C++</a>
<br><a data-href="Integer Literal &amp; Float Literal in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/integer-literal-&amp;-float-literal-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Integer Literal &amp; Float Literal in C++</a>
<br><a data-href="Know Your Hardware - Modern CPU Architecture" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/know-your-hardware-modern-cpu-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Know Your Hardware - Modern CPU Architecture</a>
<br><a data-href="Lambdas in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/lambdas-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Lambdas in C++</a>
<br><a data-href="Memory Management in C++ (Abandoned)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/memory-management-in-c++-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">Memory Management in C++ (Abandoned)</a>
<br><a data-href="Move Semantics in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/move-semantics-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Move Semantics in C++</a>
<br><a data-href="Networking - CS model (Abandoned)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/networking-cs-model-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">Networking - CS model (Abandoned)</a>
<br><a data-href="Object Oriented Programming in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/object-oriented-programming-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Object Oriented Programming in C++</a>
<br><a data-href="Part1：C++11 (Abandoned)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/part1：c++11-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">Part1：C++11 (Abandoned)</a>
<br><a data-href="Part2：Class (Abandoned)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/part2：class-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">Part2：Class (Abandoned)</a>
<br><a data-href="Perfect Forwarding in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/perfect-forwarding-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect Forwarding in C++</a>
<br><a data-href="Post-Increment and Pre-Increment in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/post-increment-and-pre-increment-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Post-Increment and Pre-Increment in C++ (NC)</a>
<br><a data-href="Program Arguments Handling in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/program-arguments-handling-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Program Arguments Handling in C++</a>
<br><a data-href="RAII and Scope in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/raii-and-scope-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">RAII and Scope in C++</a>
<br><a data-href="Smart Pointers in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/smart-pointers-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Smart Pointers in C++</a>
<br><a data-href="Standard Variant in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/standard-variant-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Standard Variant in C++</a>
<br><a data-href="Static Keyword in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/static-keyword-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Static Keyword in C++</a>
<br><a data-href="STL in C++ (Pre-Part)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-in-c++-(pre-part).html" class="internal-link" target="_self" rel="noopener nofollow">STL in C++ (Pre-Part)</a>
<br><a data-href="STL Container - Standard Array in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-container-standard-array-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">STL Container - Standard Array in C++ (NC)</a>
<br><a data-href="STL Container - Standard Vector in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-container-standard-vector-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">STL Container - Standard Vector in C++ (NC)</a>
<br><a data-href="STL Container - Standard Span Since C++20 (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-container-standard-span-since-c++20-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">STL Container - Standard Span Since C++20 (NC)</a>
<br><a data-href="STL Iterator - A Generic Pointer Wrapper (Part I)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-iterator-a-generic-pointer-wrapper-(part-i).html" class="internal-link" target="_self" rel="noopener nofollow">STL Iterator - A Generic Pointer Wrapper (Part I)</a>
<br><a data-href="STL Iterator - Range Access (Part II)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-iterator-range-access-(part-ii).html" class="internal-link" target="_self" rel="noopener nofollow">STL Iterator - Range Access (Part II)</a>
<br><a data-href="STL Iterator - Iterator Invalidation (Part III)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/stl-iterator-iterator-invalidation-(part-iii).html" class="internal-link" target="_self" rel="noopener nofollow">STL Iterator - Iterator Invalidation (Part III)</a>
<br><a data-href="String Library in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/string-library-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">String Library in C++ (NC)</a>
<br><a data-href="The Rule of Five Idiom in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/the-rule-of-five-idiom-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">The Rule of Five Idiom in C++</a>
<br><a data-href="Type Deduction - Template Type Deduction" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-template-type-deduction.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Template Type Deduction</a>
<br><a data-href="Type Deduction - Auto in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-auto-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Auto in C++</a>
<br><a data-href="Type Deduction - Decltype in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-decltype-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Decltype in C++</a>
<br><a data-href="Unions in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/unions-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Unions in C++</a>
<br><a data-href="Virtual Dispatch in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Dispatch in C++</a>
<br><a data-href="Virtual Inheritance in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-inheritance-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Inheritance in C++ (NC)</a>
<br><a data-href="Volatile Specifier in C++ (Questioning)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/volatile-specifier-in-c++-(questioning).html" class="internal-link" target="_self" rel="noopener nofollow">Volatile Specifier in C++ (Questioning)</a>


Current existing C++ English notes are listed down below:
C++ English Notes: (32 notes in total)

<br><a data-href="Alignment in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/alignment-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Alignment in C++ (ENG)</a>
<br><a data-href="ASLR for Safety (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/aslr-for-safety-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">ASLR for Safety (ENG)</a>
<br><a data-href="Branchless Programming Intro in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/branchless-programming-intro-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Branchless Programming Intro in C++ (ENG)</a>
<br><a data-href="Call Stack in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/call-stack-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Call Stack in C++ (ENG)</a>
<br><a data-href="Constexpr in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/constexpr-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Constexpr in C++ (ENG)</a>
<br><a data-href="Consteval in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/consteval-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Consteval in C++ (ENG)</a>
<br><a data-href="Constinit in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/constinit-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Constinit in C++ (ENG)</a>
<br><a data-href="Delegating Constructors in C++ (ENG, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/delegating-constructors-in-c++-(eng,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Delegating Constructors in C++ (ENG, NC)</a>
<br><a data-href="Explicit Specifier in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/explicit-specifier-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Explicit Specifier in C++ (ENG)</a>
<br><a data-href="Friend Keyword in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/friend-keyword-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Friend Keyword in C++ (ENG)</a>
<br><a data-href="Inline in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/inline-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Inline in C++ (ENG)</a>
<br><a data-href="Lock-Free Programming in C++ (ENG, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/lock-free-programming-in-c++-(eng,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lock-Free Programming in C++ (ENG, NC)</a>
<br><a data-href="Malloc in glibc (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/malloc-in-glibc-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Malloc in glibc (ENG)</a>
<br><a data-href="Member Initializer List in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/member-initializer-list-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Member Initializer List in C++ (ENG)</a>
<br><a data-href="Mutable and The M&amp;M Rule in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/mutable-and-the-m&amp;m-rule-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Mutable and The M&amp;M Rule in C++ (ENG)</a>
<br><a data-href="Namespaces in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/namespaces-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Namespaces in C++ (ENG)</a>
<br><a data-href="noexcept in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/noexcept-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">noexcept in C++ (ENG)</a>
<br><a data-href="Operator Overloading in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/operator-overloading-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Operator Overloading in C++ (ENG)</a>
<br><a data-href="Parameter Pack in C++ (ENG, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/parameter-pack-in-c++-(eng,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Parameter Pack in C++ (ENG, NC)</a>
<br><a data-href="Preprocessor in C++ (Part I, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-i,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part I, NC)</a>
<br><a data-href="Preprocessor in C++ (Part II, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-ii,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part II, NC)</a>
<br><a data-href="Preprocessor in C++ (Part III, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-iii,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part III, NC)</a>
<br><a data-href="Preprocessor in C++ (Part IV, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-iv,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part IV, NC)</a>
<br><a data-href="Preprocessor in C++ (Part V, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-v,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part V, NC)</a>
<br><a data-href="Preprocessor in C++ (Part VI, NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-vi,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">Preprocessor in C++ (Part VI, NC)</a>
<br><a data-href="RVO in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/rvo-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">RVO in C++ (ENG)</a>
<br><a data-href="Standard Array Basics (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/standard-array-basics-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Standard Array Basics (ENG)</a>
<br><a data-href="Static Dispatch in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/static-dispatch-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Static Dispatch in C++ (ENG)</a>
<br><a data-href="The pIMPL Idiom in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/the-pimpl-idiom-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">The pIMPL Idiom in C++ (ENG)</a>
<br><a data-href="This Keyword in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/this-keyword-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">This Keyword in C++ (ENG)</a>
<br><a data-href="Typename Keyword in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/typename-keyword-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Typename Keyword in C++ (ENG)</a>
<br><a data-href="Using Keyword in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/using-keyword-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Using Keyword in C++ (ENG)</a>


Design Patterns and Principles

<br><a data-href="Command Design Pattern (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/command-design-pattern-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Command Design Pattern (NC)</a>
<br><a data-href="Observer Pattern" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/observer-pattern.html" class="internal-link" target="_self" rel="noopener nofollow">Observer Pattern</a>
<br><a data-href="Singleton Pattern (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/singleton-pattern-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Singleton Pattern (NC)</a>
<br><a data-href="SOLID Design Principles (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/solid-design-principles-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">SOLID Design Principles (NC)</a>


<br><br><br>才疏学浅，笔记内容可能不会过于深入，欢迎批评指正、交流学习。如果有发现任何疑问、错误和错别字问题，欢迎在邮箱 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/congzhi's-c-plus-plus-series.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Congzhi's C Plus Plus Series.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 11 Aug 2025 09:14:29 GMT</pubDate></item><item><title><![CDATA[Const in C++]]></title><description><![CDATA[ 
 <br><br><br>现代C++中，我们会用 const 来明确对某个对象不可修改的语义约束，一般可分为一下几种情况：<br>
<br>定义一个常符号/常量；
<br>传递一个不可修改的参数；
<br>定义常成员函数以防止潜在的修改；
<br>为非成员、非友元的运算符重载函数返回值添加 const；
<br>C++11 之后，对于第一种情况而言，constexpr 通常是更好的选择（编译期语义）。因为 const 最主要的作用其实就是用指针或引用传递参数、返回值时防止修改指令 (modifiable operand) 对参数和返回值的修改。防止修改指令对参数/返回值的修改，实际上就是将潜在的运行时 bugs 转换成了编译时的一些错误。而通常情况下，编译时错误很难被忽略，所以使用 const 可以让我们更好地创建接口 (interfaces) 。<br>最后一种情况用于防止迷惑行为（如果a和b是内建类型，该代码就是非法的）<br>
这点很重要，成员函数在 const 属性上的差异是可以重载的，对于 const 的常对象，为xxx加上 const 就可以避免一些可能的情况（不仅仅是迷惑行为）<br>#include &lt;iostream&gt;
#include &lt;string&gt;

class TextBlock {
public:
    TextBlock(const std::string&amp; text) : data(text) {}
    const char&amp; operator[](size_t position) const {
        std::cout &lt;&lt; "const operator[] called\n";
        return data[position];
    }
    char&amp; operator[](size_t position) {
        std::cout &lt;&lt; "non-const operator[] called\n";
        return data[position];
    }
private:
    std::string data;
};

int main() {
    const TextBlock ctb("hello");
    TextBlock tb("world");
    
    std::cout &lt;&lt; "ctb[0]: " &lt;&lt; ctb[0] &lt;&lt; '\n';
	
	tb[0] = 'W';
	ctb[0] = 'H'; // Error!! This is where the very first const could be very useful. And this error only has to do with the return type of `const char&amp;`
    std::cout &lt;&lt; "tb[0]: " &lt;&lt; tb[0] &lt;&lt; '\n';
}

<br><br>在使用const的时候，我们经常看到：<br>int const number = 123456;
char const msg[] = "hello";
<br>上面的示例中，我们使用const来定义一些不可修改的类型，第一个定义了一个不可修改的int类型，下面定义了一个用const char所组成的array。这些类型的共同点是可读但不可修改（写）。<br><br>这些不可修改的类型只有在初始化的时候给初值，并且C++要求const类型的对象必须初始化。<br>namespace example{
	int const un_initialized_;  // Error: missing initializer.
	extern int const initialized_;  // Ok: this is a declaration, not a def
}
<br><br>
Constexpr is conster than const.
<br>在 C/C++ 中，在 array 中对维度的定义也必须是一个常值，叫做 integer constant expression。<br>int x[n];  // Error
int y[10]; // OK

int const level = 10;
int x[2 * level + 1];  // OK
<br>在定义一个位域长度时也是一样的。<br>int bf: W; // Firld width, W must be constant.
<br>C++允许你将一个非const对象用const来初始化。下面的程序允许你在运行时初始化变量 level，也因此，在编译时的level不是一个const object，对应的表达式也不再是constant expression。<br>int n = 10;
int const level = n;  // Ok
int x[2 * level + 1]; // Error, this is not a constant expression
<br>constexpr 就是在这种情况下诞生的，一个 constexpr 的对象必须用一个 constant expression 来初始化：<br>int n = 10;
constexpr int m = n; // Not ok, n is not a constant expression.
constexpr int l = 10; // Ok, 10 is a const expression.
<br><br>每个对象和函数的声明都有两个部分：declaration specifiers 和 declarator。在下面的例子中前面一长串static unsigned long int都是declaration specifiers，后面的*x[N]是declarator。<br>static unsigned long int *x[N];
<br>Declaration specifiers进一步又分为type specifier（int、unsigned、long等）和non-type specifier（extern、static、inline等）。<br><br>Declarator是一个declarator-id，围绕着一些operators。上面的例子中，x就是这个declarator-id，*和[]都属于operator。这些operators有一定的优先级关系。<br><br>根据优先级关系，我们能够轻易说出后面的*x[N]是一个array of pointers。而我们很容易能够想清楚，(*x)[N]就是一个指针指向一个array of N integers。<br><br>我们提到过，Declaration specifiers进一步又分为type specifier和non-type specifier。它们分别的作用是什么？假如我们有下面的声明：<br>static unsigned long int *x[N];
<br>其中static是修饰declarator-idx的，其他的type specifiers都是修饰其他的type specifier。所以下面的几种表达都是一样的：<br>const unsigned int x;
unsigned int const x;
unsigned const int x;
int const unsigned x;
<br><br>在我们之前学习的过程中，常常会困惑于是指向常量的指针？还是常指针指向一个变量？我们需要注意：const是一个type specifier，而constexpr是一个non-type specifier。<br>constexpr unsigned long int *x[N]; // 指向变量数组的常指针 
const unsigned long int *x[N]; // 指向常量的数组指针
unsigned long int *const x[N]; // 指向变量数组的常指针
<br><br>非const类型可以转换成const类型的变量，但是反过来并不成立。const相当于一种保证，使用变量的函数或表达式（借），可以使用const保证原先的变量（被借）不被改变。但是反过来，如果变量本来就不允许被改变，但是函数或表达式不提供不被改变的保证，那么就会出错。<br>int ver;
const int const_ver = 10;

int add1(const int const_ver, const int ver); // ok
int add2(const int const_ver, int ver); // ok
int add3(int const_ver, int ver); // error
<br>Add a const is ok, but not to lose it.(同样适用于volatile, const-volatile 被称为CV qulifier)]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/const-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Const in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 22 Jul 2025 14:06:19 GMT</pubDate></item><item><title><![CDATA[Consteval in C++ (ENG)]]></title><description><![CDATA[ 
 <br><br>After learning about the constexpr specifier, we have got some understandings about compile-time optimization. In this note, let's explore the consteval specifier.<br>The consteval specifier was introduced in C++20 and it declares a function as an "immediate function", meaning that every call to the function must produce a compile-time constant expression. This enforces that the function is always evaluated at compile-time.<br>You can put constexpr anywhere, but consteval is only allowed in function and function template declarations.<br><br>Our question here is: what's the different between constexpr and consteval? We have one main difference though. The constexpr specifier does not provide a guarantee about function evaluation at compile-time, which means the the machine code may vary depending on the compiler. However, consteval can give you a guarantee, ensuring Compile-Time Function Execution(CTFE).<br>For example, GCC may be strict about CTFE, making every possible constexpr declared function inlined and a constant expression at compile-time. On the other hand, MSVC may be more lenient about CTFE, possibly not performing the work at compile-time. And if the constexpr function do executing at run-time which we would not like to, the compiler won't complain anything about it.<br>Let's dive into an example we are familiar with:<br>#include &lt;iostream&gt;

constexpr int add(int a, int b){
    return a + b;
}

int main() {
    int a = 100; // no constexpr declared this time
    int b = 200; // no constexpr declared this time
    int c = add(a, b); // no constexpr declared this time
    return 0;
}
<br>After compilation, let's see what we get:<br>add(int, int):
        push    rbp
        mov     rbp, rsp
        mov     DWORD PTR [rbp-4], edi
        mov     DWORD PTR [rbp-8], esi
        mov     edx, DWORD PTR [rbp-4]
        mov     eax, DWORD PTR [rbp-8]
        add     eax, edx
        pop     rbp
        ret
main:
        push    rbp
        mov     rbp, rsp
        sub     rsp, 16
        mov     DWORD PTR [rbp-4], 100
        mov     DWORD PTR [rbp-8], 200
        mov     edx, DWORD PTR [rbp-8]
        mov     eax, DWORD PTR [rbp-4]
        mov     esi, edx
        mov     edi, eax
        call    add(int, int)
        mov     DWORD PTR [rbp-12], eax
        mov     eax, 0
        leave
        ret
<br>The add() function is back, which means the function will execute at runtime. This is because the variables a and b are regular variables, while a constant expression evaluation cannot depend on any runtime-modifiable regular variables. Even with constexpr, compiler cannot give you a guarantee about CTFE.<br>But with consteval, you will get a compiler error instead. This is because the function add is declared consteval, but the compiler cannot determine the function value at compile-time with regular values.<br>#include &lt;iostream&gt;

consteval int add(int a, int b){
    return a + b;
}

int main() {
	const int a = 100; // constexpr declaration will also work
	const int b = 200; // constexpr declaration will also work
	int c = add(a, b); // okay
	int c2 = add(a, add(a, b)); // okay
	
    int d = 100; // error, must be constant expression
    int e = 200; // error, must be constant expression
    int f = add(d, e); // error, add(a, b) is not a constant expression
    return 0;
}
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/consteval-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Consteval in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:29:12 GMT</pubDate></item><item><title><![CDATA[Constexpr in C++ (ENG)]]></title><description><![CDATA[ 
 <br><br>The keyword constepxr introduced in C++11 is one of the most impactful updates in the C++'s evolution. Since C++11 introduced the constexpr specifier, there's finally a way for us to write faster and more efficient code. constexpr is const-er than const, not only promise you a constant value and promise you the value would be known during compilation also.<br>The feature unlocks compile-time optimization, a technique that shifts computation from runtime to compile time, which significantly improving runtime performance and reducing overload. For variables and simple expressions, constexpr behaves like what we talked about. But for functions, thing could be getting nuanced, which I'll explain later.<br><br>constexpr objects are const-qualified, and they are guaranteed to be known at compile time. Because their values are determined during compilation, such objects may be placed in the read-only data segment. This feature is particularly beneficial for embedded systems, where memory layout and efficiency are critical. It’s important to note that const does not guarantee a value is known or initialized at compile time — it only ensures that the value cannot be modified after initialization. You can initializedconst objects at runtime.<br>For example: (note here that std::array&lt;&gt;'s size value need to be a constant expression)<br>int i; // unitialized, unknown at compile time
constexpr auto j = i; // error, i's value unknown

std::array&lt;int, j&gt; myArr; // error, j's value unknown

constexpr auto arrSize = 10; // ok
std::array&lt;int, arrSize&gt; myArr1; // ok
std::array&lt;int, 10&gt; myArr2; // ok
<br>And for const:<br>int i;
const auto j = i; // ok

std::array&lt;int, j&gt; myArr; // error

const auto arrSize = 10;
std::array&lt;int, arrSize&gt; myArr1; // error
<br>See, constexpr variables are const, but not vice versa. Therefore, when initializing size parameters for certain STL containers—like std::array—you must use a constant expression, which can only be guaranteed via constexpr.<br><br>Compared to constexpr variables, there aren't many restrictions on constexpr functions. The function context can be known either during compilation or at runtime—both are acceptable. However, only when the function arguments are constexpr can the context be determined at compile time. Otherwise, it will be resolved at runtime.<br>For example:<br>constexpr auto i = 10;
constexpr auto j = 10;
auto k = 10;

constexpr auto add(auto i, auto j) noexcept { return i + j;}

constexpr auto res1 = add(i, j); // ok
constexpr auto res2 = add(i, k); // error
auto res3 = add(i, k); // ok, runtime
<br>And since the constexpr functions really could not throw any exceptions, we typically annotate it with noexcept.<br>And if you make anything wrong (misuse of constexpr), with the constexpr specifier, you can catch errors at compile time, making the code even safer. However, there is a trade-off because performing all calculations at compile-time makes the process compiler-dependent. The result may vary with different compilers.<br>Between C++11 and C++14, the return compile-time results differ. In C++11, a constexpr function is limited to having only one single return statement, meaning branches are not allowed. But in C++14, this restriction was relaxed, which is a good thing for us.<br>For example:<br>// In C++11, you need some branchless tricks:
constexpr bool trueOrFalse(bool cond) noexcept {
	return (cond ? true : false);
}

// After C++14:
constexpr bool trueOrFalse(bool cond) noexcept {
	if(cond) {
		return true;
	} else {
		return false;
	}
}
<br>Although branchless is better, but I am out of energy thinking about it...<br>Let's first go dive in a real example to see how the compile-time optimization happens:<br>#include &lt;iostream&gt;

int add(int a, int b){ // No constant expression
    return a + b;
}

int main() {
    int a = 100;
    int b = 200;    
    int c = add(a, b);
    return 0;
}
<br>This example demonstrates a simple function add that adds two integers. In the main function, we call add with a and b as arguments. Currently, the function does not use constexpr, so the addition is performed at runtime.<br>After compilation:<br>add(int, int):
        push    rbp
        mov     rbp, rsp
        mov     DWORD PTR [rbp-4], edi
        mov     DWORD PTR [rbp-8], esi
        mov     edx, DWORD PTR [rbp-4]
        mov     eax, DWORD PTR [rbp-8]
        add     eax, edx
        pop     rbp
        ret
main:
        push    rbp
        mov     rbp, rsp
        sub     rsp, 16
        mov     DWORD PTR [rbp-4], 100
        mov     DWORD PTR [rbp-8], 200
        mov     edx, DWORD PTR [rbp-8]
        mov     eax, DWORD PTR [rbp-4]
        mov     esi, edx
        mov     edi, eax
        call    add(int, int)
        mov     DWORD PTR [rbp-12], eax
        mov     eax, 0
        leave
        ret
<br>So you can see how many assembly instructions there are needed to be execute in the run-time.<br>With the using of constexpr , our C++ code turns into:<br>#include &lt;iostream&gt;

constexpr int add(int a, int b) noexcept {
    return a + b;
}

int main() {
    constexpr int a = 100;
    constexpr int b = 200;    
    constexpr int c = add(a, b);
    return 0;
}
<br>After compilation without any compiler optimization:<br>main:
        push    rbp
        mov     rbp, rsp
        mov     DWORD PTR [rbp-4], 100
        mov     DWORD PTR [rbp-8], 200
        mov     DWORD PTR [rbp-12], 300
        mov     eax, 0
        pop     rbp
        ret
<br>You see, we don't have the add() function symbol here. Why? You may have guessed it, the constexpr specifier implies inline semantics. <br><br>The previous example might not be very intuitive, now let's do another experiment to see how awesome constexpr can be.<br>#include &lt;iostream&gt;

int fibonacci(int n) {
    return (n &lt;= 1) ? n : (fibonacci(n - 1) + fibonacci(n - 2));
}
int main() {
    int fib10 = fibonacci(10);
    int fib20 = fibonacci(20);
	int fib30 = fibonacci(30);
	int fib40 = fibonacci(40);
    std::cout &lt;&lt; "Fibonacci(10): " &lt;&lt; fib10 &lt;&lt; std::endl;
    std::cout &lt;&lt; "Fibonacci(20): " &lt;&lt; fib20 &lt;&lt; std::endl;
    std::cout &lt;&lt; "Fibonacci(30): " &lt;&lt; fib30 &lt;&lt; std::endl;
	std::cout &lt;&lt; "Fibonacci(40): " &lt;&lt; fib40 &lt;&lt; std::endl;
    return 0;
}
<br>This code calculates the Fibonacci sequence at runtime. The output might look like this:<br>du@DVM:~/cpp$ time ./proc 
Fibonacci(10): 55
Fibonacci(20): 6765
Fibonacci(30): 832040
Fibonacci(40): 102334155

real    0m0.604s
user    0m0.600s
sys     0m0.003s
<br>And with constexpr, our source code look like this:<br>#include &lt;iostream&gt;

// constexpr function to calculate Fibonacci numbers at compile-time
constexpr int fibonacci(int n) noexcept {
    return (n &lt;= 1) ? n : (fibonacci(n - 1) + fibonacci(n - 2));
}

int main() {
    constexpr int fib10 = fibonacci(10);
    constexpr int fib20 = fibonacci(20);
	constexpr int fib30 = fibonacci(30);
    std::cout &lt;&lt; "Fibonacci(10): " &lt;&lt; fib10 &lt;&lt; std::endl;
    std::cout &lt;&lt; "Fibonacci(20): " &lt;&lt; fib20 &lt;&lt; std::endl;
    std::cout &lt;&lt; "Fibonacci(30): " &lt;&lt; fib20 &lt;&lt; std::endl;
    return 0;
}
<br>With the use of constexpr, the output might look like this:<br>du@DVM:~/cpp$ time ./proc 
Fibonacci(10): 55
Fibonacci(20): 6765
Fibonacci(30): 832040
Fibonacci(40): 102334155

real    0m0.005s
user    0m0.003s
sys     0m0.002s
<br>You can see how crazy the difference is—in user time, the optimized version is 200 times faster! This demonstrates the significant impact of compile-time optimization using constexpr.<br><br>Here, we have some points to keep in mind. You must noted the right-hand side of constexpr variable declaration must be a constant expression. Because everything should be figured out at compile-time, thus everything should be known at compile-time. Otherwise, an error will occur.<br>Also, we saw the function declared by constexpr implies an inline semantics, and since C++17, the static data member declared by constexpr would imply inline semantics as well.<br>#include &lt;iostream&gt;
class myClass {
public:
	constexpr static int a = 50;
	// Equivalent to inline const static int a = 50;
	constexpr int a_square();
};
constexpr int myClass::a_square(){ // myClass::a_square would be inlined
	return myClass::a * myClass::a;
}

int main() {

    myClass obj;
    constexpr int i = obj.a_square();
    return 0;
}
<br>And variable/function template now can be declared constexpr too.]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/constexpr-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Constexpr in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 30 Jul 2025 03:18:29 GMT</pubDate></item><item><title><![CDATA[Delegating Constructors in C++ (ENG, NC)]]></title><description><![CDATA[ 
 <br>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/delegating-constructors-in-c++-(eng,-nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Delegating Constructors in C++ (ENG, NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:29:30 GMT</pubDate></item><item><title><![CDATA[Dynamic Casting in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Virtual Dispatch in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Dispatch in C++</a><br><br>works with pointers and refs (you need a vtable and all this), which is more expensive. But it's safe. If bad cast happens, it simply return a nullptr<br>rtti required]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/dynamic-casting-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Dynamic Casting in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 23 May 2025 04:46:39 GMT</pubDate></item><item><title><![CDATA[Effective C++ - Prefer the Compiler to the Processor]]></title><description><![CDATA[ 
 <br>
这是 Scott Effective C++ 中的 Item 2。
<br><br>C/C++ 在生成可执行代码前，需要经过预处理、编译、汇编和链接四个步骤（在 C++ 的标准中，更是将整个过程划分成 9 个阶段）。但无论怎样，预处理总是在编译阶段前。严格来说，编译器是看不到预处理究竟发生了什么的。这也就是为什么 Scott 说："#define may be treated as if it’s not part of the language per se."，由于编译器不知道预处理过程发生了什么，所以预处理宏可以不被看作是语言的一部分。<br>我们假设一个例子：<br>#define PI 3.1415
int main() {
	char c = {PI};
	return 0;
}
<br>预处理后，在编译器眼中的代码其实长这样：<br>int main() {
    char c = {3.1415};
    return 0;
}
<br>因为宏在预处理阶段所做的仅仅就是展开宏，而且展开宏的符号并不会进入符号表中，所以在有 #define 的宏参与时，编译器有可能并不能提供多少有用的信息。我们也看到，经过预处理后的代码已经不含有符号 PI 了。<br>如果这个时候你进行编译，编译器会提示你：<br>clang++: warning: treating 'cpp-output' input as 'c++-cpp-output' when in C++ mode, this behavior is deprecated [-Wdeprecated]
test.cpp:4:15: error: type 'double' cannot be narrowed to 'char' in initializer list [-Wc++11-narrowing]
    4 |     char c = {3.1415};
      |               ^~~~~~
test.cpp:4:15: note: insert an explicit cast to silence this issue
    4 |     char c = {3.1415};
      |               ^~~~~~
      |               static_cast&lt;char&gt;( )
test.cpp:4:15: warning: implicit conversion from 'double' to 'char' changes value from 3.1415 to 3 [-Wliteral-conversion]
    4 |     char c = {3.1415};
      |              ~^~~~~~
1 warning and 1 error generated.
<br>如果这是一个大项目，你就可能纳闷这个 3.1415 是哪里来的。如何让编译的提示保留符号名以便 debug 就是我们要讨论的。<br><br>因为 #define 所定义的符号都具有常量性，为了替换宏的方式， Scott 建议的方案就是使用 const。接着上面的例子，如果将宏替换成 const。如下：<br>const double Pi = 3.1415;
int main() {
    char c = {Pi};
    return 0;
}
<br>作为语言层面的常量定义方式，编译器是肯定能看到 const 修饰的变量名的，所以编译器在进行编译时的报错信息中就会包含具体的符号名，Pi 也理所应当地会被放到符号表中。这是宏所做不到的。<br>但这里需要注意用 const 替换宏时候下面的这两种情况：<br>
<br>
常量指针：虽然 #define 宏和指针扯不上什么联系，但如果宏是用来表示字符串常量（比如：#define NAME "CongZhi"），如果你想用 const 替代，你就不能简单地表示成 const char* Name = "CongZhi" 而是：
const char* const Name = "CongZhi";

这是因为我们不仅仅需要限定字符串内容的不可修改，同时也要限定指向字符串的指针不可修改（即 Name 始终指向字符串常量）。所以你需要写两次 const。
更安全一点的方式是引入 std::string ，同时具备类型安全，而且语义上也更加清晰。
const std::string Name = "CongZhi"


<br>
类内静态常量成员：#define 是没有作用域的，定义的宏在整个翻译单元内都有效，而 C++ 作为支撑封装和作用域的面向对象语言，很多时候我们想把常量限定在某个类的内部。此时，就需要使用 static const 来定义类专属的常量成员。但请注意，由于 C++ 实现和声明分离的特性，一般会要求你在类外的实现文件中才给出静态常量成员的定义。比方说：
// test.hpp
class Test {
private:
	static const float TestingNumber; // Declaration
}
// test.cpp
const float Test::TestingNumber = 3.14; // Definition

凡事都有例外，如果类内的整型静态常量成员只用于编译期常量，且你保证不去访问它，那么多数 C++ 编译器容许你在声明时给出初值，而在实现文件中提供无初值的定义，或直接缺省类外定义。如：
// test.hpp
class GamePlayer {
private:
    static const int MaxLives = 3; // Constant declaration.
    int lives[MaxLives];
};
// test.cpp
// const int GamePlayer::MaxLives; // Definition, no value given.


<br><br>有些老编译器可能不兼容 static const 声明时即给出初值，为了避免类外定义的麻烦，就有了 "the enum hack" 的 trick。如下：<br>	// test.hpp
	class GamePlayer {
	private:
		enum {MaxLives = 3};
	    int lives[MaxLives];
	};
	// test.cpp
<br>实际上这个小 trick 相比较于 const 更像 #define 的行为，而 the enum hack 又比宏更安全。如 the enum hack 和宏一样没有类型、不会占用存储空间（无法取地址）、但拥有作用域且保留有符号。<br><br>除了定义常量的宏，还有一些 function-like 的宏的常见用法。Scott 对于这些 function-like 的宏持批判态度。宏在预处理阶段就展开了，所以使用 function-like 的宏可以避免程序运行时的函数调用，减少开销，这也是宏最大的优点（在 C++11 引入常量表达式之前，宏确实是减小开销最有效的方法）。<br>但同样的，只要引入宏，在 C++ 里就会牵扯到污染命名空间的问题，而且展开后宏名并不会保留到符号表中，带来调试困难的问题。此外，function-like 的宏单是看看就让人头疼，还可能导致多次求值，在 Scott 的例子中：<br>// Compiled with g++ -std=c++23 test.cpp -o test

#include &lt;print&gt;

// call f with the maximum of a and b 
#define CALL_WITH_MAX(a, b) f((a) &gt; (b) ? (a) : (b))

void f(int x) {
	std::println("f({})", x);
}

int main() {
	int a = 5, b = 0;
	CALL_WITH_MAX(++a, b); // a is incremented twice
	std::println("a: {}, b: {}", a, b);
	CALL_WITH_MAX(++a, b+10); // a is incremented once
	std::println("a: {}, b: {}", a, b);
}
<br>在第一次宏展开时，因为 ++a 比 b 大，所以会被 ++ 两次；第二次宏展开时，因为 ++a &lt; b 所以只被 ++ 了一次。<br>为方便理解，宏的展开如下：<br>#include &lt;print&gt; // 这里就不展开了哈～

void f(int x) {
 std::println("f({})", x);
}

int main() {
 int a = 5, b = 0;
 f((++a) &gt; (b) ? (++a) : (b));
 std::println("a: {}, b: {}", a, b);
 f((++a) &gt; (b+10) ? (++a) : (b+10));
 std::println("a: {}, b: {}", a, b);
}
<br>输出如下：<br>f(7)
a: 7, b: 0
f(10)
a: 8, b: 0
f(8)
a: 8, b: 0
<br>这里，Scott 给出用内联函数代替的解决方案。相比于宏，函数的作用域明确，也会提供类型安全检查，还支持许多特性（模板、重载等）。比如上面的例子用内联函数替代就如下所示：<br>// Compiled with g++ -std=c++23 test.cpp -o test

#include &lt;print&gt;

template&lt;typename T&gt;
void f(const T&amp; x) {
    std::println("f({})", x);
}

template&lt;typename T&gt;
inline void callWithMax(const T&amp; a, const T&amp; b) {
    f((a &gt; b) ? a : b);
}

int main() {
    int a = 5, b = 0;

    callWithMax(++a, b);
    std::println("a: {}, b: {}", a, b);

    callWithMax(++a, b + 10);
    std::println("a: {}, b: {}", a, b);
}
<br>虽然内联提供类似宏的函数调用优化语义（消除函数调用开销，避免栈帧创建销毁开销），但内联最主要还是用于解决链接时的多重定义问题。所以内联只是对编译器的一种弱提示 (weak hint)，并不具有强制意义。即使你显式声明了 inline，编译器一般也并不会进行函数内联优化（除非可能发生符号冲突）。相关内容详见 <a data-tooltip-position="top" aria-label="Inline in C++ (ENG)" data-href="Inline in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/inline-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Inline in C++</a> 。<br>所以实际上内联可能并不能解决我们的问题，但 Scott 的出发点总归是好的——尽可能地用语言提供的机制代替预处理器技巧。那么如何真正解决我们的问题呢？更佳安全、且能够减少运行时的函数调用开销的方法何在？我们接着往下看。<br><br>在 C++11 之后，const 家族引入了新的关键字 constexpr, consteval 和 constinit。它们被称为常量表达式，用于表达编译器常量并约束初始化行为，用作函数上，就能给编译器一个强提醒去进行函数内联，避免调用开销。<br>在这里，我们接着本节中所举的例子，实例用常量表达式如何替代。常量表达式 (constexpr) 的使用和 const 几乎一样，但提供编译期化的语义，而 const 只表示值不可修改的语义，这里不赘述。<br>在第一个和第二个例子中，我们可直接将 const 替换成 constexpr。<br>constexpr double Pi = 3.1415;
int main() {
    char c = {Pi};
    return 0;
}
<br>	// test.hpp
	class GamePlayer {
	private:
	    static constexpr int MaxLives = 3; // Constant expression definition.
	    int lives[MaxLives];
	};
	// test.cpp
<br>但是需要注意的是，使用 constexpr 在类内即可给出静态成员的定义，而不需要类外定义，而且任意类型都可初始化。实际上就类似于：<br>	// test.hpp
	class GamePlayer {
	private:
	    inline static const int MaxLives = 3; // Constant definition, C++17
	    int lives[MaxLives];
	};
	// test.cpp
<br>对于函数例子，用 constexpr 替换也非常简单，只需要把 inline 改为 constexpr 即可。不过由于编译期优化的语义，函数的常量表达式需要保证所有的参数都是编译器可知的常量，之前例子显然不符合（++a, b + 10 还有 IO 调用）：<br>不过 constepxr 对编译期的语义并不像 consteval 和 constinit 那样强制，所以即使参数不是常量，替换后函数还是可以正常运行，不过和加不加 constexpr 没什么两样罢了。<br>// Compiled with g++ -std=c++23 test.cpp -o test

#include &lt;print&gt;

template&lt;typename T&gt;
void f(const T&amp; x) {
    std::println("f({})", x);
}

template&lt;typename T&gt;
constexpr void callWithMax(const T&amp; a, const T&amp; b) {
    f((a &gt; b) ? a : b);
}

int main() {
    int a = 5, b = 0;

    callWithMax(++a, b);
    std::println("a: {}, b: {}", a, b);

    callWithMax(++a, b + 10);
    std::println("a: {}, b: {}", a, b);
}
<br>有关常量表达式的详见：<br>
<br><a data-href="Constexpr in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/constexpr-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Constexpr in C++ (ENG)</a>
<br><a data-href="Consteval in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/consteval-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Consteval in C++ (ENG)</a>
<br><a data-href="Constinit in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/constinit-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Constinit in C++ (ENG)</a>
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/effective-c++-prefer-the-compiler-to-the-processor.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Effective C++ - Prefer the Compiler to the Processor.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 21 Jul 2025 05:46:57 GMT</pubDate></item><item><title><![CDATA[Enums in C++ (NC)]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/enums-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Enums in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 30 Jul 2025 03:23:42 GMT</pubDate></item><item><title><![CDATA[Friend Keyword in C++ (ENG)]]></title><description><![CDATA[ 
 <br><br>In C++, we use the friend keyword in the class body to declare a function or a class as a friend to the current class. This means that the friend function or class will have access to the private and protected members of the current class. The declaration syntax is as follows:<br>class MyClass {
    int a, b, c; // private members

    // Friend function declaration
    friend void printMember();

    // Friend class declaration
    friend class AnotherClass;
};
<br>#include &lt;iostream&gt;

class MyClass {
private:
    int a, b, c;

public:
    MyClass(int x, int y, int z) : a(x), b(y), c(z) {}

    // Declare AnotherClass as a friend
    friend class AnotherClass;
};

class AnotherClass {
public:
    void showValues(const MyClass&amp; obj) {
        std::cout &lt;&lt; "a: " &lt;&lt; obj.a &lt;&lt; ", b: " &lt;&lt; obj.b &lt;&lt; ", c: " &lt;&lt; obj.c &lt;&lt; std::endl;
    }
};

int main() {
    MyClass obj(1, 2, 3);
    AnotherClass anotherObj;
    anotherObj.showValues(obj); // Friend class can access private members
    return 0;
}
<br><br>Interestingly, the friendship friend brings is not transitive, which means a friend of your friend is not your friend. Additionally, friendship is not inherited.<br>Using the friend keyword makes private data transparent, which can be convenient, but it breaks the encapsulation of a class. In most cases, your code should not include any friend declarations, as this might indicate poor design.<br>One of the most common use cases for the friend keyword in C++ is in non-member operator overloads. By declaring a non-member operator function as a friend, you grant it access to the private and protected members of your class. This is particularly useful for operator overloading, such as the &lt;&lt; operator for output streams.<br>#include &lt;iostream&gt;

class MyClass {
private:
    int a, b, c; // private members

public:
    MyClass(int x, int y, int z) : a(x), b(y), c(z) {}

    // Friend function declaration for operator&lt;&lt;
    friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const MyClass&amp; obj);
};

// Definition for operator&lt;&lt;
std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const MyClass&amp; obj) {
    os &lt;&lt; "a: " &lt;&lt; obj.a &lt;&lt; ", b: " &lt;&lt; obj.b &lt;&lt; ", c: " &lt;&lt; obj.c;
    return os;
}

int main() {
    MyClass obj(1, 2, 3);
    std::cout &lt;&lt; obj &lt;&lt; std::endl; // Uses the friend function to access private members
    return 0;
}

]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/friend-keyword-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Friend Keyword in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:30:27 GMT</pubDate></item><item><title><![CDATA[Generics Programming in C++ (NC)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Type Deduction - Template Type Deduction" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-template-type-deduction.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Template Type Deduction</a><br><br><br><br>C 语言不支持函数重载(function overloading)，这就意味着我们必须给不同的函数起不同的函数名，这对于完成相同任务但接收不同参数的函数来说非常麻烦。而 C++ 从一开始就支持函数重载，允许代码实现静态多态性。通过传递不同的参数，编译器会调用相应的函数。<br>函数重载当然是一个伟大的特性，但是如果我们要重载许多函数，我们就需要复制粘贴许多遍。但使用函数重载会给我们带来一些潜在问题。除了复制粘贴许多遍之外，我们还可能碰到一些未定义重载函数的情况，比如下面我们使用 long long sum_ll = sum(5342ll, 5864ll); 我们并没有定义相关的函数重载，编译器就不知道调用哪个函数。<br>函数重载当然是一个伟大的特性，但是如果我们要重载许多函数，我们就需要复制粘贴许多遍。但使用函数重载会带来一些潜在问题。除了需要复制粘贴许多遍之外，我们还可能碰到一些未定义重载函数的情况，比如下面我们使用 long long sum_ll = sum(5342ll, 5864ll); 我们并没有定义相关的函数重载，编译器就不知道调用哪个函数。<br>int add(int a, int b){
	return a + b;
}
float add(float a, float b){
	return a + b;
}
double add(double a, double b){
	return a + b;
}
// ...
int main(){
	int sum_i = add(20, 30); // Call int add(int a, int b)
	float sum_f = add(3.14f, 5.58f); // Call float add(float a, float b)
	double sum_d = sum(3.254, 2.546); // Call double add(double a, double b)
	long long sum_ll = sum(5342ll, 5864ll); // Which to call? It's ambiguous!
	return 0;
}
<br>而在 C++98 后，函数模板（泛型函数）的出现避免了多次代码的复制粘贴。模板相当于一个你编写的蓝图，编译器在编译代码的时候会根据你提供的蓝图帮你生成重载函数。从而，你不需要记忆你到底重载了哪些函数。<br>// Compiler will generate function code at compile-time
// And function template is not a function btw
template&lt;typename T&gt;
T add(T a, T b) {
    return a + b;
}
int main() {
    int sum_i = add&lt;int&gt;(20, 30); // Same as add(20, 30) expicitly
    float sum_f = add&lt;float&gt;(3.14f, 5.58f); // Same as add(3.14f, 5.58f)
    double sum_d = add&lt;double&gt;(3.254, 2.546); // Same as add(3.254, 2.546)
    long long sum_ll = add&lt;long long&gt;(5342, 5864); // Same as add(5232ll, 5864ll)
    return 0;
}
<br>上面的例子中，编译器帮我们实例化生成了四个重载函数。在 C++17 后，你实际上可以不再显式地提供模板重载类型，编译器会帮你推导相关的类型。<br><br>每个模板类型都至少被类型推断一次<br>multiple template parameters and non object type params<br>with auto<br>Variadic arguments and Variadic Function Templates<br><br><br><br><br><br>reference collapsing<br><br><br>同样，类模板也不是一个类。它是让编译器帮我们创建类的模板蓝图。<br>STL - you use all the time<br>class myContainer {
public:
	myContainer(int N) {
		m_data = new int[N];
	}
	~myContainer() {
		delete[] m_data;
	}
private:
	int* m_data;
};

class myContainer {
public:
	myContainer(int N) {
		m_data = new float[N];
	}
	~myContainer() {
		delete[] m_data;
	}
private:
	float* m_data;
};
<br>template&lt;typename T&gt;
class myContainer {
public:
	myContainer(int N) {
		m_data = new T[N];
	}
	~myContainer() {
		delete[] m_data;
	}
private:
	T* m_data;
};
<br>还可以提供一个 non-type object<br>#include &lt;cstddef&gt; // For size_t

template&lt;typename T, size_t N&gt;
class myContainer {
public:
    myContainer() {
        m_data = new T[N];
    }
    ~myContainer() {
        delete[] m_data;
    }
private:
    T* m_data;
};

int main() {
    myContainer&lt;int, 5&gt; container;
    return 0;
}
<br><br><br><br>typeid().name()<br><br><br>#include &lt;vector&gt;

// Without alias templates:
typedef std::vector&lt;int&gt; myvec_int; // C++03 alias syntax
typedef std::vector&lt;float&gt; myvec_float; // C++03 alias syntax

// With alias templates:
template&lt;typename T&gt;
using myvec = std::vector&lt;T&gt;; // C++11 syntax

int main(){
    myvec_int vi = {1, 2, 3, 4, 5}; // Using the old alias syntax
    myvec_float vf = {1.1f, 2.2f, 3.3f}; // Using the old alias syntax

    myvec&lt;int&gt; vi_alias = {6, 7, 8, 9, 10}; // Using the new alias template syntax
    myvec&lt;float&gt; vf_alias = {4.4f, 5.5f, 6.6f}; // Using the alias template syntax
    return 0;
}
<br>Alias template cannot be specialized.<br><br><br><br>模板的参数并不必须为一种”类型“：<br>template&lt;class T, size_t N&gt;
class Array{
	T    m_data[N];
	// ...
};

Array&lt;foobar, 10&gt; some_foobars;
<br>NTTPs中的常数类型必须在编译阶段或链接时确定好，而且类型必须为：<br>
<br>整型或枚举（最常见）
<br>指针
<br>std::nullptr_t
<br>其他
<br>Unwatched: <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=VIz6xBvwYd8&amp;t=11s" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=VIz6xBvwYd8&amp;t=11s" target="_blank">CppCon 2016: Arthur O'Dwyer “Template Normal Programming (part 2 of 2)"</a>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/generics-programming-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Generics Programming in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 23 Jul 2025 09:18:00 GMT</pubDate></item><item><title><![CDATA[Google C++ Style Guide]]></title><description><![CDATA[ 
 <br><br>Google C++ 命名规范是 <a data-tooltip-position="top" aria-label="https://google.github.io/styleguide/cppguide.html" rel="noopener nofollow" class="external-link" href="https://google.github.io/styleguide/cppguide.html" target="_blank">Google C++ Style</a> 的一小部分，主要介绍 Google 的 C++ naming convention style。你可以根据项目需求调整命名风格，但遵循统一规范能提升可读性。<br>良好的命名是最能够体现代码风格的行为之一，好的命名能够让你仅通过符号名，你就可以知道这个符号名代表着什么，到底是变量、常量、函数、类？随着代码规模增长，合理的命名习惯有助于提高可读性和维护性，使代码更加清晰易懂。<br>命名的核心作用是提高可读性，所以除了那些广为人知的缩写外，命名应尽量避免缩写的使用。此外，如果现有信息可以简单的得出符号所代表的含义的话，命名应避免不必要的冗长。<br><br>文件名应当采用全小写的 snake case convention，每个单词使用 - 或 - 间隔开。比如：<br>my-file.cpp<br>
my_file.cpp<br>
my_file_test.cpp<br>此外，当命名头文件时，应当避免与已经存在的文件名冲突（ /usr/include 中的头文件）。<br><br>对于局部变量和全局变量的命名，一律使用小写+下划线的方式，也就是 snake case。<br>int my_value;
std::string my_string;
<br>对于类内的成员变量，一律在变量名后加上 _ ，如：<br>class MyClass {
private:
	int class_value_;
	std::string string_name_;
};
<br>无论是类外的静态变量还是类内的静态变量，其命名都遵循全局变量的命名方式。<br>如果变量名是常量，则以 k 开头，遵循 camel case convention：<br>static const int kMyInteger = 10;
const std::string kMyString = "Hello";
<br><br>对于类类型的命名，一律使用大写驼峰命名方式（即 PascalCase），第一个字母大写的驼峰命名法：<br>class MyClass {};
struct MyData {};
union MyUnion {};
<br><br>函数命名同样适用类似类类型的命名方式——大写驼峰命名。如：<br>int GetUserId() {}
void PrintData() {}
<br><br>命名空间的命名采用 snake case convention。<br><br>采用大写驼峰命名，如：<br>enum ThreadState {
	JOINABLE,
	DETACHED,
	FINISHED,
	UNCREATED	
};
<br><br>宏使用全大写+下划线的方式，如：<br>#define MAX_BUFFER_SIZE 1024
<br><br>模板参数使用 T 开头的大写驼峰命名：<br>template &lt;typename TData&gt;
class Container {};
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/google-c++-style-guide.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Google C++ Style Guide.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 10 Jun 2025 14:20:42 GMT</pubDate></item><item><title><![CDATA[GPU Programming in C++ (NC)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="What is CUDA" href="https://congzhi.wiki/some-notes/what-is-cuda.html" class="internal-link" target="_self" rel="noopener nofollow">What is CUDA</a>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/gpu-programming-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/GPU Programming in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 26 May 2025 22:22:15 GMT</pubDate></item><item><title><![CDATA[Inheritance in C++]]></title><description><![CDATA[ 
 <br><br>继承是面向对象技术得以发扬光大的原因之一。本节课，我们来学习 C++ 中最基础的继承知识，简单了解一下继承是什么。我们一般将被继承的类称为基类（也称为父类、祖先类），将继承其他类的类称为派生类（也称为子类、扩展类）。下面用一个例子展示这种关系：<br>class Base{ // parent class or ancestor class
// Some implementation
};
class Derived : public Base{
// Some implementation
};
<br>一个类可以被继承和派生，正如上面基类 Base 和派生类 Derived 的关系。这里的继承可以理解为派生类接收到来自基类的传承，脉脉相承就是继承最主要的思想。类与类之间有两种关系："is-a" 和 "has-a"，请不要混淆，我们下面就来解释这两种关系。<br><br>我们所说的继承是 "is-a" 的关系，也就是“是一种”的关系。而 "has-a" 表示的是“有一个”的关系，即“composition”。我们用下面的示例来直观地感受这两种关系：<br>class Widget {};
class Base {};
class Derived : public Base { // class Derived is-a Base
    // Class data member
    Widget w; // class Derived has-a Widget
};
<br>在这个例子中，我们说派生类 Derived 继承了基类 Base，所以 Derived 是一种 Base。而派生类内有个 Widget 成员，所以我们说派生类 Derived 中有一个 Widget，也就是 "has-a" 的关系。<br>And by the way, we also have another 'has-a' relationship called aggregation, which has a weaker relationship than compositional 'has-a'. For example, you may say a car has an engine, this is compositional 'has-a'. And also, a school could have many students, this is aggregational 'has-a'.<br><br>通过继承，你能得到什么呢？我们说派生类 is-a 基类。那么，我们是否可以将派生类中的一些共性提炼出来放在基类中？实际上，我们的确就是这么做的。通过继承，派生类重用基类的代码，减少了代码的重复，提高了代码的可维护性。在功能扩展时，只需要让派生类在基类的基础上增加新的功能。<br>此外，继承使得基类提供不同的接口，使得派生类可以通过相同的接口做出不同的实现，这被称为多态（或动态多态）。这与我们在 <a data-tooltip-position="top" aria-label="Static Dispatch in C++ (ENG)" data-href="Static Dispatch in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/static-dispatch-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">静态多态</a> 中介绍的“假”多态不同。多态的出现可以使代码在运行时通过不同的接口调用不同的派生类方法。我们将在 <a data-href="Virtual Dispatch in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Dispatch in C++</a> 介绍这种动态多态。 <br><br>在 C++ 中，我们有三种不同的继承访问控制方式： public、protected 和 private。不同的继承访问方式决定了派生类中对基类成员的不同访问权限。我们用一个例子来展示这三种不同的继承访问控制方式的作用：<br>class Base {
public:
    int pub_i; // Public members are accessible from outside
protected:
    int prot_i; // Protected members are accessible to derived classes
private: 
    int pri_i; // Private members are only accessible within Base class
};

class Pub_Derived : public Base {
public:
    void accessBase() {
        pub_i = 1; // Accessible
        prot_i = 2; // Accessible
        // pri_i = 3; // Not accessible
    }
};

class Prot_Derived : protected Base {
public:
    void accessBase() {
        pub_i = 1; // Accessible
        prot_i = 2; // Accessible
        // pri_i = 3; // Not accessible
    }
};

class Pri_Derived : private Base {
public:
    void accessBase() {
        pub_i = 1; // Accessible
        prot_i = 2; // Accessible
        // pri_i = 3; // Not accessible
    }
};

int main() {
    Pub_Derived pubDerived;
    pubDerived.pub_i = 1; // Accessible from outside
    // pubDerived.prot_i = 2; // Not accessible from outside
    // pubDerived.pri_i = 3; // Not accessible from outside
    
    Prot_Derived protDerived;
    // protDerived.pub_i = 1; // Not accessible from outside
    // protDerived.prot_i = 2; // Not accessible from outside
    // protDerived.pri_i = 3; // Not accessible from outside

    Pri_Derived priDerived;
    // priDerived.pub_i = 1; // Not accessible from outside
    // priDerived.prot_i = 2; // Not accessible from outside
    // priDerived.pri_i = 3; // Not accessible from outside
    return 0;
}
<br>从例子中，你可以看到这三种不同的继承访问控制方式的作用。我们发现，无论是哪种继承关系，派生类都不能访问基类中的 private 成员。对于 public 继承，基类中的 public 和 protected 成员在派生类中的访问权限不变。对于 protected 继承，基类中的 public 成员在派生类中的访问权限会变为 protected。对于 private 继承，基类中所有类型的成员在派生类中都会变为 private 成员。<br><img alt="Pasted image 20250225102217.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250225102217.png"><br><br>
<br>构造顺序：基类 → 派生类成员变量（按声明顺序）→ 派生类构造函数体。
<br>析构顺序：派生类析构函数体 → 派生类成员变量（逆声明顺序）→ 基类析构函数
<br>class Base {
public:
    Base() { std::cout &lt;&lt; "Base constructed\n"; }
    ~Base() { std::cout &lt;&lt; "Base destroyed\n"; }
};

class Derived : public Base {
public:
    Derived() { std::cout &lt;&lt; "Derived constructed\n"; }
    ~Derived() { std::cout &lt;&lt; "Derived destroyed\n"; }
};

int main() {
    Derived d;
}
<br><br>当有多个不同的基类构造函数时，派生类需要在构造函数初始化列表中显式指定要调用的基类构造函数，不然编译器尝试调用基类默认构造函数。<br>class Base {
public:
    Base() { std::cout &lt;&lt; "Base()\n"; }
    Base(int x) { std::cout &lt;&lt; "Base(" &lt;&lt; x &lt;&lt; ")\n"; }
};

class Derived : public Base {
public:
    Derived() { std::cout &lt;&lt; "Derived()\n"; }
    Derived(int x) : Base(x) { std::cout &lt;&lt; "Derived(int)\n"; }

    Derived(int x, double y) : Base(x), y_(y) { 
        std::cout &lt;&lt; "Derived(int, double)\n"; 
    }
private:
    double y_;
};

int main() {
    Derived d1;        // Base() → Derived()
    Derived d2(10);    // Base(10) → Derived(int)
    Derived d3(5, 3.14);// Base(5) → Derived(int, double)
}
<br><br>int x = 0;
void Func() {
	double x = 10;
	std::cout &lt;&lt; x;
}
<br>如果我们在派生类中定义了与基类同名的非虚函数，无论该函数的参数列表是否相同，基类中的函数都会被隐藏，而不是简简单单的覆盖（override）。这种隐藏现象是名称级别的，即基类的同名函数被“隐藏”。因而，你不能直接通过派生类对象调用基类的同名函数。<br>#include &lt;iostream&gt;
class Base {
public:
    void func() {
        std::cout &lt;&lt; "Base::func()" &lt;&lt; std::endl;
    }
};

class Derived : public Base {
public:
    void func(int x) {
        std::cout &lt;&lt; "Derived::func(int)" &lt;&lt; std::endl;
    }
};

int main() {
    Derived d;
    d.func(42);  // call Derived::func(int)
    // d.func(); // error, because Base::func() was hiden
    d.Base::func(); // call Base::func()
    return 0;
}
<br>上面的例子中，如果你想重写基类中的同名函数，你就需要用到虚函数，我们会在<a data-tooltip-position="top" aria-label="Virtual Dispatch in C++" data-href="Virtual Dispatch in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">虚多态</a>介绍这种机制，这是 C++ 中多态实现的方式。<br>同样，同名变量（内部的变量也会hide外部的变量（顺序反了）]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/inheritance-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Inheritance in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 11 Aug 2025 10:33:44 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250225102217.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250225102217.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Inheritance of Interface and Inheritance of Implementation in C++ (NC)]]></title><description><![CDATA[ 
 <br>C++ 的继承满足你对自由的向往]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/inheritance-of-interface-and-inheritance-of-implementation-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Inheritance of Interface and Inheritance of Implementation in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 11 Aug 2025 11:42:13 GMT</pubDate></item><item><title><![CDATA[Inline in C++ (ENG)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Call Stack in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/call-stack-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Call Stack in C++ (ENG)</a><br><br>When a function is called, typically, a stack frame is established. To build the frame, the machine needs to do a lot of work, including saving register values, copying parameters, saving the return address, and keeping track of rbp and rsp. After that, the function does its work.<br>The inline specifier simply tells the compiler to not build this frame, effectively "unboxing" the function code and adding it directly into the caller's frame. This can provide several benefits, such as reducing function call overhead (thus improving performance) and slightly improving readability.<br>Although the inline specifier declares the function to be inlined, it serves merely just as a suggestion to the compiler. If the compiler finds no need to be inlined, it may not take the suggestion. As a result, in most cases, the inline specifier might not be applied at all.<br>With GCC optimizations, the compiler can often eliminate the stack frame even without the use of the inline specifier, which is funny.<br><br>The inline specifier is particularly useful when dealing with external linkage, whether for inline functions or inline variables (since C++17). The use of inline helps prevent linker errors caused by naming conflicts, which would otherwise violate the One Definition Rule (ODR). This is because inline functions and variables can be defined in multiple translation units without causing conflicts during linking.<br>To comply with the ODR, the inline specifier comes into play. In the example below, the compiler ensures that the rule is followed by inlining the test function in the test.cpp file during the compilation stage, replacing function calls with the actual function code in the assembly output.<br>// test.cpp
#include &lt;cstdio&gt;
inline void test() { // This function will not exist in its original form after compilation.
// The compiler will replace calls to this inline function with the actual code during compilation.
    printf("This is an inlined hello in %s\n", __FILE__);
}

void printsomething() {
    test(); // Calls the inline version of the test function defined in this file.
}

// main.cpp
#include &lt;cstdio&gt;
extern void printsomething(); // Declares the external function implemented in test.cpp.
void test() { // This is a regular (non-inline) function definition.
    printf("This is a hello in %s\n", __FILE__);
}

int main() {
    test(); // Calls the test function defined in main.cpp.
    printsomething(); // Calls the printsomething function from test.cpp, which, in turn, calls the inline test function from test.cpp.
    return 0;
}
<br>After compilation, these inline functions and variables are directly inserted into the assembly code at their call sites. Consequently, they do not have separate names in the generated object files, thereby avoiding potential conflicts. However, the inline functions or variables must still be declared in every translation unit where they are used.<br>Note: A function declared constexpr or consteval in its first declaration is implicitly inline. Similarly, a static data member declared constexpr in its first declaration is implicitly an inline variable.<br><br>Inline variables are a feature introduced in C++17, which allows variables to be declared with the inline specifier. Inline variables allow variables to be dealt with external linkage like inline functions.<br>Besides, inline variables are also very useful when dealing with header-only libraries. Some design specifics dictate that the interface (.h/.hpp) should be separate from the implementation (.c/.cpp). Inside a class, when we declare a static variable or function, we are traditionally told not to put the definition (initialization) inside the class, as it is forbidden. However, you can do so if you are using the inline specifier.<br>Old way:<br>// .hpp file
class myLib{
	static int counter; // Declareation
	// Other data member
public:
    // Other member functions and definitions
};
<br>// .cpp file
int myLib::counter = 0;
// some other definition and implementation
<br>Header-only library way:<br>// header_only_library.hpp
// This hpp file contains all the declarations and definitions
// An example to "class as a library"

class myLib {
    static int counter_1 = 0; // Not allowed, definition must be outside the class
    const static int counter_2 = 0; // Okay, but requires constexpr for inline
    const static int counter_3; // Not allowed, must be initialized (defined)
    inline static int counter_4 = 0; // Okay, inline allows definition inside the class
    inline static int counter_5; // Also okay, will be default initialized to 0
    constexpr static int counter_6 = 0; // Okay, constexpr implies inline
    inline constexpr static int counter_7 = 0; // Same as above

    // Other data members

public:
    // Other member functions and definitions
};
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/inline-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Inline in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 16 Aug 2025 09:11:38 GMT</pubDate></item><item><title><![CDATA[Integer Literal & Float Literal in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Type Deduction - Auto in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-auto-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Auto in C++</a><br><br>在使用 auto 关键字时，我们发现 auto 往往会将整数字面量推导成 int 类型。如下，我们给 a 赋值 0 远远小于 int 能够表示的最大值。既然 short 类型也能表示 0 ，甚至 char 类型也可以。那为什么 auto 不将这个表达式中的 a 推导成 short 类型，甚至 char 类型？<br>auto a = 0; // a is deduced as int type, why not short or char?
<br>这是因为在 C++ 中，整数字面量（例如 0）的默认类型是 int。编译器会优先选择最适合的类型来表示这个字面量，因此在这种情况下，auto 会推导出 int 类型，而不是其他的类型。<br><br>在 C++ 中，整数字面量可以是十进制的、八进制的、十六进制的和二进制的（C++14）。通过加入不同的前缀，我们就可以改变整数的基数。如下：<br>int decimal = 16;
int octal = 020; // 16 on decimal
int hexdecimal = 0x10; // or 0X10
int binary = 0b10000;
<br><br>当整数字面量不带任何后缀时，一般默认的类型就是 int 类型。C++ 整型字面量一共有这几种不同的后缀（基数为10）：<br>
<br>u 或 U：表示 unsigned int （默认）
<br>l 或 L：表示 long int （默认）
<br>ul / UL 等 ：表示 unsigned long int （默认）
<br>ll 或 LL：long long int
<br>ull 或 ULL 等：表示 unsigned long long int
<br>z 或 Z：表示 std::size 类型（C++23）
<br>uz 或 UZ：同上。
<br>auto a = 0; // a is deduced as type int
auto b = 0u; // b is deduced as type unsigned int
auto c = 0l; // c is deduced as type long
auto d = 0ul; // d is deduced as type unsigned long
auto e = 0ll; // e is deduced as type long long
auto f = 0ull; // f is deduced as type unsigned long long
<br>当字面量很长时，你可以用单引号作为分隔符在任意位置将字面量分隔开。在确定字面量时会忽略这些单引号。<br><br>同样的，浮点字面量也可以通过后缀来决定字面量的类型。当没有后缀时，默认定义为 double 类型。对于浮点数字面量，C++ 有以下的后缀：<br>
<br>f 或 F：表示 float
<br>l 或 L：表示 long double
<br>自 C++23 起，C++ 还引入了 f16、f32、f64、f128、bf16、F16、F32、F64、F128、BF16 的后缀。
<br>auto a = 0.0; // a is deduced as type double
auto b = 0.0f; // b is deduced as type float
auto c = 0.0l; // c is deduced as type long double
<br>下一节：<a data-href="Character Literal &amp; String Literal in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/character-literal-&amp;-string-literal-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Character Literal &amp; String Literal in C++</a>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/integer-literal-&amp;-float-literal-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Integer Literal &amp; Float Literal in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 05 Aug 2025 13:21:22 GMT</pubDate></item><item><title><![CDATA[Know Your Hardware - Modern CPU Architecture]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="ISA, Instructions and CPU" href="https://congzhi.wiki/some-notes/isa,-instructions-and-cpu.html" class="internal-link" target="_self" rel="noopener nofollow">ISA, Instructions and CPU</a>、<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=o_WXTRS2qTY" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=o_WXTRS2qTY" target="_blank">Architecture All Access - Intel</a>、<a data-href="Modern CPU Evolution (ENG)" href="https://congzhi.wiki/computer-architecture/modern-cpu-evolution-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Modern CPU Evolution (ENG)</a><br><br>CPU 是整个计算机系统的核心，它负责接收数据、执行运算、并输出结果。传统上，CPU 就是处理器，处理器就是 CPU，CPU 也简单地分为算术逻辑单元 ALU 和控制单元 CU。其中 ALU 负责执行算术和逻辑运算，CU 负责指令的解码，执行流程控制以及协调处理器内部的各个组件。<br>但随着半导体技术和计算架构 (computing architecture) 数十载的发展，CPU 早已突破课本中传统的 ALU+CU 的简单结构，形成了复杂的架构，包含更多的模块。广义上，如今的 CPU 不仅仅包含 CPU 核心 (CPU cores)，还把内存管理、IO管理、图形显示（集成显卡）都集成在一颗芯片上，我们把这种设计称为片上系统 (System-on-Chip, Soc)。<br>在 SoC 结构中，CPU 核心仍然是整个系统的中心，而且当代 CPU 微架构除了传统上的 ALU 和 CU 外，还集成许多关键模块一提升计算效率。我们会在本节简单地看看现代 CPU 微架构上什么样子。<br><br>本节，我们会简单的了解一下现代 CPU 的流水线技术、分支预测器、缓存、超标量计算和指令的乱序执行。<br><br>我们知道，高级语言源程序是没有办法在机器（CPU）上运行的，CPU 能读懂的只有用 01 表示的机器语言代码，所以运行源文本程序前我们有一步编译操作。<br>你可以把 CPU 想象成一个小机器人，你传递给它一条机器语言指令并期望得到它的回复。每次在回复你传递的每一条指令前，小机器人会完成一系列的操作，包括：取指令 (Fetch)、解码指令 (Decode)、执行微指令 (Execute) 和回写结果 (Write Back)。<br>在取指令阶段，CPU 这个小机器人会从缓存或内存中接收读取机器指令并将其存放在一个指令寄存器.(Instruction Register) 中。随后的解码指令阶段，CPU 这个小机器人可能会接收一条或多条指令并输出一条或多条微指令（一般为 1 对 1）。微指令 (Micro-Operations, μOps) 是 CPU 执行阶段能够直接执行的基本操作。<br>将解码完成机器指令后，CPU 就开始了执行阶段，这个阶段中，CPU 会执行这些微操作。在 CPU 这个黑匣子里面的计算任务完成后，它还需要把结果存入寄存器供后续指令使用或是存回内存反馈给我们，这是指令周期的最后一步——回写阶段。<br><br>如果我们有许多要执行的机器指令，现代 CPU 并不会逐条地执行指令，而是以一种流水线的方式并行化指令执行，这就是指令流水线 (pipeline)。就像生产流水线一样，前面的工人正在组装零件，后面的工人已经开始测试产品。指令流水线技术使得一个 CPU 核心可以同时执行多条不同阶段的指令，可以显著地提高吞吐量。<br>随着时间的推移，流水线的阶段已不仅仅是指令周期的四个阶段了，每个阶段（如取指令阶段）还会进一步的拆分成更小的子阶段，用于更好的提高 CPU 的吞吐量，现代 CPU 通常采用深流水线设计 (deep pipeline depth)，深度通常为 15-20 级。<br>流水线级数的增加源于 CPU 主频的停滞不前，因为 CPU 是时钟驱动的，一个 stage 到另一个 stage 需要一个时钟周期。而深流水线设计可以让每一个时钟周期同时执行 15-20 条指令，也就是即使 CPU 的频率恒定，IPC 也能够提升。<br>在深流水线中，取指令和解码阶段通常包含 6-10 个阶段，因为这两个阶段位于 CPU 流水线的前端，所以 CPU 中负责处理这两个阶段的 infra 也被称作处理器前端 (front-end)。执行和写回阶段也通常包含 6-10 个阶段，负责这两个阶段的处理器 infra 被称为后端 (back-end)。<br>但这种深流水线的设计还可能导致性能下降，比如说当 CPU 遇到判断语句的时候，<br><br>前面提到了 CPU 级数增多可能导致性能下降，这是什么意思？在 CPU 指令流水线中，最开始前端会从内存或缓存中  fetch 一条指令，经过 decode 并在 execute 阶段执行这条指令。然而，ISA 给我们提供了一些跳转指令 (branches)。当 CPU 在 execute 阶段碰到 branches 时，它所面临的就是一个双叉路口，就相当于：<br>if condition_is_true:
	this_branch_of_code
else:
	another_branch_of_code
<br>这些跳转指令决定了程序的执行路径。在早期的流水线处理器中，由于没有分支预测机制，fetch 阶段在遇到跳转指令时，通常会等待后端 execute 阶段处理的结果。这种设计会导致流水线停顿 (pipeline stall)，直到 execute 阶段明确跳转目标。如果代码中包含大量跳转指令，CPU 的性能就会严重受损。（The deeper the pipeline stage depth, the greater the penalty you incur）<br>为了优化这种等待问题，现代处理器微架构引入了分支预测和投机执行（也就是 prefetch）。分支预测由一个前端部件——分支预测器提供，用于判断程序的执行分支路径，减少流水线停顿，通常情况下，准确率可以达到 90% 以上。<br>投机执行就是在分支预测的基础上，CPU 提前执行预测路径上的指令。如果预测正确，那么计算结果直接使用，如果预测错误，那么丢弃前面流水线阶段中的计算结果，重新执行正确的路径。同时更新分支预测器的算法。所以分支预测错误的惩罚实际上是很高的，我们将错误预测的情况称为分支预测错误惩罚 (Branch Misprediction Penalty)。<br>尽管平均 90% 的分支预测准确率看上去已经很高了，但实际上这个准确率是很差的，对于想要优化性能的软件工程师，他们会考虑尽量剔除程序中有关分支的代码，也就是无分支编程，尽可能的将分支预测率提高到 99.9% 以上。详见：<a data-href="Branchless Programming Intro in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/branchless-programming-intro-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Branchless Programming Intro in C++ (ENG)</a>。<br><br>缓存技术，用于解决 CPU 和内存速度不匹配的问题。现代 CPU 的运行速度远远快于主存（差距达百倍）。通常 CPU 的时钟周期是纳秒级别的，而内存的访问延迟可能是几十到几百纳秒，这种情况下，对内存的访问就会成为性能瓶颈。<br>Cache 是 CPU 前端和内存之间的高速存储层，用于临时存储经常性访问的数据。现代 CPU 会采用多级缓存来优化访问速度，而且缓存会预取 (prefetch) CPU 前端可能 fetch 的指令数据，进一步减少访问延迟。<br>在 fetch 阶段，CPU 会尝试先从 cache 中读取指令和数据，如果 cache 命中（数据和指令在缓存中）则 CPU 无需访问主存。若未命中，CPU 就需要访问 RAM 并把指令和数据加载到缓存中，以便后续访问。现代计算机系统的 cache 命中率通常能够达到 95% 以上。<br><br>ALU 负责一些微指令的执行，在过去（如 8086 ），每个时钟周期只能按顺序执行一条指令。我们把这种指令执行方式叫做标量执行。随着 CPU 进入微架构时代，每个 CPU 时钟周期都可以执行多条指令，这主要依赖于现代 CPU 核心后端的许多 ALU。此外，随着 CPU 核心数的增加，处理器可以并行地执行更多指令，从而显著提高吞吐量。所有的现代 CPU 都是超标量的。<br>由于每个核心都有许多个 ALU，所以每次 CPU 后端进行计算时，都可以同时（并行地）进行多个算数逻辑运算，在现代 CPU 眼中，这两段代码所耗费的时间是一样的：<br>...
std::vector&lt;int&gt; vec1[N], vec2[N];
... // Initilization
int a = 0;

for(size_t i = 0; i &lt; N; ++i) {
	a = vec1[i] + vec2[i];
}
<br>...
std::vector&lt;int&gt; vec1[N], vec2[N];
... // Initilization
int a = 0, b = 0, c = 0, d = 0;

for(size_t i = 0; i &lt; N; ++i) {
	a = vec1[i] + vec2[i];
	b = vec1[i] - vec2[i];
	c = vec1[i] * vec2[i];
	d = vec1[i] / vec2[i];
	... // Even more!!
}
<br>上面的实例中并不存在数据依赖（比如要得到 b，我们先得求出 a），而在大多数的情况下，我们的程序都难免存在数据依赖。为了最大化地利用后端的执行部件，并保证后端的执行部件的并行执行不会影响运算结果，现代 CPU 会采用乱序执行来优化指令调度，让指令不必按照编写顺序执行，而是根据数据可用性和执行单元状态进行调度。假如我们有：<br>...
std::vector&lt;int&gt; vec1[N], vec2[N];
... // Initilization
int a = 0, b = 0, c = 0, d = 0, e = 0, f = 0, g = 0, h = 0;

for(size_t i = 0; i &lt; N; ++i) {
	a = vec1[i] + vec2[i];
	e += a;
	b = vec1[i] - vec2[i];
	f += b;
	c = vec1[i] * vec2[i];
	g += c;
	d = vec1[i] / vec2[i];
	h += d;
	... // Even more!!
}
<br>在指令调度后，CPU 后端会先执行：<br>	a = vec1[i] + vec2[i];
	b = vec1[i] - vec2[i];
	c = vec1[i] * vec2[i];
	d = vec1[i] / vec2[i];
<br>随后执行：<br>	e += a;
	f += b;
	g += c;
	h += d;
<br>在指令乱序执行的过程中，后端涉及到的组件有：重排序缓冲区 (Reorder Buffer, ROB), 调度器 (Scheduler), 储存缓存器 (Store) 以及缓存子系统 (包括 L1 缓存、L2 缓存和相关的队列)。<br>ROB 负责跟踪所有正在执行的指令状态，确保指令的退休严格按照原始程序的顺序进行。当某条指令完成，而且这条指令在 ROB 中是当前 latest 的指令（其他的所有指令都已完成），那么该指令就可以退休 (retire)，表示该指令的计算结果可以正式的生效。<br>调度器负责监视已分配的指令的操作数是否已经就绪并检查对应的 ALU 是否空闲，一旦指令操作数就绪的同时 ALU 可用，那么调度器就会把指令分派给对应的 ALU 乱序执行。<br>存储缓存器用于缓存已退休但未写入缓存的存储操作，因为对缓存/内存的写入是不可逆的，为了保证最后的写入是一致性的，我们就需要在存储缓存器中对这些存储操作进行排序。<br>最后写入缓存子系统或是内存中。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/know-your-hardware-modern-cpu-architecture.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Know Your Hardware - Modern CPU Architecture.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 24 Jul 2025 14:48:14 GMT</pubDate></item><item><title><![CDATA[Lambdas in C++]]></title><description><![CDATA[ 
 <br><br>我们将函数视为对黑匣子内部操作的抽象封装。函数隐藏了内部细节，只向外界暴露其提供的接口。而在内存中，这个黑匣子需要占用存储空间（.text&nbsp;段）。因此，函数是有地址的。函数地址就是指向内存中‘黑匣子’所存放的起始地址，而函数指针则是指向函数地址的指针。<br>因为函数可以取地址，所以我们可以将函数指针作为参数去传递。我们下面举例：<br>#include &lt;iostream&gt;
void hello(){
	std::cout &lt;&lt; "Hello" &lt;&lt; std::endl;
}
void test(void(*ptr)()){
    ptr();
}
int main(){
	auto funcPtr = hello; // same as `void(*funcPtr)() = hello;`
	test(funcPtr); // passing function pointer as a parameter
    funcPtr(); // call hello() using function pointer
}
<br>例二：<br>#include &lt;iostream&gt;
#include &lt;vector&gt;

void PrintValue(int value){
	std::cout &lt;&lt; "Value: " &lt;&lt; value &lt;&lt; std::endl;
}
void ForEach(const std::vector&lt;int&gt;&amp; values, void(*print)(int)){
	for(int value : values)
		print(value);
}

int main(){
	std::vector&lt;int&gt; values = {1, 2, 3, 5, 9};
	ForEach(values, PrintValue);
	return 0;
}
<br>例二使用模板和lambda表达式后，我们能够优雅地遍历打印基本类型的vector。从这里我们就能够感受到lambda表达式的便利性，因为你不需要单独定义一个函数了，简化了代码。<br>#include &lt;iostream&gt;
#include &lt;vector&gt;

template&lt;typename T&gt;
void ForEach(const std::vector&lt;T&gt;&amp; values, void(*funcPtr)(T))
{
    for (const T&amp; value : values)
        print(value);
}
int main()
{
    std::vector&lt;int&gt; values = {1, 2, 3, 5, 9};
    ForEach(values, [](int value){std::cout &lt;&lt; "Value: " &lt;&lt; value &lt;&lt; std::endl;});
    return 0;
}
<br><br><br>仿函数，也叫函数对象(function object)，是可以像函数一样被调用的对象，常常用重载运算符 operator() 实现。我们用一个例子来说明为什么被称为仿函数：<br>#include &lt;iostream&gt;
struct Increment
{
    int number;
    Increment(int n) : number(n) {}
    int operator()(int x) {
        number += x;
        return number;
    }
};

int main() {
    Increment inc(5);
    std::cout &lt;&lt; inc(15) &lt;&lt; std::endl; // call functor just like functions
    return 0;
}
<br>由于重载了 operator()，仿函数的调用能够像调用函数一样方便自然。而且通过类内的变量 number 能够保留每次被调用后的状态信息，这时函数所不能够做到的。仿函数结合了类的状态管理和函数的调用方式，提供了比普通函数更高的灵活性和功能。<br><br>#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;

class Print {
public:
    void operator()(int x) const {
        std::cout &lt;&lt; "Value: " &lt;&lt; x &lt;&lt; std::endl;
    }
};
int main() {
    std::vector&lt;int&gt; values = {1, 2, 3, 4, 5};
    std::for_each(values.begin(), values.end(), Print());
    return 0;
}
<br><br>如果刚刚接触Lambdas，你一定会困惑于如此多的括号。在Lambda中，我们有()、[]、{}。如果带有参数模板你甚至还会看到&lt;&gt;。我们先看看这些Lambda表达式。<br><br><br>
Lambda constructs a closure : an unnamed function object capable of capturing variables in scope.
<br>Lambda表达式构造了一个闭包，在3.2节中我们将会学到（其中[]叫做捕获列表）。通过Lambda表达式，我们实际上能够定义一个具有捕获作用域内变量能力的匿名函数对象。什么意思呢？即在仿函数（函数对象）的基础上，还加入了捕获当前栈帧中局部变量的能力。<br>虽然匿名函数对象虽然是匿名的，但我们仍然可以让一个函数指针指向我们的 Lambda 表达式，从而达到 Store the Lambda 的目的。在下面的例子中，Lambda 表达式被转换为函数指针&nbsp;Print，并且我们可以通过这个指针调用 Lambda 表达式，实现存储和调用。<br>#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;

int main() {
	int i = 5;
	double j = 6;
	char c = 's';
    //auto Print = [=](int v){
    //    std::cout &lt;&lt; "value:" &lt;&lt; v &lt;&lt; std::endl;
    //};
    std::vector&lt;int&gt; values = {1, 2, 3, 4, 5};
    std::for_each(values.begin(), values.end(), 
					[=](int v){
					    std::cout &lt;&lt; "value:" &lt;&lt; v &lt;&lt; std::endl;
					}); 
    // std::for_each(values.begin(), values.end(), Print); 
    return 0;
}
<br>如果我们想看看 Lambda 背后的仿函数真身是什么，接着上面的例子，我们会看到编译器实际上生成了一个仿函数，捕获的成员变量会作为仿函数类中的私有成员变量存储。编译器生成的代码大概是这样的：<br>#include &lt;iostream&gt;
#include &lt;vector&gt;
#include &lt;algorithm&gt;

int main()
{
  int i = 5;
  double j = 6;
  char c = 's';
  std::vector&lt;int, std::allocator&lt;int&gt; &gt; values = std::vector&lt;int, std::allocator&lt;int&gt; &gt;{std::initializer_list&lt;int&gt;{1, 2, 3, 4, 5}, std::allocator&lt;int&gt;()};
    
  class __lambda_14_6
  {
    public: 
    __lambda_14_6(int i, double j, char c) : i(i), j(j), c(c) {}
    
    inline /*constexpr */ void operator()(int v) const
    {
      std::operator&lt;&lt;(std::cout, "value:").operator&lt;&lt;(v).operator&lt;&lt;(std::endl);
    }
    
    private:
    int i;
    double j;
    char c;
  };
  
  std::for_each(values.begin(), values.end(), __lambda_14_6(i, j, c));
  return 0;
}
<br><br>所以，Lambda 表达式是如何实现的？我们现在能明白的是编译器会将 Lambda 表达式转换成一个匿名类，而且 Lambda 是由仿函数实现的，捕获的变量会作为仿函数类中的成员变量。那从 Lambda 表达式到仿函数这一过程中会发生什么？<br>首先会进行语法解析和捕获，[] 叫做 CAPCHA method，它会按照既定方式捕获作用域内的局部变量（按值或按引用）虽然我们[=]自动按值捕获所有的局部变量，但由于一个都没有用，所有匿名类中也是不会对这些局部变量进行构造的。之后会生成匿名类，类中包括我们所捕获的变量并完成对运算符operator()的重载。<br>Lambda 中 CAPCHA method 中捕获的变量就会在生成匿名类时对这些变量进行构造，当然，这些变量会被作为类内的 private 变量存在。下面，我们来看看 Lambda 的捕获规则。<br><br>我们用捕获std::string对象作为例子来展示不同类型的捕获类型。我们下面会介绍按值捕获、按引用捕获和按右值捕获。<br><br>首先，我们来看看当捕获列表中按值捕获一个对象时会发生什么。最开始，我们用&nbsp;std::string str = "Hello!";&nbsp;定义了一个&nbsp;std::string&nbsp;类型的对象。这时，str&nbsp;指针在栈上，而字符串内容&nbsp;"Hello!"&nbsp;实际上存储在堆上。<br>然后，auto&nbsp;capFunc&nbsp;=&nbsp;[newStr&nbsp;=&nbsp;str]()&nbsp;mutable&nbsp;{}&nbsp;这行代码将&nbsp;lambda&nbsp;表达式的地址赋给了&nbsp;capFunc。在&nbsp;lambda&nbsp;表达式中，[newStr&nbsp;=&nbsp;str]&nbsp;相当于auto newStr = str;。在构造匿名类时，会在匿名类的&nbsp;private&nbsp;域中增加一个&nbsp;std::string&nbsp;newStr;&nbsp;成员，并生成相应的复制构造函数。<br>由于 std::string 类中对拷贝赋值运算符进行了重写，拷贝时进行深拷贝。所以按值捕获后，我们在栈上有两个不同的指针指向两个堆上字符串。我们对拷贝的字符串进行修改，打印出来。<br>#include &lt;iostream&gt;
#include &lt;string&gt;

int main()
{
    std::string str = "Hello!";
    auto capFunc = [newStr = str]() mutable
    {
        std::cout &lt;&lt; "In lambda: " &lt;&lt; newStr &lt;&lt; std::endl;
        if (!(newStr).empty())
        {
            newStr = "Changed!";
            std::cout &lt;&lt; "In lambda: " &lt;&lt; newStr &lt;&lt; std::endl;
        }
    };
    capFunc();
    std::cout &lt;&lt; str &lt;&lt; std::endl;
	return 0;
}
<br>按值拷贝的对象改变并不会影响原来的被拷贝对象。<br>$ ./lambda
In lambda: Hello!
In lambda: Changed!
Hello!
<br>使用&nbsp;mutable&nbsp;关键字的原因是，在生成匿名类时，它会去掉重载运算符后的&nbsp;const&nbsp;限定符，这样我们就可以在&nbsp;lambda&nbsp;中修改按值捕获的变量。如下所示，常方法不允许对类内对象进行修改，所以加入&nbsp;mutable&nbsp;关键字，使得修改按值捕获的副本成为可能。<br>inline /*constexpr */ void operator()() const {} // const by default
<br><br>当我们像下面这样按指针或是用引用捕获时，我们其实不太需要考虑 mutable 了。反而，我们这时应当开始关注指针悬挂或引用悬挂的问题。（按指针捕获可以看作是按值捕获）<br>int i = 5;
std::string str0 = "Hello ";
std::string str1 = "World!";
	// captured by pointer
    auto capFunc = [_i = &amp;i, _str0 = &amp;str0, _str1 = &amp;str1]() 
    {
    // any dereference operations will change original object
    }

    // captured by reference
    auto capFunc = [&amp;_i = i, &amp;_str0 = str0, &amp;_str1 = str1]() 
    {
    // any operations will change original object
    }
<br>由于我们用指针解引用和引用修改源对象的方式实际上修改的是指针指向的值或引用原对象的值，并不会修改匿名类内的对象，所以我们并不需要去添加mutable关键字。然而，我们需要注意，假如lambda返回了相关的指针或是引用，而原对象在使用返回值之前释放掉了，就会引发指针悬挂或引用悬挂的问题。<br><br>除了常见的按值捕获、按引用捕获和按指针捕获，我们还可以在此引入移动语义从而实现按右值捕获。通过 std::move 将 str 转移到 _str ，使得 _str 拥有原 str 的资源（即所有权转移了）。对象的所有权一旦转移，str 就不再可用，想要之后继续用到同一个 string 对象，我们可用返回该对象的引用，甚至返回其右值，转移所有权。<br>#include &lt;iostream&gt;
#include &lt;string&gt;
#include &lt;utility&gt; // std::move

int main() {
    std::string str = "Hello World!";
    auto capFunc = [_str = std::move(str)]() mutable -&gt; std::string&amp; {
        std::cout &lt;&lt; "In lambda: " &lt;&lt; _str &lt;&lt; std::endl;
        if (!_str.empty()) {
            _str = "Changed!";
            std::cout &lt;&lt; "In lambda: " &lt;&lt; _str &lt;&lt; std::endl;
        }
        return _str;
    };
    std::string&amp; refStr = capFunc();
    // str is moved, thus it's now unusable
    std::cout &lt;&lt; "Original string after move: " &lt;&lt; str &lt;&lt; std::endl;
    // 
    std::cout &lt;&lt; "Original string after move: " &lt;&lt; refStr &lt;&lt; std::endl;
    return 0;
}
<br>同样，我们能这么做是因为 std::string 中对<a data-href="The Rule of Five Idiom in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/the-rule-of-five-idiom-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">The Rule of Five Idiom in C++</a>有完整的实现。<br><br>当我们在类成员函数中使用 lambda 表达式时，我们可能想要在 lambda 中输出类成员变量的值。如下面代码中所示：<br>#include &lt;iostream&gt;

class myClass {
private:
    int counter{0};
public:
    void printCounter() {
        auto lambda = []() {
            std::cout &lt;&lt; "counter: " &lt;&lt; counter &lt;&lt; std::endl;
        };
        lambda();
    }
};

int main() {
    myClass obj;
    obj.printCounter();
    return 0;
}

<br>因为我们的捕获列表中并没有捕获任何东西，所以编译器会报错。我们学过 this 关键字，知道每个实例化的类都会有一个 this 指针指向实例的数据成员（相当于指向 C-style struct）。由于 this 指针是隐性提供的，所以对 this 的捕获是特殊的。<br>class myClass {
private:
    int counter{0};
public:
    void printCounter() {
        auto lambda = [this](){}; // capture `this` by reference
        auto lambda = [*this](){}; // capture `this` by value(since c++17)
    }
};
<br>我们可以按值捕获或者按引用捕获 this 指针。<br><br>Lambda 可以在作用域外调用么？我以为不行，但实际上可以。<br>这是一道面试题，在被问到这个问题时，脑袋空空。虽然我也明白 lambda 会生成一个匿名类，但我此刻脑中想的是：“既然 lambda 是可调用对象。我先将他看成一个作用域中的调用函数。” 如：<br>void outsideFunction(){
	void insideFunction(){
	}
}
int main(){
	insideFunction(); // Is this legal?
}
<br>我脑子里面想到的是上面这样的情景。在 main() 里面， insideFunction 肯定无法调用啊，它又不是全局函数，没有自己的符号，在链接地址重定位时怎么知道它在哪里？我觉得我分析的没有问题，但是确实是一个知识盲区。随即斩钉截铁地回答：“我认为是不行的，对“。<br>但是函数和类对象还是不一样的。Lambda表达式是闭包对象，而函数是函数符号。函数需要编译期的函数符号来调用，而类对象是类在运行中的实例。虽然 Lambda 的行为类似于函数，但是它遵循对象的作用域规则。所以你可以用一个全局的函数对象来存储作用域内的 Lambda 。<br>我们先举一个非常简单的例子：<br>#include &lt;iostream&gt;
int insideInteger;

void outsideFunction() {
    int value = 42;
    insideInteger = value;
}

int main() {
	outsideFunction();
	std::cout &lt;&lt; insideInteger &lt;&lt; std::endl;
    return 0;
}
<br>在上面的例子中，我们通过一个全局变量来获取函数内部的一个 int 型的变量。同样的，我们可以通过一个全局的 std::function 来获取 lambda 类对象变量。如下：<br>#include &lt;iostream&gt;
#include &lt;functional&gt;

std::function&lt;void()&gt; insideLambda;

void outsideFunction() {
    int value = 42;
    insideLambda = [value]() { std::cout &lt;&lt; value; };
}

int main() {
    outsideFunction();
    insideLambda();
    return 0;
}
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/lambdas-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Lambdas in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 26 Jul 2025 17:35:02 GMT</pubDate></item><item><title><![CDATA[Lock-Free Programming in C++ (ENG, NC)]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/lock-free-programming-in-c++-(eng,-nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Lock-Free Programming in C++ (ENG, NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 26 May 2025 00:05:09 GMT</pubDate></item><item><title><![CDATA[Malloc in glibc (ENG)]]></title><description><![CDATA[ 
 <br><br>In simple terms, malloc() is an abstraction built on system calls like mmap() and brk(). This means you don’t need to worry about the underlying system details. This is what high-level languages have provided such conveniences for us since the 1970s.<br>If you are familiar with how systems allocate memory, you would know that a page (typically 4KB) is the smallest unit that an operating system can process for memory allocation. However, in C, C++, or other high-level languages, you actually can allocate as little as a single byte(in reality, this might round up to 32 bytes). Then there's must be something happened under the hood.<br>For those who aren’t familiar with these system calls or OS memory management, you are encouraged to learn more about mmap() and brk() from <a data-tooltip-position="top" aria-label="7. Inter-Process Communication > 4.2.1 `mmap()`" data-href="7. Inter-Process Communication#4.2.1 `mmap()`" href="https://congzhi.wiki/congzhi's-os-series/7.-inter-process-communication.html#4.2.1_`mmap()`" class="internal-link" target="_self" rel="noopener nofollow">here</a> and <a data-tooltip-position="top" aria-label="12. Memory Management > 10.x.2 Dynamic Memory Allocation" data-href="12. Memory Management#10.x.2 Dynamic Memory Allocation" href="https://congzhi.wiki/congzhi's-os-series/12.-memory-management.html#10.x.2_Dynamic_Memory_Allocation" class="internal-link" target="_self" rel="noopener nofollow">here</a>. If you’re entirely new to memory management, feel free to explore my OS series, [Chapter 11](<a data-href="12. Memory Management" href="https://congzhi.wiki/congzhi's-os-series/12.-memory-management.html" class="internal-link" target="_self" rel="noopener nofollow">12. Memory Management</a>).<br><br>At first, let's look at how malloc is originally declared in &lt;stdlib.h&gt;:<br>#include &lt;stdlib.h&gt;
void* malloc(size_t size);
/*
Parameters:
    1. size: The number of bytes to allocate. This specifies the size of the process memory block to be allocated.
   
Return value: 
    - On success: Returns a pointer to the beginning of the allocated memory block. The memory is uninitialized.
    - On failure: Returns NULL, and errno is set to indicate the error.
*/
<br>malloc accepts only one parameter, which specifies the number of bytes to allocate. If the memory chunk allocation succeeds, the function returns a pointer to the beginning of the allocated memory chunk. If the allocation fails, it returns NULL, and then errno is set.<br>Pretty easy, right? But this is only scratching the surface—we want to dig deeper and uncover the inner workings of malloc. In this section, we’ll use the following example to see what’s happening beneath the surface:<br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
struct linked_chunk{
	int chunk_value;
	struct linked_chunk* next_chunk;
};

int main(){
	struct linked_chunk *chunk1;
    printf("Size of struct linked_chunk is %ld bytes.\n", sizeof(struct linked_chunk));
	chunk1 = malloc(sizeof(struct linked_chunk));
	chunk1 -&gt; chunk_value = 100;
	printf("The address of first chunk is: %p\n", chunk1);
	block1 -&gt; next_block = malloc(sizeof(struct linked_block));
	printf("The address of second chunk is: %p\n", chunk1 -&gt; next_chunk);
	chunk1 -&gt; next_chunk -&gt; chunk_value = 200;
	chunk1 -&gt; next_chunk -&gt; next_chunk = NULL;
    free(chunk1 -&gt; next_chunk);
    free(chunk1);
	return 0;
}
<br>Using this example, we have just allocated memory chunk with malloc. Let's analyze this with the image under below:<br>
<img alt="linked_block.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/linked_block.png"><br>
Outputs:<br>du@DVM:~/cpp$ setarch $(uname -m) -R ./proc
Size of struct linked_chunk is 16 bytes.
The address of first chunk is: 0x5555555596b0
The address of second chunk is: 0x5555555596d0
<br>Notice anything unusual? Our structure is a 16-byte struct, yet there is a 0x10-byte gap (16 bytes) between allocations. Why is that? Because there’s metadata behind the scenes. As said, the OS only allows processes to allocate memory in multiples of the system's page size. However, with malloc metadata, your compiler does a remarkable fine-grained job for you. For example, it allocates a 32-byte memory chunk, and we are going to explore how that happens.<br>Use your GNU debugger, and you will uncover what’s happening in that gap at 0x5555555596c0, which is a total of 16 bytes (system-specific). Within this space, we encounter 0x21, and it is reasonable to deduce that this is where our metadata resides. So, what does 0x21 signify?<br>(gdb) print chunk1
$1 = (struct linked_block *) 0x5555555596b0
(gdb) x/20x 0x5555555596b0
0x5555555596b0: 0x00000064      0x00000000      0x555596d0      0x00005555
0x5555555596c0: 0x00000000      0x00000000      0x00000021      0x00000000
0x5555555596d0: 0x000000c8      0x00000000      0x00000000      0x00000000
0x5555555596e0: 0x00000000      0x00000000      0x00020921      0x00000000
0x5555555596f0: 0x00000000      0x00000000      0x00000000      0x00000000
<br>To be straightforward, the metadata includes how many bytes are allocated and whether that memory chunk is occupied. For example, 0x20 in 0x21 denote the allocated memory size, while 0x1 indicates that this allocated memory is occupied. To free the chunk, simply set the occupied bit to 0x0, and it's all done. Quite straightforward no? (That explained why you only allocate 1 byte but actually allocated 32 bytes of memory)<br>Understanding that, let's now delve into how malloc manages heap memory. Using gdb, we can observe that our heap spans from 0x555555559000 to 0x55555557a000, totaling 0x21000 bytes in size.<br>Mapped address spaces:

Start Addr           End Addr       Size     Offset  Perms  objfile
0x555555559000     0x55555557a000  0x21000    0x0     rw-p   [heap]
<br>0x555555559000: 0x00000000      0x00000000      0x00000291      0x00000000
0x555555559290: 0x00000000      0x00000000      0x00000411      0x00000000
0x5555555596a0: 0x00000000      0x00000000      0x00000021      0x00000000
0x5555555596c0: 0x00000000      0x00000000      0x00000021      0x00000000
0x5555555596e0: 0x00000000      0x00000000      0x00020921      0x00000000
<br>Bit by bit, we can observe how malloc manages heap memory. In total, we have 0x21000 bytes of heap memory (0x290 + 0x410 + 0x20 + 0x20 + 0x20920 = 0x21000). Visually, it looks like this:<br>
<img alt="malloc_map.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/malloc_map.png"><br>
Also in memory mapping segment, you can find this metadata behind the dynamic allocated memory.]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/malloc-in-glibc-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Malloc in glibc (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:32:45 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/linked_block.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/linked_block.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Member Initializer List in C++ (ENG)]]></title><description><![CDATA[ 
 <br><br>There isn't much talk about this member initializer list really. We only use initializer list to initialize a new class object. You may see the member initializer list after a function definition of any kind of constructor, syntax with a colon character : . For example, consider the following:<br>class MyClass{
	int x, y, z;
public:
	// Most commonly used and seen
	MyClass(int _x, int _y, int _z) : x(_x), y(_y), z(_z) {}

	// Brace-enclosed initializer list semantics after C++11
	MyClass(int _x, int _y, int _z) : x{_x}, y{_y}, z{_z} {}
};
<br>But we have to following some rules here to order:<br>
<br>Reference members cannot be bound to temporaries in a member initializer list.
class MyClass{
	const int&amp; ref;
	MyClass() : ref(10) {} // This would cause an error
};


<br>Initialization do follow some orders:

<br>The compiler will initialize the object members in the order they are declared in the class, not the order they appear in the initializer list.

class MyClass {
    int x, y, z;
public:
	// x is still initialized before y and z
    MyClass(int _x, int _y, int _z) : z(_z), y(_y), x(_x) {} 
};


<br>So, when the x is initialized after y or z, or we say y is initialized after z, shit happens:

class MyClass {
    int x, y, z;
public:
	// x is initialized using the uninitialized y
	MyClass(int _x, int _y, int _z) : x(y), y(_y), z(_z) {}
};


<br><br>So, why are you recommended to use {} to initialize an object even though we have ()? Because they have different semantics. An exact example is how they are used in STL container.<br>std::vector&lt;int&gt; v(100, 1);  // Initializes a vector containing 100 items of value 1
std::vector&lt;int&gt; v{100, 1};  // Initializes a vector containing 2 items: 100 and 1

int i(3.14);  // Okay, i will be assigned the value 3 (narrowing conversion)
int i{3.14};  // Error, the assigned value must be type-specific (no narrowing)
<br>Another great thing with curly braces {} is automatically initial everything to zero for basic data types. For example:<br>int a{}; // a is initialized to 0
void* ptr{}; // ptr is initialized to nullptr

class MyClass {
public:
    int a;
    void* ptr;
    MyClass() = default;
};

MyClass obj{}; // All members are initialized to 0 (pointer to nullptr)
<br>But note that if you have a user-defined constructor like this, it can cause undefined behavior:<br>class MyClass {
public:
    int a;
    void* ptr;
    MyClass() {} // Would cause undefined behavior
};

MyClass obj{}; // Not all members are initialized
<br><br>Use initializer list for copy avoidance purposes.<br>#include &lt;iostream&gt;

class MyClass {
public:
    MyClass(int x, int y) {
        std::cout &lt;&lt; "Regular constructor called with " &lt;&lt; x &lt;&lt; " and " &lt;&lt; y &lt;&lt; std::endl;
    }

    MyClass(std::initializer_list&lt;int&gt; list) {
        std::cout &lt;&lt; "Initializer list constructor called with ";
        for (auto elem : list) {
            std::cout &lt;&lt; elem &lt;&lt; " ";
        }
        std::cout &lt;&lt; std::endl;
    }
};

int main() {
    MyClass obj1(100, 200);     // Calls the regular constructor
    MyClass obj2{100, 200};     // Calls the initializer list constructor
	// MyClass obj3{100.5, 200.5}; // No narrowing conversion
    return 0;
}
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/member-initializer-list-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Member Initializer List in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:33:13 GMT</pubDate></item><item><title><![CDATA[Move Semantics in C++]]></title><description><![CDATA[ 
 <br><br>在 C 中，我们常看到这样的表达式：int i = 10;。这种式子中，变量 i 是有地址属性的，而数字 10 是一个栈中的临时量，它只有值属性。由于 i 这样拥有地址属性的变量总是出现在等号的左边而 10 这种只有值属性的值总是出现在等号右侧。我们就将这种有地址属性在内存中有存储区域的表达式或变量称作左值(lvalue)，而不具有持久存储的值我们称作右值(rvalue)。<br>int i = 1; // okay
int j = i;  // okay
int 1 = j; // error
int k = (i + j); // okay, (i + j) is a rvalue
int (i + j) = k; // error
<br>由于左值有地址属性，所以我们可以对左值取地址，也就是 &amp;i 。<br>int i = 10;
int* pi = &amp;i; // okay
int* pi2 = i + i; // error, (i + i) is a rvalue
<br>除了一些字面量，一般而言，函数的非引用返回值也是典型的右值。因为函数的非引用返回一般存储在寄存器中，而寄存器是不能取地址的。<br>int addValue(int a, int b){
	return a + b;
}
int main(){
	int i = addValue(3, 4); // okay
	addValue() = 5;     // error
}
<br><br>由左值和右值的概念由引入了左值引用和右值引用的概念。（左值）引用是一个变量的别名。比如我们有以下的程序：<br>int i = 10;
int&amp; lvref = i; // okay
int&amp; lvref = 10;// error, lvalue ref only bind to the lvalue
<br>在这里，变量 ref 是变量 i 的别名。既然是别名，即这两个变量共用一个地址空间。前面我们提到了右值没有地址属性，所以左值引用对右值的绑定就是非法的。当你运行程序，编译器会提示：cannot bind non-const lvalue reference of type ‘int&amp;’ to an rvalue of type ‘int’。<br>既然我们并不能给没有地址属性的右值起一个别名。但为什么如下的用 const 修饰的右值就可以为左值引用所绑定？<br>const int&amp; lvref = 10;
<br>这是由于编译器会隐式地生成一个临时对象（左值）来存储右值，这被称为 temporary materialization 。所以实际上我们的左值引用是对存储 10 这个右值的左值的绑定。当然，这种方式只是 C++11 之前右值引用没有出现时 C++ 绑定右值的方式。<br>// as same as:
const int temp = 10;
const int&amp; lvref = temp;
<br>在汇编层面，它们是一样的。<br>main:

        push    rbp
        mov     rbp, rsp
        mov     dword ptr [rbp - 4], 0
        
		; const int&amp; ref = 10;
        mov     dword ptr [rbp - 20], 10
        lea     rax, [rbp - 20]
        mov     qword ptr [rbp - 16], rax
		
		; const int temp = 10;
		; const int&amp; ref = temp;
        mov     dword ptr [rbp - 24], 10
        lea     rax, [rbp - 24]
        mov     qword ptr [rbp - 32], rax

        xor     eax, eax
        pop     rbp
        ret
<br>在左值引用中，我们最常见的用法就是用左值引用替换掉传统用指针传参的做法。引用和指针汇编后是一样的，但是指针我们需要取地址解引用。而引用完全不需要，你可以把引用理解成一种用户友好化的指针。<br>#include &lt;iostream&gt;

void printVal(int&amp; val) {
    std::cout &lt;&lt; val &lt;&lt; std::endl;
}
int main() {
    int x = 1;
    printVal(x);
    printVal(20); // cannot bind non-const lvalue reference to an rvalue
}
<br>同样的，在传递右值（20）的时候，你也需要用 const int&amp; 绑定右值参数。这时，编译器会生成一个临时的 const int 左值对象，并将其绑定到 val 上。<br>#include &lt;iostream&gt;

void printVal(const int&amp; val) {
    std::cout &lt;&lt; val &lt;&lt; std::endl;
}
int main() {
    int x = 1;
    printVal(x); // ok
    printVal(20);// ok
}
<br>此外，我们还可以将函数的返回值变成左值引用类型。比如我们有一个左值 val ，让 int&amp; access_Val() 函数返回左值引用。因为左值引用是对已有左值的别名，所以左值引用类型的返回值是可以取地址的，其返回值就相当于 val 变量本身。我们可以用这种方式来 set 或 get 类内变量：<br>#include &lt;iostream&gt;

class example{
	int val;
public:
	int&amp; access_Val(){
		return val;
	}
};
int main(){
	example e1;
	e1.access_Val() = 10;
    std::cout &lt;&lt; &amp;e1.access_Val() &lt;&lt; std::endl;
	return 0;
}
<br>通过这种方式，我们实际上让 accessVal() 变成数据双向流向的函数。还可以避免不必要的拷贝。这样，我们可以直接操作类中的成员变量，并且在需要时进行赋值和获取，所有这些操作都避免了不必要的拷贝操作，提高了性能和效率。<br><br>在前面，我们发现类似这样的语句是非法的：int&amp; rvref = 10;。编译器会告诉你cannot bind non-const lvalue reference of type ‘int&amp;’ to an rvalue of type ‘int’。因为左值引用所引用的对象是一个右值，而左值引用是不能绑定到右值的。<br>而且我们注意到加上 const 实际上绑定的实际上还是左值。为了实现对右值的绑定，C++11 引入了右值引用 &amp;&amp; 。右值引用只能绑定右值，而不能绑定左值：<br>int&amp;&amp; rvref = 10;

int i = 10;
int&amp;&amp; rvref = i; // cannot bind rvalue reference of type to lvalue
<br>当我们试图去打印 rvref 时，我们可以得到期望的结果。但为什么要这样做？<br>#include &lt;iostream&gt;

int main(){
	int&amp;&amp; rvref = 10; // difference from `int i = 10;` ?
	int i = rvref;
	std::cout &lt;&lt; rvref &lt;&lt; std::endl; // prints out the integer 10
	std::cout &lt;&lt; &amp;rvref &lt;&lt; std:endl; // okay
	std::cout &lt;&lt; &amp;i &lt;&lt; std:endl;     // okay
	return 0;
}
<br>接着之前的例子：<br>#include &lt;iostream&gt;

// lvalue ref copy
void printVal(const int&amp; lval) {
    std::cout &lt;&lt; lval &lt;&lt; std::endl;
}
// rvalue ref no copy
void printVal(int&amp;&amp; rval) {
    std::cout &lt;&lt; rval &lt;&lt; std::endl;
}
// thus, this is not allowed with whose overloading above:
// void printVal(int val){
//
// } // not allowed
int main() {
    int x = 1;
    printVal(x); // ok, call printVal(const int&amp; lval)
    printVal(20);// ok, call printVal(int&amp;&amp; rval)
}
<br>有了因为右值引用绑定右值，所以当我们在函数里面传递的是右值时，它会调用右值引用的重载函数。我们后面会看到，使用右值引用可以减少不必要的内存拷贝的次数。当对象很大的时候，使用右值引用就可以显著优化资源管理和执行效率。<br><br>移动语义是C++11引入的一项特性，右值或右值引用是其最主要的应用。它允许对象的资源（如堆上分配的内存）在不进行深度复制的情况下进行所有权的转移。可以将对象的资源所有权从一个对象转移到另一个对象，从而避免不必要的内存拷贝，提高程序性能和效率。<br>假如我们有如下程序：<br>#inlcude &lt;vector&gt;
int main(){
	std::vector&lt;int&gt; v1{ 1, 2, 3, 4, 5 }; 
	std::vector&lt;int&gt; v2{}; 
	v2 = v1;
	return 0;
}
<br>在进行拷贝操作时，标准库容器会进行深拷贝，这就意味着v2会拥有v1的所有资源的拷贝。<br><img alt="Pasted image 20250111194242.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250111194242.png"><br>虽然深拷贝可以避免数据的不一致性，但通常会涉及大量的内存操作，进而造成一定的存储和性能开销。而移动语义通过移动构造函数和移动赋值运算符，可以将资源“移动”到新对象中，而不是复制，从而减少了开销。<br><br>std::move 是 C++11 引入的一个标准库函数，std::move()并不移动任何东西。而是将其输入无条件地转换为右值引用（rvalue reference）。这意味着无论传递给 std::move 的是什么类型的对象，它都会被转换为右值引用，从而允许移动语义的应用。<br>template&lt;typename _Tp&gt;
    _GLIBCXX_NODISCARD
    constexpr typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;
    move(_Tp&amp;&amp; __t) noexcept
    { return static_cast&lt;typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;&gt;(__t); }
<br>为了避免额外的复制开销，C++ 引入了移动语义。在 STL 标准容器库中，移动赋值运算符已经在类内进行了重载、移动构造函数也进行了实现。确保等号的右边是右值的时候可以进行移动赋值。<br>假如我们有如下程序：<br>#inlcude &lt;vector&gt;
int main(){
	std::vector&lt;int&gt; v1{ 1, 2, 3, 4, 5 }; 
	std::vector&lt;int&gt; v2{}; 
	v2 = std::move(v1);
	return 0;
}
<br>这个例子中，我们用std::move()将v1的资源移动到v2，避免了深拷贝。这个过程图示如下：<br>
<br>
v2将v1所持有的资源进行引用：<br>
<img alt="Pasted image 20250111204014.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250111204014.png">

<br>
所有权进行转移，v1不再持有资源，进而重置v1：<br>
<img alt="Pasted image 20250111204033.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250111204033.png">

<br>通过std::move()，资源从v1转移到v2，v1不再持有这些资源。移动后的v1处于有效但未定义的状态，不再持有原资源。对于程序员而言，我们需要承诺在所有权转移后不再使用v1。<br><br>移动语义出现之后，我们想实践对象所有权的转移，我们需要两个特别的新函数：移动构造函数和移动赋值运算符(move constructor and move assignment operator)。从下面的代码中，你可以窥见所有权的转移是怎么样的。<br>#include &lt;string&gt;
class Widget {
private:
    int i{0};
    std::string s{};
    int* pi{nullptr}; // with unique opinter, you can make move constructor defalut
public:
    Widget(Widget&amp;&amp; w) noexcept
        : i(std::move(w.i)), s(std::move(w.s)), pi(std::move(w.pi)) // member-wise move
    {
        w.pi = nullptr; // Reset
    }
    Widget&amp; operator=(Widget&amp;&amp; w) noexcept {
        if (this != &amp;w) {
            delete pi; // Clean-up
            i = std::move(w.i); // Member-wise move
            s = std::move(w.s); // Member-wise move
            pi = std::move(w.pi); // Member-wise move
            w.pi = nullptr; // Reset
        }
        return *this;
    }
};
/*
#include &lt;string&gt;
#include &lt;memory&gt;

class Widget {
private: 
    int i{0};
    std::string s{};
    std::unique_ptr&lt;int&gt; pi{nullptr};
public:
    Widget(Widget&amp;&amp; w) = default; // noexcept by default
    Widget&amp; operator=(Widget&amp;&amp; w) = default; // noexcept by default
};
*/
<br>为了避免不必要的拷贝操作，“五法则”成为了 class designing 的最佳实践。然而，随着 C++11 引入了移动语义和智能指针等特性，“零法则”逐渐成为一种更理想的设计哲学。<br>“零法则”提倡不要手动定义任何特殊成员函数，而是依赖于编译器生成的默认实现。通过组合已有的类和资源管理器（例如 std::unique_ptr 和 std::shared_ptr），我们可以在需要时自动拥有正确的拷贝或移动行为。<br>// The Rule of Zero
#include &lt;memory&gt;

class myClass {
private:
    std::unique_ptr&lt;int&gt; data;
public:
    myClass() = default;
    ~myClass() = default;
    // No declaration of copy/move constructor or copy/move assignment operator
};

<br><br>例如，考虑一个简单的类 MyString，它包含一个指向字符数组的指针。在使用移动构造函数时，新的 MyString 对象会接管原对象的指针，而原对象的指针会被置为空指针，这样在销毁原对象时就不会重复释放内存。<br>#include &lt;iostream&gt;
#include &lt;string.h&gt;

class String{
private:
   char* m_Data;
   uint32_t m_Size;
public:
    String() = default;
    String(const char* string){
        m_Size = strlen(string);
        m_Data = new char[m_Size];
        memcpy(m_Data, string, m_Size);
        std::cout &lt;&lt; "String created!" &lt;&lt; std::endl;
    }
    String(const String&amp; other){
        m_Size = other.m_Size;
        m_Data = new char[m_Size];
        memcpy(m_Data, other.m_Data, m_Size);
        std::cout &lt;&lt; "String copied!" &lt;&lt; std::endl;
    }
    String(String&amp;&amp; other) noexcept{
        m_Size = other.m_Size;
        m_Data = other.m_Data;
        other.m_Size = 0;
        other.m_Data = nullptr;
        std::cout &lt;&lt; "String Moved!" &lt;&lt; std::endl;
    }
    ~String(){
        delete m_Data;
        std::cout &lt;&lt; "String destroyed!" &lt;&lt; std::endl;
    }
    void Print(){
        for (uint32_t i = 0; i &lt; m_Size; i++)
        {
            printf("%c", m_Data[i]);
        }
    }
};
class Entity{
private:
    String s_Name;
public:
    Entity(const String&amp; name) : s_Name(name){}
    Entity(String&amp;&amp; name) : s_Name((String&amp;&amp;)name){}

    void printName(){
        s_Name.Print();
    }
};

int main(){

    Entity entity("Hello\n");
    entity.printName();
    std::cin.get();
    return 0;
}
<br>在这个例子中，MyString 的移动构造函数通过 std::move 将资源从一个对象转移到另一个对象，从而避免了不必要的复制操作。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/move-semantics-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Move Semantics in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 07 Jun 2025 00:49:36 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250111194242.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20250111194242.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Mutable and The M&M Rule in C++ (ENG)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Const in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/const-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Const in C++</a><br><br>The mutable specifier allows a member to be modified in a const member function. In the example below, we qualify a function with const, which means we are not supposed to change the class members in the function. However, we do modify the mutable count member within the function scope.<br>class myClass{
private:
	int variable = 0;
	mutable int count;
public:
	int getVar() const{
	    count++;
	    return variable;
    }
};
int main(){
    myClass obj;
    int value = obj.getVar();
    return 0;
}
<br>"Const as a promise", using mutable specifier can break the constness of a function, which can be seen as poor code design. The principal of "const as a promise" implies that a const member function should not modify any member variables. But mutable is an exception to this rule, which can lead to unexcepted behavior.<br>However, mutable could be useful in some situations like mutexes for safer code(the M&amp;M rule), cache memory, and lazy evaluation.<br><br>This code example is copied from  <a data-tooltip-position="top" aria-label="https://en.cppreference.com/w/cpp/language/cv" rel="noopener nofollow" class="external-link" href="https://en.cppreference.com/w/cpp/language/cv" target="_blank">cppreference</a>. As you can see, we have a const get() function, and with std::mutex, we create safe concurrent code. Why is that? While we are not supposed to modify anything in the const specified functions, with mutable, another thread can actually modify the data in a const specified function. This is no thread-safe.<br>class ThreadsafeCounter
{
    mutable std::mutex m; // The "M&amp;M rule": mutable and mutex go together
    int data = 0;
public:
    int get() const
    {
        std::lock_guard&lt;std::mutex&gt; lk(m);
        return data;
    }
 
    void inc()
    {
        std::lock_guard&lt;std::mutex&gt; lk(m);
        ++data;
    }
};
<br>For achieving the goal of safer code and const correctness at the same time, we have to use the mutable keyword to make the mutex mutable in the const function. While a thread is modifying the critical section, using a lock to prevent other threads from entering and accessing the resource is thus necessary.]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/mutable-and-the-m&amp;m-rule-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Mutable and The M&amp;M Rule in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 16 Aug 2025 09:16:19 GMT</pubDate></item><item><title><![CDATA[Networking - CS model (Abandoned)]]></title><description><![CDATA[ 
 <br>在 Computer Networking: A Top-Down Approach 课程中，我们学习了数据是如何从 application layer 经由 transport layer 和 network layer 的两层封装，将数据传送到另一台主机上的特定端口号进程上的。在这个过程中，底层的网络如同一道道传送门，我们开发人员是不需要知道其中的实现细节的。当我们要将数据传送给另一台主机上的 application layer 进程时。我们只需要告诉网络目标主机的IP地址和端口号就能够使用网络进行数据的传输了。<br><br>在TCP实现的C/S原型模型中，我们实现的功能极为简单：服务器进程一直检测来自客户端进程的连接，一旦检测到有客户端“敲门”，就创建客户端Socket并给客户端发送"hello world"字符串，然后客户端回发，服务器显示。<br>//server
#include &lt;iostream&gt;
#include &lt;WinSock2.h&gt;	//声明
#include &lt;WS2tcpip.h&gt;
#include &lt;thread&gt;
#include &lt;chrono&gt;       //时间头函数
#pragma comment(lib, "ws2_32.lib")	//实现

const u_int BACKLOG = 128;
u_int link_count = 0;
const char* msg = "hello world";

//若当前socket连接数量小于BACKLOG，则每隔一秒打印一次打印监听信息。
void printListeningStatus() {
    while (link_count &lt; BACKLOG) {
        std::cout &lt;&lt; "Server is listening on port 2345...\tWe now can connect " 
                  &lt;&lt;BACKLOG - link_count&lt;&lt;" hosts." &lt;&lt; std::endl;
        std::this_thread::sleep_for(std::chrono::seconds(2));
    }
}
//管理客户端进程
int clientHandle(SOCKET* servSock, SOCKET*clntSock) {

    // 打印客户端信息
    link_count++;
    sockaddr_in clntAddr;
    int clntAddrSize = sizeof(clntAddr);
    getpeername(*clntSock, (sockaddr*)&amp;clntAddr, &amp;clntAddrSize);
    char clntIP[INET_ADDRSTRLEN];
    inet_ntop(AF_INET, &amp;clntAddr.sin_addr, clntIP, INET_ADDRSTRLEN);
    std::cout &lt;&lt; "Client connected from IP: " &lt;&lt; clntIP &lt;&lt; " and port: " 
              &lt;&lt; ntohs(clntAddr.sin_port) &lt;&lt; std::endl;
    
    //处理与client的会话
    char recvBuf[128]{};
    if (send(*clntSock, msg, strlen(msg), 0) == -1) {
        std::cout &lt;&lt; "Sending fail!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        closesocket(*servSock);
        closesocket(*clntSock);
        WSACleanup();
        return -1;
    }
    int recvRet = recv(*clntSock, recvBuf, 128, 0);
    if (recvRet == 0) {
        std::cout &lt;&lt; "Client drop the connection gracefully." &lt;&lt; std::endl;
    }
    else if (recvRet &lt; 0) {
        int ErrNum = WSAGetLastError();
        if (ErrNum == 10054) {
            std::cout &lt;&lt; "Client drop the connection forcefully." &lt;&lt; std::endl;
        }
        else {
            std::cout &lt;&lt; "Receiving fail!\tError number is:" &lt;&lt; ErrNum &lt;&lt; std::endl;
            closesocket(*servSock);
            closesocket(*clntSock);
            WSACleanup();
            return -1;
        }
    }
    std::cout &lt;&lt; recvBuf &lt;&lt; std::endl;
    closesocket(*clntSock);
    link_count--;
    std::cout &lt;&lt; "Client from IP: " &lt;&lt; clntIP &lt;&lt; " Port:" &lt;&lt; ntohs(clntAddr.sin_port) 
              &lt;&lt;" has been disconnected."&lt;&lt; std::endl;
    return 0;
}
int main() {
    //初始化WSADATA结构。
    WSADATA data{};
    if (WSAStartup(MAKEWORD(2, 2), &amp;data) == SOCKET_ERROR) {
        std::cout &lt;&lt; "WSAStart failed! \tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        WSACleanup();
        return -1;
    }
    //为服务器进程创建用于TCP/IP通信的套接字。
    SOCKET servSock = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
    if (servSock == INVALID_SOCKET) {
        std::cout &lt;&lt; "Server socket creation fail!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        WSACleanup();
        return -1;
    }
    /*定义一个sockaddr_in(Socket address internet)的结构体变量用来存储IP地址、端口号信息。
    之后将进程的“地址信息”一律转换成网络字节序（大端序）。*/
    sockaddr_in servAddr;
    servAddr.sin_family = AF_INET;      //设置地址族为IPv4
    servAddr.sin_port = htons(2345);	//将port号转换成网络字节序
    inet_pton(AF_INET, "127.0.0.1", &amp;servAddr.sin_addr.S_un.S_addr);//将IP地址转换成网络字节序

    //将服务器进程和特定的端口号进行绑定
    if (bind(servSock, (sockaddr*)&amp;servAddr, sizeof(sockAddr)) == SOCKET_ERROR) {
        std::cout &lt;&lt; "Binding fail!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        closesocket(servSock);
        WSACleanup();
        return -1;
    }

    //服务器socket状态从closed转换成listening。
    if (listen(servSock, BACKLOG) == -1) {
        std::cout &lt;&lt; "Listen function fail!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        closesocket(servSock);
        WSACleanup();
        return -1;
    }

    // 启动一个线程来打印监听状态
    std::thread statusThread(printListeningStatus);

    //当有client进程“敲门”，服务器进程就创建一个client socket线程来与客户端进程进行通信。
    while(link_count &lt; BACKLOG){
        SOCKET clntSock = accept(servSock, nullptr, nullptr);
        if (clntSock == INVALID_SOCKET) {
            std::cout &lt;&lt; "Client socket creation fail!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
            closesocket(servSock);
            WSACleanup();
            return -1;
        }
        // 创建一个线程来处理客户端通信
        std::thread clientThread(clientHandle, &amp;servSock, &amp;clntSock);
        //分离线程，让线程单独执行
        clientThread.detach();
    }

    // 等待线程结束
    statusThread.join(); 
    
    closesocket(servSock);

    WSACleanup();

    system("pause");
    return 0;
}
<br>// client
#include &lt;WinSock2.h&gt;
#include &lt;WS2tcpip.h&gt;
#include &lt;iostream&gt;
#pragma comment(lib, "ws2_32.lib")

int main() {
    // 初始化 WSADATA 结构。
    WSADATA data{};
    if (WSAStartup(MAKEWORD(2, 2), &amp;data) == SOCKET_ERROR) {
        std::cout &lt;&lt; "WSAStartup failed! \tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        return -1;
    }

    // 为客户端进程创建用于 TCP/IP 通信的套接字。
    SOCKET servSock = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
    if (servSock == INVALID_SOCKET) {
        std::cout &lt;&lt; "Client socket creation failed!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        WSACleanup();
        return -1;
    }

    // 定义一个 sockaddr_in 结构体变量用来存储服务器的 IP 地址和端口号信息。
    sockaddr_in servAddr;
    servAddr.sin_family = AF_INET;      // 设置地址族为 IPv4
    servAddr.sin_port = htons(2345);    // 将端口号转换成网络字节序
    inet_pton(AF_INET, "127.0.0.1", &amp;servAddr.sin_addr.S_un.S_addr); // 将 IP 地址转换成网络字节序

    // 连接到服务器
    if (connect(servSock, (sockaddr*)&amp;servAddr, sizeof(servAddr)) == SOCKET_ERROR) {
        std::cout &lt;&lt; "Connection failed!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        closesocket(servSock);
        WSACleanup();
        return -1;
    }

    // 接收服务器发送的数据
    char recvBuf[128]{};
    int recvRet = recv(servSock, recvBuf, 128, 0);
    if (recvRet == 0) {
        std::cout &lt;&lt; "Server dropped the connection gracefully." &lt;&lt; std::endl;
    }
    else if (recvRet &lt; 0) {
        int ErrNum = WSAGetLastError();
        if (ErrNum == 10054) {
            std::cout &lt;&lt; "Server dropped the connection forcefully." &lt;&lt; std::endl;
        }
        else {
            std::cout &lt;&lt; "Receiving failed!\tError number is:" &lt;&lt; ErrNum &lt;&lt; std::endl;
            closesocket(servSock);
            WSACleanup();
            return -1;
        }
    }

    // 发送数据到服务器
    if (send(servSock, recvBuf, recvRet, 0) == SOCKET_ERROR) {
        std::cout &lt;&lt; "Sending failed!\tError number is:" &lt;&lt; WSAGetLastError() &lt;&lt; std::endl;
        closesocket(servSock);
        WSACleanup();
        return -1;
    }

    closesocket(servSock);
    WSACleanup();
    system("pause");
    return 0;
}
<br><br>
void fileDirMnagr::listDir(int depth = 0)
{
	DIR *dir = opendir(currDir.c_str());
	if (!dir)
	{
		formattedMesg == "Cannot open " + currDir + ".\n";
		perror("Fail to open directory");
		return;
	}
	dirent *pDirent = nullptr;
	while ((pDirent = readdir(dir)) != nullptr)
	{
		const std::string filename = pDirent-&gt;d_name;
		if (filename == "." || filename == "..")
		{
			continue;
		}
		for (int i = 0; i &lt; depth; ++i)
		{
			formattedMesg += "    ";
		}
		formattedMesg += "|---" + filename;
		if (pDirent-&gt;d_type == DT_DIR)
		{
			formattedMesg += "/\n";
			std::string previousDir = currDir;
			currDir = currDir + "/" + filename;
			listDir(depth + 1);
			currDir = previousDir;
		}
		else
		{
			formattedMesg += "\n";
		}
	}
	if (closedir(dir) != 0)
	{
		perror("Fail to close directory");
	}
}

]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/networking-cs-model-(abandoned).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Networking - CS model (Abandoned).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 20 Feb 2025 21:57:20 GMT</pubDate></item><item><title><![CDATA[noexcept in C++ (ENG)]]></title><description><![CDATA[ 
 <br><br>I believe some of us will question sometime about what this noexpct keyword does when we start learning "the rule of five". The move constructor and the move assignment operator often have a specifier noexcept, what for? Why do we always need to make the move constructor noexcept?<br>class myClass{
private:
	// data members
public:
	myClass(){}
	~myClass(){}
	myClass(const myClass&amp; obj){} // Copy constructor
	myClass&amp; operator=(const myClass&amp; obj){} // Copy assignment operator
	myClass(myClass&amp;&amp; obj) noexcept {} // Move constructor
	myClass&amp; operator=(myClass&amp;&amp; obj) noexcept {} // Move assignment operator
};
<br>If you go to the reference, you will get a simple answer. It's for two purposes: one is for the compiler to generate better code, and the other is for better readability for the user. Every function in C++ is either&nbsp;non-throwing&nbsp;or&nbsp;potentially throwing, and you could specify them explicitly:<br>void funcA() noexcept {} // non-throwing, which means the function guarantees that it won't throw any exceptions
void funcB() noexcept(true) {} // non-throwing, more explicitly stating that the function guarantees it won't throw any exceptions

void funcA() {} // potentially throwing, meaning the function might throw exceptions
void funcB() noexcept(false) {} // potentially throwing, explicitly stating that the function might throw exceptions
<br>You see, every time you specify noexcept(noexcept(true)) , you are simply telling the compiler that the function will not throw errors at run-time, so that the compiler will do its best to optimize the code. Additionally, users can immediately understand that the function will not throw errors by seeing noexcept.<br>noexcept is a promise to the compiler and the runtime that the function will not throw exceptions, and breaking this promise will lead to program termination.<br>The noexcept specifier doesn't make the compiler check for potential exceptions at compile-time. It simply means that if an exception occurs within a function marked noexcept, the program will call std::terminate and terminate abruptly since the promise is broken. <br><br>Then back to our question, why do we need a noexcept with the move constructor? The real reason lies in STL containers. Functions marked with noexcept can enable move semantics, while without noexcept, they fallback to copy operations.<br>Normally, during a move operation, there should only be ownership transfers (just pointer evaluations). So you cannot really have anything fail happening there.<br>Is this a must? Well, it's not a must. If you call some potentially throwing function inside a noexcept specified function, there would be an error. It's easy to understand because you break the promise. Here’s an example illustrating a bad practice:<br>#include &lt;utility&gt;
class myClass {
private:
    // data members
public:
    // other member functions
    void throwing() {
        throw 20;
    } // not marked as noexcept
    myClass(){}
    myClass(myClass&amp;&amp; obj) noexcept(true) {
        // data moves
        throwing();
    } // this would be an error because the throwing function is not noexcept
};
int main(){

    myClass obj;
    myClass obj2(std::move(obj)); // throw an exception
    return 0;
}
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/noexcept-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/noexcept in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 29 Jul 2025 18:21:58 GMT</pubDate></item><item><title><![CDATA[Object Oriented Programming in C++]]></title><description><![CDATA[ 
 <br><br><br>除非你只学过 C，否则你肯定听说过面向对象的编程范式以及它的的三大特征：封装、继承和多态。但 C++ 中的 OOP 又区别于其他语言：<br>
<br>C++ 支持作用于”访问区域“的访问修饰符、虚函数机制、支持 RAII 的构造和析构函数等。
<br>C++ 的继承可以是单继承，也可以是多继承；
<br>每种继承都可以是 public 的、protected 或 private 的；
<br>每种继承都可以是 virtual 或者 non-virtual 的；
<br>类成员方法也可以是虚函数、非虚函数或是纯虚函数；
<br><br>
Separation from interface and implementation.
<br>C++&nbsp;用 “类类型&nbsp;(class&nbsp;types)” 来对数据进行封装。封装很好理解，即将类内成员变量和函数与外界分开。不像在&nbsp;C&nbsp;语言中，函数的实现光溜溜地暴露在全局范围&nbsp; (global&nbsp;scope)&nbsp;。在&nbsp;C++&nbsp;中，我们可以用类类型将这些函数的实现封装起来，只暴露使用的接口。<br>C++提供三种类类型：class、struct、union，而只有前两者提供对数据的有效封装性，因而我们忽略union类型。<br><br>
Don’t inherit for code reuse. Inherit, when you want to express a logical structure.
<br><br>C++ 支持三种不同的继承访问控制修饰符：public，protected 和 private。在使用 public 继承方式时<br>D is-a B, but not every B is-a D.<br>继承是对一个类的继承。当我们继承一个类得到新的类时，我们需要明白，在不同的继承访问控制修饰符下，成员变量的可见性和访问权限是怎么样的。<br>class Account{
public:
  int pub{0};
protected:
  int prot{0};
private:
  int pri{0};
};
class PubAccount: public Account{
public:
  PubAccount(){
    pub + prot;  // public + protected
  }
};
class ProtAccount: protected Account{
public:
  ProtAccount(){
    pub + prot;  // protected + protected
  }
};
class PriAccount: private Account{
public:
  PriAccount(){
    pub + prot;  // private + private
  }
};

int main(){
  PubAccount pubAccount;
  ProtAccount proAccount;
  PriAccount priAccount;
  pubAccount.pub;
}
<br>为了封装性，基类的私有成员是不可以被继承到派生类中的。上面的例子中，展示了基类的不同成员的访问权限在不同继承方式下的访问权限。<br><br>在C++中，class 和 struct 的区别就是 struct 因为C兼容的缘故，默认的成员变量和成员函数是 public 的，而不像 class 中的 private。还有一个区别就是继承时，struct 默认的继承方式为 public ，而 class 默认的继承方式是 private 。这就是它们的区别。<br><br>Make every class in your hierarchy either a base-only or leaf only. <br><br>
The separation of the interface and its implementation is one of the crucial ideas of modern software design.
<br><br>多-态，即一种物体的多种状态。多态分为编译时多态性和运行时多态性。在OOP中，通常指后者。<br><br>也称为静态多态性，主要通过函数的重载和模板实现。C语言并不支持相同函数名的重载，C++通过将参数类型也作为符号名的一部分来支持函数的重载。如plus(int i)的符号名可能是_Z4plusi，最后面的i就表示有一个int类型的参数。如果在类中可能是这样的_ZN5Class4plus<br>由于这种多态性在编译期就确定下来了，所以效率要高一点。由于这种在编译时就确定的多态性，有时并不将静态多态性看作是真正意义上的多态。我们用模板的代码简单演示一下。<br>#include &lt;iostream&gt;

template&lt;typename T&gt;
T plus(T x){
    return x+1;
}
int main(){
    auto y = plus(20);
    auto z = plus(3.14);
    auto e = plus('c');
    return 0;
}
<br>由于模板只有在调用相应类型的时候才会实例化，所以在这段代码中，我们在编写代码时就能说出来编译后会相应地产生三个关于 plus 的函数符号。而且这三个符号是独立的，链接时并不将其看作一个函数看待。<br>编译完成后查看符号表，我们确实看到了三个不同的函数符号：<br>du@DVM:~/Desktop/DSA$ nm -n stat_poly
                 U __cxa_atexit
                 U __dso_handle
                 U _GLOBAL_OFFSET_TABLE_
                 U _ZNSt8ios_base4InitC1Ev
                 U _ZNSt8ios_base4InitD1Ev
0000000000000000 T main
0000000000000000 W _Z4plusIcET_S0_
0000000000000000 W _Z4plusIdET_S0_
0000000000000000 W _Z4plusIiET_S0_
0000000000000000 b _ZStL8__ioinit
0000000000000047 t _Z41__static_initialization_and_destruction_0ii
000000000000009d t _GLOBAL__sub_I_main

<br><br>也称动态多态性，通过虚函数和继承来实现。由于虚多态只有在程序运行时才能确定实际调用的时哪个函数，所以效率较低。（存储虚表造成的空间复杂度和虚指针索引导致的时间复杂度增加）<br>相关请参阅<a data-href="Virtual Dispatch in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Dispatch in C++</a>。<br><br>
OOP is a programming paradigm in C++ using polymorphism based on runtime function dispatch using virtual functions.
<br><br>在OOP范式中，由于派生类是基类派生而来的，所以它们的库所用的API是相同的。通过创建对象将派生类实例化，每个对象可以具有不同的状态和行为。这种特性就是由"virtual"所提供的动态多态性。即基类定义的API可以被派生类对象重写，从而在不同对象中表现出不同的实现。在编译期(compile time)，确定派生对象的类型。在运行时(runtime)，我们才能知道派生类的状态。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/object-oriented-programming-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Object Oriented Programming in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 16 Aug 2025 09:12:04 GMT</pubDate></item><item><title><![CDATA[Observer Pattern]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/observer-pattern.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Observer Pattern.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 04 Aug 2025 16:20:46 GMT</pubDate></item><item><title><![CDATA[Perfect Forwarding in C++]]></title><description><![CDATA[ 
 <br>Do these at first:<br>
<br><a data-href="Move Semantics in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/move-semantics-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Move Semantics in C++</a>
<br><a data-href="Generics Programming in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/generics-programming-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Generics Programming in C++ (NC)</a>
<br><br>在 <a data-tooltip-position="top" aria-label="Move Semantics in C++" data-href="Move Semantics in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/move-semantics-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Move Semantics</a> 中，我们提到，左值引用只能绑定左值，右值引用只能绑定右值。右值的应用让那些绑定右值的函数能够对物体直接进行所有权转移的操作，避免了不必要的拷贝。<br>然而，你会发现有时候当你传递左值给一个右值引用时，仍然行得通。但这应当是非法的。为什么呢？这是因为函数模板参数类型推导过程中会发生引用折叠。<br>#include &lt;cstdio&gt;
template&lt;typename T&gt;
void foo(T &amp;&amp;){
	printf("foo(T&amp;&amp;)");
}
int main(){
	int i = 5;
	foo(i); // i is a lvalue
	foo(5); // pass a rvalue
	return 0;
}
<br>这里会发生 "reference collapsing"。我们传入了一个左值 i，在编译器推导函数 foo() 接收的类型时，会将其推导成 T&amp; &amp;&amp;。而 C++ 有一些 reference collapsing rules 如下所示。进一步地，编译器会将类型推导成一个接受左值的函数 foo(T&amp;)。（引用折叠，自 C++11）<br><br>那传入一个右值会怎么样？传递右值给 foo 函数当然会推导出 T&amp;&amp;，从而保持右值引用的性质。也就是说，传递左值时，会折叠成左值引用；传递右值时，会保持右值引用。<br><br>有时候，我们希望写出一个既能够接受左值，也能接受右值的函数。在这种情况下，C++11 引入了转发引用（forwarding references），也称为通用引用（universal references）。了解了引用折叠是怎么回事后，我们可以利用其特性编写一个中继函数来同时接受左值和右值。再通过中继函数对参数的转发，我们就实现了完美转发。<br>要实现完美转发，需要根据原始参数的值类别（左值/右值），保留其引用性质。这些都是在中继函数中完成的。我们需要用到 std::forward 保留原始的引用类别来传递给下层的函数。<br>template&lt;typename T&gt;
T&amp;&amp; forward(typename std::remove_reference&lt;T&gt;::type&amp; arg) noexcept {
    return static_cast&lt;T&amp;&amp;&gt;(arg);
}
/*
std::move turns a arg into a rvalue
std::forward turns arg into a rvalue ref
*/
<br>如：<br>#include &lt;cstdio&gt;
#include &lt;utility&gt;

template&lt;typename T&gt;
void foo(T&amp; arg) {
    printf("lvalue foo\n");
}

template&lt;typename T&gt;
void foo(T&amp;&amp; arg) {
    printf("rvalue foo\n");
}

template&lt;typename T&gt;
void relay(T&amp;&amp; arg) {
    foo(std::forward&lt;T&gt;(arg)); // correctly forward the argument
}

int main() {
    int i = 5;
    relay(i); // i is a lvalue
    relay(5); // pass a rvalue
    return 0;
}

<br>通过使用模板参数和 std::forward，我们实现了完美转发，从而在编写泛型代码时处理不同类型的引用。<br><br>如果我们有几个重载的引用转发，我们有下面的代码， f(w) 等调用这些重载的顺序是什么？<br>#include &lt;utility&gt;
struct Widget{};
// function with lvalue ref (1)
void f( Widget&amp; ){}

// function with lvalue ref to const (2)
void f( const Widget&amp; ){}

// function with rvalue ref (3)
void f( Widget&amp;&amp; ){}

// function with rvalue ref to const (4)
void f( const Widget&amp;&amp; ){};

// function with forwarding ref (5)
template&lt; typename T &gt;
void f( T&amp;&amp; ){}

// function template with rvalue ref to const (6)
template&lt; typename T &gt;
void f( const T&amp;&amp; ){}

int main(){
	Widget w{};
	f(w); // (1), (5), (2)

	const Widget w2{};
	f(w2); // (2), (5)

	f(std::move(w)); // (3), (5), (4), (6), (2)

	const Widget w3{std::move(w)};
	f(w3); // (4), (6), (5), (2)
	return 0;
}
<br>Avoid overloading on forward references.]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/perfect-forwarding-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Perfect Forwarding in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 13:30:45 GMT</pubDate></item><item><title><![CDATA[Post-Increment and Pre-Increment in C++ (NC)]]></title><description><![CDATA[ 
 <br>后置递减和前置递减的原理是一样的。<br><br>后置自增和前置自增运算符其实并不难理解<br>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/post-increment-and-pre-increment-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Post-Increment and Pre-Increment in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 20 Apr 2025 08:06:44 GMT</pubDate></item><item><title><![CDATA[Preprocessor in C++ (Part I, NC)]]></title><description><![CDATA[ 
 <br><br>#&lt;directives&gt;&lt;stuff&gt;
<br>A directive could be:<br>
<br>include (C++ modules as an alternative)
<br>error
<br>conditional
<br>compiler extension
<br>replace
<br>modules (C++23)
<br>Preprocessor stack is fun]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/preprocessor-in-c++-(part-i,-nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Preprocessor in C++ (Part I, NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 26 May 2025 10:59:12 GMT</pubDate></item><item><title><![CDATA[RAII and Scope in C++]]></title><description><![CDATA[ 
 <br><br><br>在C++中，程序会管理许多资源，比如内存、POSIX 文件、网络 I/O 和互斥锁等。我们能够通过某些表达（系统调用、库函数等）来获取资源，同样地，我们也能通过某些表达来释放资源。<br>资源的正确管理十分重要，错误的资源的管理会导致资源的泄漏，比如内存的泄漏（可能导致系统崩溃）、文件句柄的泄漏（可能导致无法创建文件）、持续上锁的互斥锁（可能导致死锁）等。除此，还可能造成资源的双重释放、释放后使用等问题。<br><br>C++ 是面向对象的语言，在 C++ 中，对象的生命周期是可以被定义的。对象生命的开始与结束都相应的会对应事件的发生。这些事件背后的代码会在对象生命的开始与结束自动的执行对象初始化和销毁对象的代码。我们把对象初始化发生的事件称为构造函数(constructors)，而对象销毁时发生的事件称作析构函数(destructors)。<br>当我们需要创建对象时，我们通过构造函数初始化对象。而当对象需要销毁时我们则需要考虑额外的因素。对于局部变量对象而言，当期超出作用域时，就会自动地调用析构函数。而对于哪些动态分配内存的对象，使用delete或delete[]显式地释放内存时，析构函数才会被调用。<br><br>
The RAII class is said to "own" the resource. It is responsible for cleaning up that resource at the appropriate time.
<br><br>Resource&nbsp;acquisition&nbsp;is&nbsp;initialization（RAII），对象获取即初始化；或称作&nbsp;Constructor&nbsp;Acquires,&nbsp;Destructor&nbsp;Releases (CADRe)，即构造函数获取资源，析构函数释放资源。这是最初用于C++的编程惯用法，旨在通过对象的生命周期来管理资源的获取和释放。<br>通过RAII，我们可以显式地定义相应的构造函数和析构函数，在对象生命周期的开始通过构造函数自动初始化并获取资源，并在对象超出作用域或删除对象时自动调用析构函数来释放资源，避免资源泄漏。需要显式定义的函数包括析构函数、拷贝构造函数、拷贝赋值运算符、移动构造函数、移动赋值运算符，这被称为"The&nbsp;Rule&nbsp;of&nbsp;Five"。<br>标准库中大量使用RAII和The&nbsp;Rule&nbsp;of&nbsp;Five，从而达到自动释放资源的目的。通过使用标准库和RAII，我们能够尽量避免手动管理资源，从而简化代码，提高安全性和可维护性。<br><br><br>局部对象就是在栈上创建的对象。当你在栈上创建对象时，它会自动调用构造函数。这是因为栈上的对象具有自动存储期（automatic storage duration），它们的生命周期由作用域(scope) 控制。当执行到 '}' 时，就会自动调用析构函数，随之栈帧销毁、函数返回。<br>#include &lt;iostream&gt;

class scope
{
public:
	scope(){
		std::cout &lt;&lt; "scope constructor" &lt;&lt; std::endl;
	}
	~scope(){
		std::cout &lt;&lt; "scope deconstructor" &lt;&lt; std::endl;
	}
};
struct test
{
	test(){
		std::cout &lt;&lt; "test constructor" &lt;&lt; std::endl;
	}
	~test(){
		std::cout &lt;&lt; "test deconstructor" &lt;&lt; std::endl;
	}
};

int main(){

	scope s;
	test t;
	return 0;
} // &lt;- End of scope
<br>du@DVM:~/Desktop/Cpp$ ./scope 
scope constructor
test constructor
test deconstructor
scope deconstructor
<br><br>由于堆上对象的生命周期通常比栈上对象的生命周期更长，因此堆上对象的构造和析构通常与&nbsp;new&nbsp;和&nbsp;delete&nbsp;关键字的使用相关联。当你使用&nbsp;new&nbsp;关键字创建一个新的对象时，会调用该对象的构造函数；而当你使用&nbsp;delete&nbsp;关键字销毁对象时，会自动调用该对象的析构函数。<br><br>malloc 和 free 函数改变为 new 和 delete 关键字的使用是从C语言过渡到C++内存管理方式的改进。虽然 new 和 delete 关键字在底层还是会调用 malloc 和 free，但是使用 new 和 delete 会调用对象的构造函数和析构函数，这是 malloc 和 free 所不具有的。<br>也就是说，new和delete提供RAII这种机制。这也是 new、delete 和 malloc、free 的最主要的差别。以下展示 new 和 delete的底层实现：<br>#include &lt;iostream&gt;
#include &lt;cstdlib&gt; // for malloc and free

void* operator new(size_t size) {
    void* ptr = std::malloc(size);
    if (!ptr) {
        throw std::bad_alloc();
    }
    return ptr;
}

void operator delete(void* ptr) noexcept {
    std::free(ptr);
}

class Entity {
public:
    Entity() {
        std::cout &lt;&lt; "Entity created" &lt;&lt; std::endl;
    }
    ~Entity() {
        std::cout &lt;&lt; "Entity destroyed" &lt;&lt; std::endl;
    }
};

int main() {
    Entity* e = new Entity();
    delete e;
    return 0;
}
<br><br>
<br>智能指针：C++11 引入了&nbsp;std::unique_ptr&nbsp;和&nbsp;std::shared_ptr&nbsp;智能指针。它们在构造时获取动态内存，在析构时自动释放内存。
<br>互斥锁：在多线程编程中，std::lock_guard&nbsp;和&nbsp;std::unique_lock&nbsp;利用 RAII 管理互斥锁的获取和释放。
<br>文件操作：C++ 标准库中的&nbsp;std::ifstream&nbsp;和&nbsp;std::ofstream&nbsp;也使用 RAII 来管理文件的打开和关闭。
<br><br>当我们想要申请堆内存资源时，我们会用 new 和 delete 关键字来申请和释放我们的内存资源。这两者总是成对出现，即当我们使用 new ，就不要忘记使用 delete。但是智能指针为我们提供了省去使用 delete 的便利。<br>智能指针有三种：<br>
<br>Unique pointers：是最简单的智能指针。unique_ptr可以看作一种作用域指针(scoped pointer)，在超出作用域时会自动销毁所管理的对象。（RAII）
<br>Shared pointer：引用计数，多个shared_ptr可以共享同一个对象。当引用数为0，自动释放堆内存资源。（RAII）
<br>Weak pointer：不增加引用计数，解决shared_ptr之间循环引用的问题。辅助 shared_ptr 防止循环计数导致的内存资源泄漏。
<br>智能指针实际上就是对裸指针的封装，实际上两个关键字仍然成对出现。其中规定 unique_ptr 所指向的堆对象只能有一个引用，不可以拷贝，也就是为什么叫做 unique pointer。例如：<br>std::unique_ptr&lt;Entity&gt; entity0 = std::make_unique&lt;Entity&gt;();
auto entity1 = entity0; // error
<br>unique_ptr 还提供异常安全性（exception safety）。也就是说它能够在异常发生时确保资源的正确释放，避免资源泄漏。具体来说，unique_ptr&nbsp;通过 RAII机制管理动态分配的内存，确保在对象生命周期结束时自动释放内存。<br>#include &lt;iostream&gt;
#include &lt;memory&gt;

class Entity
{
public:
	Entity(){
		std::cout &lt;&lt; "Entity created" &lt;&lt; std::endl;
	}
	~Entity(){
		std::cout &lt;&lt; "Entity destroyed" &lt;&lt; std::endl;
	}
	
};

template&lt;class T&gt;
class scopedPointer
{
private:
	T* m_ptr;
public:
	scopedPointer(const T* other_ptr) = delete;
	scopedPointer(T* other_ptr) = delete;
	scopedPointer&amp; operator=(const scopedPointer&amp; other) = delete;
	scopedPointer(T* ptr)
		: m_ptr(ptr)
	{
	}
	~scopedPointer(){
		delete m_ptr;
	}
    scopedPointer&amp; operator=(scopedPointer&amp;&amp; other) noexcept {
        if (this != &amp;other) {
            delete m_ptr;
            m_ptr = other.m_ptr;
            other.m_ptr = nullptr;
        }
        return *this;
	}
};
int main(){
	{
		scopedPointer* s_ptr = new Entity();

	}//&lt;--s_ptr销毁的时刻
	{
		std::unique_ptr&lt;Entity&gt; entity(new Enity()); 
		std::unique_ptr&lt;Entity&gt; entity = std::make_unique&lt;Entity&gt;();
	}
	std::cin.get();
	return 0;
}
<br>Copyable? NO! Movable? YES! 由于unique_ptr可移动不可拷贝，所以我们应当删去拷贝构造函数和对拷贝赋值运算符的重载。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/raii-and-scope-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/RAII and Scope in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 01 Aug 2025 01:25:37 GMT</pubDate></item><item><title><![CDATA[RVO in C++ (ENG)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Call Stack in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/call-stack-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Call Stack in C++ (ENG)</a><br>(U)RVO<br>
the return value is a unnamed prvalue<br>
since C++17, the compilers are required to provide URVO support<br>NRVO Named Return Value Optimization<br>
the return value is a named lvalue<br>
optional but recommended<br>you can turn off RVO using<br>
-fno-elide-constructors flags<br>
for GCC and Clang<br>better performance than move constructor<br><br>disable flags applied<br>when the object construction happens outside the scope of the current function<br>// no RVO
Widget a = Widget(params);
Widget Process() {
	return a;
}
// no RVO
Widget Process(Widget b{Widget(params)}) {
	return b;
}


// NRVO
Widget Process() {
	Widget c = Widget(params);
	return c;
}

// URVO
Widget Process() {
	return Widget(params);
}
<br>the return type is not the same as what's being returned<br>
CV qualifiers don't affect the RVO<br>When there are multiple return statements, returning different objects (for NRVO only), because at compile time, compiler doesn't know which branch is going to hit (is this real?)<br>
When there's expressions only runs at run-time<br>When returning a complex expression (NRVO only)<br>
std::move is a complex expression, ((expression) ? A : B) is also a complex expression<br>
even a+=1 is a complex expression<br>For guaranteed URVO has no need to move or copy constructor required<br><br>If writing a copy/move constructor<br>Never make the constructor do anything else other than a copy/move<br>Because constructors can get elided<br><br>when the name of a non-volatile object:<br>// no RVO, volatile back-off any optimization
Widget example() {
	Widget volatile a = Widget(1);
	return a;
}
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/rvo-in-c++-(eng).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/RVO in C++ (ENG).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 09 Aug 2025 18:17:41 GMT</pubDate></item><item><title><![CDATA[Singleton Pattern (NC)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Static Keyword in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/static-keyword-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Static Keyword in C++</a><br><br>单例模式很好描述，也很好理解。它的核心思想是：确保一个类最多有一个实例，并提供外界一个全局的访问点。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/singleton-pattern-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Singleton Pattern (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 05 Aug 2025 11:24:09 GMT</pubDate></item><item><title><![CDATA[Smart Pointers in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="RAII and Scope in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/raii-and-scope-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">RAII and Scope in C++</a><br><br>在 C++ 中，我们一般会使用 new 和 delete/delete[] 在堆上实例化和删除对象。如果你使用裸指针时，同一个作用域中的 new 和 delete 的数量必须严格匹配，不然就可能引起内存泄漏或是未定义行为。<br>在使用裸指针删除对象时，你还需要注意类型的匹配：单个对象使用 delete，数组对象使用 delete[]。如果使用错误的释放方式，就可能引起未定义行为。而通过裸指针的，声明你并不能确定其到底是一个单个的对象还是一个数组。<br>此外，如果有多个裸指针指向同一个对象，一当该对象被销毁，那么其他没有被置为 nullptr 的指针就会变成悬挂指针 (dangling pointers)，而且对于裸指针，你没法知道到底指针有没有悬挂。更何况裸指针还不支持 RAII，即使裸指针的使用天衣无缝，异常的发生也可能导致一些资源泄漏问题。尽管裸指针的使用强大高效，但是人为地管理这些资源的释放是容不得一点粗心的。<br>为了避免以上问题，C++ 为我们提供了智能指针。智能指针是对裸指针的封装抽象，而通过运算符重载，智能指针的行为又和普通裸指针别无二致。<br>C++11/C++14标准中，我们有四种类型的智能指针，分别是：<br>
<br>std::auto_ptr<br>
C++98:&nbsp;初次引入，独占所有权的智能指针。<br>
C++11:&nbsp;被标记为过时 (Deprecated)。<br>
C++17:&nbsp;被移除。
<br>std::unique_ptr<br>
C++11:&nbsp;引入作为&nbsp;auto_ptr&nbsp;的替代品，独占所有权的智能指针。<br>
C++14:&nbsp;添加了&nbsp;make_unique&nbsp;工厂函数。
<br>std::shared_ptr<br>
C++11:&nbsp;引入引用计数智能指针。和 make_shared&lt;T&gt;&nbsp;工厂函数。<br>
C++17:&nbsp;增添对数组类型的支持。<br>
C++20:&nbsp;添加了对&nbsp;make_shared&lt;T[]&gt;&nbsp;的支持。
<br>std::weak_ptr<br>
C++11:&nbsp;引入“弱”引用智能指针，用于解决循环引用问题。
<br>std::auto_ptr 后来被 std::unique_ptr 所替代，原因是 C++98 并没有移动语义，而 std::auto_ptr 又提供对资源的独占权语义，所以 std::auto_ptr 智能通过拷贝的语义来模拟移动。看起来是拷贝，但实际上是移动，导致被拷贝的指针变为 null，这种行为在 C++98 是很迷惑人的。C++11 引入移动语义后，std::unique_ptr 就来替代原先语义矛盾的 std::auto_ptr。<br>这些智能指针通过对裸指针的包装，使得我们可以放心地申请资源而不担心发生任何的资源泄漏。这些智能指针会管理动态申请对象的生命周期，并在合适的适合销毁对象（包括异常事件）。<br><br>当你想使用智能指针来自动管理资源时，你第一个该想到的就是 std::unique_ptr。作为一种作用域指针，像我们在 <a data-href="RAII and Scope in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/raii-and-scope-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">RAII and Scope in C++</a> 中展示的一样，std::unique_ptr的大小和裸指针的大小是一样的。当你使用std::unique_ptr时，所使用的代码和使用裸指针时的是一样的。<br><img alt="Pasted image 20241023154500.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023154500.png"><br><br>由于 std::unique_ptr&nbsp;提供独占所有权的特性，在任何时候，std::unique_ptr&nbsp;都能保证只能有一个指针拥有资源。所以我们不能对 std::unique_ptr 进行任何拷贝操作，只能进行移动操作。试想，如果我们对&nbsp;std::unique_ptr&nbsp;进行了拷贝，那就会有两个拥有对象所有权的指针，这与其的设计初衷相悖。当&nbsp;std::unique_ptr&nbsp;移动时，所有权转移，原来的指针会丢失对资源的所有权（nullptr），从而保持独占所有权的特性。<br><img alt="Pasted image 20241023153930.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023153930.png"><br>std::unique_ptr移动的操作示例：<br>#include &lt;memory&gt;
// Create a unique pointer using make_unique factory function.
std::unique_ptr&lt;Entity&gt; uptr1 = std::make_unique&lt;Entity&gt;();
// or `std::unique_ptr&lt;Entity&gt; uptr1 = std::unique_ptr&lt;Entity&gt;(new Entity);`

// Change ownership to uptr2.
std::unique_ptr&lt;Entity&gt; uptr2 = std::move(upre1);
<br><br>当你需要一个指向数组的&nbsp;unique&nbsp;pointer&nbsp;时，我们就要用到&nbsp;std::unique_ptr&lt;T[]&gt;。这种智能指针在析构时会调用&nbsp;delete[]，以正确释放数组内存。<br><img alt="Pasted image 20241023154839.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023154839.png"><br><br>除了模板参数T之外，std::unique_ptr也一直有第二个模板参数，我们称之为 deleter，即删除器。删除器的作用是什么呢？智能指针难道不应该接管一切么？智能指针让我们免于资源泄漏，但是我们需要删除器来释放这些资源。<br>如果我们不自定义 deleter，那 std:unique_ptr就会使用默认的删除器std::default_delete （和 delete 关键字的行为一样）。默认的删除器只负责对堆上动态资源进行释放。如果想要通过智能指针管理非常见的资源（非堆上资源），或是在释放资源时打印一些 logging，那我们就需要定义一个自己的删除器。<br><img alt="Pasted image 20241023155605.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023155605.png"><br>如果 deleter 是一个函数指针，那么 std::unique_ptr&lt;T, deleter&gt; 就会变成两个裸指针的大小。而如果 deleter 是一个函数对象（如 lambda ），那么根据其 capture list，deleter 的大小也会随之改变，如果 lambda 无状态，那么 deleter 将不会额外占用空间。<br>template&lt;class T, class Deleter = std::default_delete&lt;T&gt;&gt;
class unique_ptr {
    T *p_ = nullptr;
    Deleter d_;
public:
    explicit unique_ptr(T* p = nullptr, Deleter d = Deleter()) : p_(p), d_(d) {}
    ~unique_ptr() {
        if (p_) d_(p_);
    }
    // Copy is not allowed.
    unique_ptr(const unique_ptr&amp;) = delete;
    unique_ptr&amp; operator=(const unique_ptr&amp;) = delete;
    // Object is movable.
    unique_ptr(unique_ptr&amp;&amp; other) noexcept : p_(other.p_), d_(std::move(other.d_)) {
        other.p_ = nullptr;
    }
    unique_ptr&amp; operator=(unique_ptr&amp;&amp; other) noexcept {
        if (this != &amp;other) {
            if (p_) d_(p_);
            p_ = other.p_;
            d_ = std::move(other.d_);
            other.p_ = nullptr;
        }
        return *this;
    }
    T* get() const { return p_; }
    T* release() { T* tmp = p_; p_ = nullptr; return tmp; }
    void reset(T* p = nullptr) {
        if (p_ != p) {
            if (p_) d_(p_);
            p_ = p;
        }
    }
};
template&lt;class T&gt;
struct default_delete {
    void operator()(T *p) const { delete p; }
};
<br>如果我们想在作用域内释放内存，我们可以使用reset()方法提前释放内存，相当于原先 delete 关键字的作用，或者置一个 std::unique_ptr 指向一个新的资源。<br><br>文件句柄、网络连接、设备句柄等这些资源可能并不是在堆上分配的，我们并不能用delete删除这些资源。Deleter的作用就显现出来了，对那些不能delete的资源进行管理。智能指针可以通过自定义删除器来正确释放它们。假设我们有一个数据库连接：<br>struct DBConnection{
// All kinds of resources.
// All kinds of methods.
};
struct DBDeleter{
	void operator()(DBConnection* db){
		std::cout &lt;&lt; "Closing Database Connection.\n";
		dbClosing(db);
	}
};
class DBManager{
	std::unique_ptr&lt;DBConnection, DBDeleter&gt; dbConnect_;
public:
	explicit DBManager(DBConnection* dbConnect) : dbConnect_(dbConnect){}
	void query(const std::string&amp; q){
		if(dbConnect_){
			db_query(dbConnection_.get(), q.c_str());
		}
	}
	void close(){
		dbConnection_.reset();
	}
};

int main(){
	DBConnection* db = dbOpen("example.db");
	DBManager dbManager(db);
	dbManager.query("SELECT * FROM Student");
	// dbManager.close(); // &lt;-Close the database now.
	return 0;
} // &lt;- Close the database by using DBDeleter.
<br><br>std::shared_ptr&nbsp;是使用上最接近裸指针的智能指针。不同于上一节中的&nbsp;std::unique_ptr，使用&nbsp;std::shared_ptr&nbsp;创建的对象的所有权是由所有指向该对象的&nbsp;std::shared_ptr&nbsp;共享的。只有当最后一个&nbsp;std::shared_ptr&nbsp;停止指向对象时，对象的资源才会被释放。<br><br>std::shared_ptr&nbsp;的这种共享特性是通过原子类型的引用计数来实现的。通过引用计数，就能知道当前有多少个 std::shared_ptr 指针在共享资源，在析构时，只需要检查当前引用计数是否为 1 就可以控制对象资源的释放。这种对对象资源的控制使得&nbsp;std::shared_ptr&nbsp;需要额外的控制块来管理对象资源的释放。那么控制块放到哪里？<br>控制块通常会放在堆上，与对象一起分配。这样做的好处是确保控制块的生命周期与对象的生命周期一致。当所有指向对象的&nbsp;shared_ptr&nbsp;被销毁时（并且这时如果weak ref count = 0），控制块也会被自动释放。如果控制块放在栈上，当函数返回或栈帧被销毁时，控制块就会被销毁，而不管是否还有&nbsp;shared_ptr&nbsp;指向对象。这将导致引用计数失效，可能在某些情况下导致对象被提前销毁，或者造成资源泄漏。<br>因而，构造 std::shared_ptr 时除了指向资源的指针外还会额外需要一个指向控制块的指针，因而，std::shared_ptr 的大小是裸指针大小的两倍大。当std::unique_ptr转换为std::shared_ptr时，构造函数也会生成相应的控制块。<br><img alt="Pasted image 20241024004507.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241024004507.png"><br>
要实现引用计数，我们在构造函数和进行拷贝时中加入reference_count + 1; 的操作。并且要在析构函数或在 std::shared_ptr 等于 nullptr 时将引用计数减一。当引用计数为 0 时释放掉对象资源。因为在对引用计数（弱引用计数也一样）进行操作时，++、--是 read-modify-write 操作指令，所以对引用计数的操作必须是原子性的。<br>下面我们对 std::shared_ptr 进行拷贝时，我们拷贝了 ptr to T 和 ptr to control block，并使得引用计数增加1。但当我们进行移动操作时，我们不需要对引用计数进行操作，这是因为所有权转移了。另外，在下面第二种情况下，控制原先对象（ReadBuffer）的控制块中的引用计数需要减 1。<br>std::shared_ptr&lt;Resource&gt; pResource1 = new Resource;
std::shared_ptr&lt;Resource&gt; pResource2;
pResource2 = pResource1; // Ref count + 1

std::shared_ptr&lt;ReadBuffer&gt; pReadBuf = new ReadBuffer;
pReadBuf = pResource1;  // ref count for shared_ptr&lt;Resource&gt; + 1
						// ref count for shared_ptr&lt;ReadBuffer&gt; - 1

<br><img alt="Pasted image 20241024005717.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241024005717.png"><br><br>我们已经有一个指向对象的指针了，为什么控制块中还有指向对象的指针？假设我有这几个类：<br>struct Fruit { int juice; }; 
struct Vegetable { int fiber; }; 
struct Tomato : Fruit, Vegetable { int sauce; };
<br>当我创建了：<br>std::shared_ptr&lt;Tomato&gt; tomato = make_shared&lt;Tomato&gt;;
std::shared_ptr&lt;Fruit&gt; fruit = tomato;
std::shared_ptr&lt;Vegetable&gt; fruit = tomato;
<br>当我们创建一个指向&nbsp;Tomato&nbsp;实例的&nbsp;std::shared_ptr&lt;Fruit&gt;&nbsp;或&nbsp;std::shared_ptr&lt;Vegetable&gt;，这些指针会指向&nbsp;Tomato&nbsp;对象内的&nbsp;Fruit&nbsp;或&nbsp;Vegetable&nbsp;部分。由于控制块仍然是共享的，它们最终管理的是同一个&nbsp;Tomato&nbsp;实例，所以控制块会指向这个完整的对象。<br>而且由于 shared_ptr 共享对象的所有权，所以只有控制块中的指针才能够控制对象的生命周期，即释放其资源。即使引用计数为 1 （当前独占所有权），你也不能提前删除对象。<br><br>和&nbsp;std::unique_ptr&nbsp;一样，std::shared_ptr&nbsp;也使用&nbsp;delete&nbsp;作为其默认的资源销毁机制（Resource-destruction&nbsp;mechanism）。但是不同的是，每个&nbsp;std::unique_ptr&nbsp;独立持有对所指对象的控制权，而&nbsp;std::shared_ptr&nbsp;的控制权是共享的。对象资源的控制由控制块来完成。<br><img alt="Pasted image 20241024015023.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241024015023.png"><br>这种由于控制权的不同，就使得std::unique_ptr本身有权力来管理对象。所以std::unique_ptr中，deleter 是智能指针的一部分。<br>struct CustomDeleter {
    void operator()(int* p) const {
        delete p;
    }
};
// Deleter type is part of the smart pointer.
std::unique_ptr&lt;int, CustomDeleter&gt; p1(new int); 
<br>而std::shared_ptr不同，没有指针真正拥有对象的所有权，所有权由控制块管理管理。因此，在std::shared_ptr中，deleter 并不作为智能指针的一部分。而控制块又只有一个，所以即使我们有两个std::shared_ptr，只要指向资源相同，deleter就是相同的：<br>struct CustomDeleter {
    void operator()(int* p) const {
        delete p;
    }
};
// Deleter is not part of the smart pointer.
std::shared_ptr&lt;int&gt; p(new int, CustomDeleter()); 
std::shared_ptr&lt;int&gt; p2 = p; 
<br><br>当我们用工厂函数创建好std::shared_ptr了之后，我们在堆上会存在两块空间，这就会产生两次内存申请的开销。但我们可以将这两块空间合并起来，以避免外部碎片的产生。可庆的是，大多数的编译器都支持这种实现方式。如果你使用 make_shared 工厂函数来创建共享指针，那么堆内存两块空间就会一块申请，而其空间布局就会优化成这样：<br><img alt="Pasted image 20241024145414.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241024145414.png"><br>由于控制块和对象资源在同一块内存块中，编译器甚至能够将控制块中的指针优化掉。<br><img alt="Pasted image 20241024145703.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241024145703.png"><br>但是这样也有弊端，那就是对象的生命周期将和控制块一致，而现实中，控制块的生命周期通常比对象的生命周期要长。<br><br>在构造 std::shared_ptr&lt;A&gt; 时，你可以传入 std::unique_ptr&lt;A&gt;、std::shared_ptr&lt;B&gt;、std::weak_ptr&lt;B&gt; 等智能指针，但你需要避免使用裸指针来创建 std::shared_ptr 对象。一是因为你不能确保裸指针不被误用，而且还可能发生：<br>auto ptr = new A;
{
	std::shared_ptr&lt;A&gt; sptrA1(ptr); // create 1st control block for *ptr
	std::shared_ptr&lt;A&gt; sptrA2(ptr); // 2nd control block is created...
} // double delete
<br>如果有特殊的需求，没有裸指针就手痒得不行，那你可以直接用裸指针来创建共享型智能指针，或者先创建 std::unique_ptr，然后再把其转换成 std::shared_ptr：<br>std::shared_ptr&lt;A&gt; sptrA(new A);

std::unique_ptr&lt;A&gt; uptrA(new A);
std::shared_ptr&lt;A&gt; sptrA = std::move(uptrA);
<br><br>接着上面关于裸指针的讨论，除了 new 出来的裸指针外，我们还需要注意非常容易被忽略的裸指针——this指针。由于智能指针能够自动管理对象声明周期，所以在一些资源并非严格要求互斥的情况下，我们会使用 std::shared_ptr 来自动地管理对象的声明周期。比如下面的例子：<br>std::vector&lt;std::shared_ptr&lt;Widget&gt;&gt; processedWidgets;
class Widget{
private:
...
public:
...
	void process();
...
};

void Widget::process() {
...
// After process
processedWidgets.emplace_back(this); // Compiles, but disaster.
}
<br>还是<a data-tooltip-position="top" aria-label="Smart Pointers in C++ > Forbid Any Raw Pointers" data-href="Smart Pointers in C++#Forbid Any Raw Pointers" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/smart-pointers-in-c++.html#Forbid_Any_Raw_Pointers" class="internal-link" target="_self" rel="noopener nofollow">这里</a>同样的问题，使用 this 构造 std::shared_ptr 后，C++ 就会为这个 this 新建一个对 *this 资源的控制块，和上面裸指针的毛病是一样的。<br>为了让类成员函数通过 this 指针来安全地创建指向自身的 std::shared_ptr，std::shared_ptr API 提供 std::enable_shared_from_this&lt;T&gt;，这是一个辅助性质的基类模板，而类型参数总是派生类的类型。如：<br>std::vector&lt;std::shared_ptr&lt;Widget&gt;&gt; processedWidgets;
class Widget : public std::enable_shared_from_this&lt;Widget&gt; {
private:
...
public:
...
	void process();
...
};
<br>这种基类使用派生类作为模板类型参数的行为看似很奇怪，但是这样的确可以让基类得到派生类的类型信息，这种类背后的设计模式叫 The Curiously Recurring Template Pattern (CRTP)。std::enable_shared_from_this 提供一个我们要用到的成员函数:std::shared_from_this()，它的作用就是为当前对象安全地创建 std::shared_ptr，问题解决了。<br>采用 std::shared_from_this() 后，我们就不需要担心双重释放等问题了。如：<br>void Widget::process() {
	...
	// After process
	processedWidgets.emplace_back(std::shared_from_this());
}
<br>当使用 std::shared_from_this() 时，它会查找有没有指向当前对象的 std::shared_ptr，然后复用这个智能指针。如果找不到任何管理当前对象的 std::shared_ptr，那么 std::shared_from_this() 就会抛出异常。<br>为确保构造对象时已经有 std::shared_ptr 指向该对象，通常将构造函数放在 private 域中，然后提供 public 的工厂函数，在工厂函数中构造对象并让 std::shared_ptr 指向该对象（使用 std::make_shared()）。比如：<br>class Widget : public std::enable_shared_from_this&lt;Widget&gt; {
private:
    Widget() {}

public:
    static std::shared_ptr&lt;Widget&gt; create() {
        return std::make_shared&lt;Widget&gt;();
    }
    void process() {
        auto self = shared_from_this();
    }
};
<br><br>std::shared_ptr 可以自动管理对象的生命周期，每次 std::shared_ptr 的拷贝都会使得引用计数加一，进而延长资源的存活时间。而在实现某些设计模式时，我们可能想拥有行为类似于 std::shared_ptr ，但又不参与资源所有权的智能指针。这种非拥有型的智能指针允许其指向对象被销毁（悬空），并根据观察对象的状态做出不同的反应。简而言之，我们想要一个资源观察者：如果资源仍在，我们就安全地使用它，如果资源已被销毁，则采取其他策略。<br>C++11 引入的 std::weak_ptr 弱指针就是这样一个资源的观察者：行为上类似 std::shared_ptr，但又不参与对所有权的管理。std::weak_ptr 和 std::shared_ptr 底层使用的数据结构是相同的，所以 std::weak_ptr 的创建依赖于已经存在的 std::shared_ptr。<br><img alt="Pasted image 20241214224127.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241214224127.png"><br>std::weak_ptr 可以由 std::shared_ptr 或 std::weak_ptr 拷贝赋值。在创建 std::weak_ptr 时，管理对象生命周期的引用计数并不会增加，而弱引用计数 (weak count) 会增加（表示有多少个 weak_ptr 在观察资源），而且当 std::shared_ptr 销毁后，std::weak_ptr 仍会指向原先资源的控制块。（控制块会在弱引用计数清零后销毁）<br><img alt="Pasted image 20241214224219.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241214224219.png"><br>std::weak_ptr API 提供 expired() 方法来检测资源是否已经被销毁：<br>auto sptr = std::make_shared&lt;A&gt;();
std::weak_ptr&lt;A&gt; wptr(sptr);

sptr.reset(); // resource released safely

if (wptr.expired()) {
	// wptr now dangle
} else {
	// can we use it?
}
<br>假如我们检测时资源没有被销毁，是否意味着我们可以在 else 分支中使用其指向的资源呢？因为弱指针不拥有对资源的所有权，所以 std::weak_ptr 天生就不能解引用。即使可以解引用，这种分开的检测+解引用的模式也会导致数据竞争问题。所以我们需要原子化地检测+解引用的方式，为解决这一问题 API 给我们提供了 lock() 方法：<br>auto sptr = std::make_shared&lt;A&gt;();
std::weak_ptr wptr(sptr);
...
auto sptr2 = wptr.lock();
if (sptr2) {
	// sptr2 not null, the resource has not been destroyed
} else {
	// is null
}
<br>由于 std::weak_ptr 不拥有对所指向资源的所有权，所以实际上 lock() 方法会返回一个 std::shared_ptr 对象。而且 lock() 操作是原子化的，所以不必担心数据竞争的问题。如果 std::weak_ptr 已经 expired 了，那么 lock() 就会返回 nullptr。<br>虽然你可以直接使用 std::weak_ptr 构造 std::shared_ptr，但如果那时已经 expired 了，那么就会抛出。<br>auto sptr = std::make_shared&lt;A&gt;();
std::weak_ptr wptr(sptr);
...
std::shared_ptr&lt;A&gt; sptr2(wptr); // throw std::bad_weak_ptr if expired
<br><br>最开始就说在实现某些设计模式时，我们会想要一个“资源观察者”：如果资源仍在，我们就安全地使用它，如果资源已被销毁，则采取其他策略。假设我们要设计一个缓存系统，资源以唯一 ID 的方式存储在数据库中。为了让缓存的资源让所有缓存系统的 client 共享，避免在内存中出现多份资源拷贝，所以我们不能用 std::unique_ptr，因为它具有独占所有权，不支持共享访问。<br>另外，虽然作为 C++ 的垃圾回收机制的 std::shared_ptr 支持多个引用共享资源，但它的问题在于：只要还有任何一个 shared_ptr 指向该资源，资源就不会被释放。这对于缓存系统来说并不理想，因为我们希望在没有任何用户使用资源时自动释放它，以节省内存。<br>这就是引入 std::weak_ptr 的意义之一了，通过结合 shared_ptr 和 weak_ptr，我们就可以满足：<br>
<br>保证资源只存在一份 (shared_ptr)
<br>支持多用户安全访问 (std::weak_ptr::lock())
<br>无人使用时自动释放资源 (weak_ptr 不拥有资源，支持资源的悬空)
<br>可以检测资源是否已被销毁 (weak_ptr 能够检测资源状态)
<br>下面是一个用 std::weak_ptr 实现的简单的缓存系统，在有人使用时，就加载缓存。没人使用时（出作用域）就释放资源。<br>std::shared_ptr fastLoadWidget(WidgetID id) { 
	static std::unordered_map&lt;WidgetID, std::weak_ptr&lt;Widget&gt; cache; 
	auto objPtr = cache[id].lock(); // objPtr is std::shared_ptr 
									// to cached object (or null 
									// if object's not in cache) 

	if (!objPtr) {                  // if not in cache, 
		objPtr = loadWidget(id);    // load it 
		cache[id] = objPtr;         // cache it 
	} 
	return objPtr; 
}
<br><br>weak_ptr 还有一个很重要的应用就是避免 std::shared_ptr 的循环引用问题。假如我们现在有三个对象 A, B 和 C，然后 A 和 C 共享 B 的生命周期：<br><br>如果我们需要让 B 中的一个指针指向 A，那么我们需要什么指针？<br><br>裸指针是用不成的，因为引入裸指针可能造成一下未定义行为。比如 A 在某时刻释放掉了，但是 C 仍指向 B，这时 B 指向 A 的裸指针就说一个悬挂指针了，而你不能解引用一个悬挂指针。<br>现在看智能指针：<br>
<br>unique_ptr：使用 unique_ptr 一般来说不会有问题，但你需要保证 A 资源所有权的独占。因为 unique_ptr 是独占所有权的指针，如果不能确定 A 是否还被其他对象所引用，那 B 就无法独占 A，即不能使用 unique_ptr。
<br>shared_ptr：这种模式下，A 和 B 就各有一个 shared_ptr 指向对方，形成 shared_ptr 的循环引用。在作用域结束时，两个 shared_ptr 都会阻止对象资源的释放（即引用计数为一，不会降为零）。一旦循环引用发生，资源就会泄漏，除非程序退出，否则无法访问泄漏的资源。
<br>weak_ptr：如果你不能确定 A 资源所有权是独占的，那么 weak_ptr 就是你唯一的选择了。因为 weak_ptr 可以检测底层资源的状态，所以不用担心指针的悬挂；此外，由于 weak_ptr 还不拥有对资源的所有权，所以也不需要担心循环引用问题。（如果需要使用 lock()，也可以保证 lock() 后获得的 shared_ptr 先释放）
<br>这里可能有疑问，那就是既然 lock() 操作会返回一个 shared_ptr，那这个返回的指针不会造成循环引用么？答案是不会，因为使用 lock() 创建的 shared_ptr 是栈上的临时变量，会随着栈帧销毁而释放。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/smart-pointers-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Smart Pointers in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 11 Aug 2025 09:07:30 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023154500.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/pasted-image-20241023154500.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[SOLID Design Principles (NC)]]></title><description><![CDATA[ 
 <br><br>This note is all about the SOLID principals, which define five fundamental design principles for writing solid code. Those principles are:<br>
<br>The Single Responsibility  Principle (SRP)
<br>The Open/Closed Principle (OCP)
<br>The Liskov Substitute Principle (LSP)
<br>The Interface Segregation Principle (ISP)
<br>The Dependency Inversion Principle (DIP)
<br>The SOLID principles rooted in object-oriented programming, it should be significantly improving OOP-based code structure and maintainability, making software easier to extend and modify with a good understanding of these principles. Who knows, everything's trade-offs today.<br><br>"A class should have only one reason to change."<br>hard to know if you are get it right, easy to know if you are get it wrong<br><br>A class should have a single, well-defined purpose. If a class has multiple responsibilities, changes in one aspect may lead to unexpected modifications in another, increasing the risk of bugs.<br><br>❌ Violation: A class doing too many things<br>java<br>class ReportManager {
    void generateReport() { /* Logic to create report */ }
    void printReport() { /* Logic to print report */ }
    void saveReport() { /* Logic to save report */ }
}
<br>✅ SRP-Compliant Design<br>java<br>class ReportGenerator {
    void generateReport() { /* Creates report */ }
}

class ReportPrinter {
    void printReport() { /* Prints the report */ }
}

class ReportSaver {
    void saveReport() { /* Saves report to disk */ }
}
<br><br>✔️ Improved readability and maintainability ✔️ Easier testing and debugging ✔️ Prevents unintended coupling between features<br><br>"Software entities should be open for extension but closed for modification."<br><br>A class should allow new functionality to be added without modifying existing code. This reduces the risk of introducing new bugs when extending a system.<br><br>❌ Violation: Modifying existing code<br>python<br>class Rectangle:
    def draw(self): 
        print("Drawing Rectangle")

class Circle:
    def draw(self): 
        print("Drawing Circle")

def render(shape):
    if isinstance(shape, Rectangle):
        shape.draw()
    elif isinstance(shape, Circle):
        shape.draw()
<br>✅ OCP-Compliant Design (Using Polymorphism)<br>python<br>class Shape:
    def draw(self):
        pass

class Rectangle(Shape):
    def draw(self):
        print("Drawing Rectangle")

class Circle(Shape):
    def draw(self):
        print("Drawing Circle")

def render(shape: Shape):
    shape.draw()
<br><br>✔️ Simplifies adding new functionality ✔️ Avoids modifying stable and tested code ✔️ Reduces maintenance cost<br><br>"Subtypes must be substitutable for their base types."<br><br>Objects of a derived class should be able to replace objects of the base class without causing errors or unexpected behavior.<br><br>❌ Violation: Breaking expected behavior<br>cpp<br>class Bird {
public:
    virtual void fly() { cout &lt;&lt; "Flying"; }
};

class Penguin : public Bird {
public:
    void fly() override { throw std::logic_error("Penguins can't fly!"); }
};
<br>✅ LSP-Compliant Design<br>cpp<br>class Bird {
public:
    virtual void move() { cout &lt;&lt; "Bird moves"; }
};

class Penguin : public Bird {
public:
    void move() override { cout &lt;&lt; "Penguin swims"; }
};
<br><br>✔️ Prevents unexpected errors ✔️ Ensures reliable polymorphism ✔️ Improves code flexibility<br><br>"Clients should not be forced to depend on interfaces they do not use."<br><br>A class should only implement the methods it actually needs. Large, general-purpose interfaces should be broken down into smaller, more specific interfaces.<br><br>❌ Violation: A class forced to implement unnecessary methods<br>csharp<br>interface Worker {
    void work();
    void eat();
}

class Robot : Worker {
    public void work() { Console.WriteLine("Working"); }
    public void eat() { throw new NotImplementedException(); }
}
<br>✅ ISP-Compliant Design<br>csharp<br>interface Workable {
    void work();
}

interface Eatable {
    void eat();
}

class Robot : Workable {
    public void work() { Console.WriteLine("Working"); }
}
<br><br>✔️ Avoids unnecessary dependencies ✔️ Improves flexibility and usability ✔️ Ensures smaller, more manageable interfaces<br><br>"High-level modules should not depend on low-level modules. Both should depend on abstractions."<br><br>Instead of a class directly depending on a concrete implementation, it should depend on an abstraction to make the system more flexible.<br><br>❌ Violation: Tight Coupling<br>java<br>class MySQLDatabase {
    void connect() { /* MySQL connection logic */ }
}

class DataHandler {
    private MySQLDatabase db = new MySQLDatabase();
    void fetchData() { db.connect(); }
}
<br>✅ DIP-Compliant Design (Using Interface)<br>java<br>interface Database {
    void connect();
}

class MySQLDatabase implements Database {
    public void connect() { /* MySQL connection logic */ }
}

class DataHandler {
    private Database db;
    
    public DataHandler(Database db) { this.db = db; }
    void fetchData() { db.connect(); }
}
<br><br>✔️ Loose coupling ✔️ Easier to swap dependencies ✔️ Facilitates unit testing<br><br>The SOLID principles provide a structured way to write clean, maintainable, and scalable code. By following these principles, developers can reduce technical debt, enhance software flexibility, and prevent common design pitfalls.<br>Would you like any further refinements or additional insights? 😊]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/solid-design-principles-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/SOLID Design Principles (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 28 May 2025 09:10:56 GMT</pubDate></item><item><title><![CDATA[Static Casting & Reinterpret Casting in C++ (NC)]]></title><description><![CDATA[ 
 <br><br><br>const_cast: Removes or adds const or volatile qualifiers from or to a variable, connot change the type<br>
it doesn't change the cv qualification of the original variable<br>
ONLY USE IT TO PREVENT DUPLICATION FOR MEMBER FUNCTIONS]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/static-casting-&amp;-reinterpret-casting-in-c++-(nc).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Static Casting &amp; Reinterpret Casting in C++ (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 23 May 2025 04:44:08 GMT</pubDate></item><item><title><![CDATA[Static Keyword in C++]]></title><description><![CDATA[ 
 <br><br>当变量或者函数被声明为 static 后，我们就称其为静态变量或静态函数。那么静态代表了什么？在进程的虚拟内存空间里，我们看到，当变量被声明为 static 后，它会被放在数据段（初始化非 0 ）或 BSS 段（初始化为 0 /未初始化）中。这意味着静态变量的生命周期会一直持续到程序结束。也被称作 static storage duration。<br>#include &lt;iostream&gt;

int global_var; // Un-initialized, will be placed in the BSS segment
static int static_var; // Un-initialized, also placed in the BSS segment

static void static_func() {
    global_var++;
    static_var++;
}
// Non-static function accessible from other translation units
// Increments static_var
void increase_static() {
    static_var++;
}

int main() {
	create_static();
	increase_static();
	increase_static();
	increase_static();
    int local_var; // Un-initialized local variable, will be placed on the stack

    std::cout &lt;&lt; global_var &lt;&lt; std::endl
              &lt;&lt; static_var &lt;&lt; std::endl;
    return 0;
}
<br>那静态函数有什么用呢？这就不得不提到 static 的另一个特性：内部链接性。<br><br>我们说被声明为 static 的静态函数和变量具有内部链接性，什么意思呢？就是说我们定义的静态变量/函数只在定义它们的翻译单元 (translation unit) 内可见。一个翻译单元是指由一个源文件及其直接或间接包含的所有头文件组成。编译器会独立编译每个翻译单元，最后链接器会将这些编译好的翻译单元合并成一个可执行程序。<br>我们用一个例子说明：<br>// main.cpp
#include &lt;iostream&gt;
#include "static_unit1.cpp"

int main() {
    // static_unit_func(); // ld error, no function definition
    static_func(); // Calls static_func() defined in static_unit1.cpp
    func(); // Calls func() which is defined in static_unit2.cpp
    return 0;
}
<br>// static_unit1.cpp
extern int global_var; // Declares that global_var is defined in another translation unit.

static int static_var; // Declaration

// Function declarations
static void static_unit_func();
static void static_func();
void func();

static void static_func(){ // No naming conflict, no linker error
	global_var += 10;
	static_var += 10;
	std::cout &lt;&lt; "global_var shared by all the tranlation unit : " &lt;&lt; global_var &lt;&lt; std::endl
              &lt;&lt; "static_var specific owning by main.cpp       : " &lt;&lt; static_var &lt;&lt; std::endl;
}
<br>在这个例子中， main.cpp 和 static_unit1.pp 是一个翻译单元。 #include 预编译指令的作用就是把 included 的文件全部复制到源文件中。<br>static_unit2.cpp 是另外一个翻译单元。待会当你运行程序后，你会发现静态变量/函数只能在相同的翻译单元中被使用。<br>// static_unit2.cpp
#include &lt;iostream&gt;

int global_var; // Uninitialized, will be placed in the BSS segment.
static int static_var = 0; // Definition

static void static_unit_func(){ // A static_unit.cpp specific function
}

static void static_func() {
    global_var++;
    static_var++;
}

// Non-static function accessible from other translation units. (An interface)
// Increments both global_var and static_var.
void func() {
    global_var++;
    static_var++;
    std::cout &lt;&lt; "global_var shared by all the tranlation unit : " &lt;&lt; global_var &lt;&lt; std::endl
              &lt;&lt; "static_var specific owning by static_unit.cpp: " &lt;&lt; static_var &lt;&lt; std::endl;
}
<br>运行上面的代码，你会发现尽管我们在不同的翻译单元内定义了相同的 static_func() 函数，但链接时并没有引发任何的命名冲突。而且每个翻译单元中的静态变量只能由翻译单元内的函数进行访问。不难发现，内部链接性能够规避不同单元之间的命名冲突，确保变量或函数在其他翻译单元中不可见。<br>运行结果如下：<br>global_var shared by all the tranlation unit : 10
static_var specific owning by main.cpp       : 10
global_var shared by all the tranlation unit : 11
static_var specific owning by static_unit.cpp: 1
<br><br>在类中，我们还有静态成员变量和静态成员函数。和前面我们了解到的 static 语义不同，在类中，static 关键字用于声明这些成员并不受类实例的约束。换言之，静态成员变量/函数是被所有实例所共享的。<br>比如，我们有下面的例子：<br>// example.hpp

class Example{
public:
	static int s_var;
	static int get_s_var();
	Example();
	~Example();
};
<br>// main.cpp
#include &lt;iostream&gt;
#include "example.hpp"
int main(){
	Example e1;
	Example e2;
	{
		Example e3;
		Example e4;
		std::cout &lt;&lt; "We now have " &lt;&lt; Example::get_s_var() &lt;&lt; " instance." &lt;&lt; std::endl;
	}
	std::cout &lt;&lt; "We now have " &lt;&lt; Example::get_s_var() &lt;&lt; " instance." &lt;&lt; std::endl;
	std::cout &lt;&lt; "I can read the static data member: " &lt;&lt; Example::s_var &lt;&lt; std::endl;
}
<br>// example.cpp
#include "example.hpp"

int Example::s_var = 0;
int Example::get_s_var(){
	return s_var; // Equals return Example::s_var;
}
Example::Example(){
	s_var++;
}
Example::~Example(){
	s_var--;
}
<br>输出结果：<br>We now have 4 instance.
We now have 2 instance.
<br>我们发现，即使类成员变量/函数的定义在 example.cpp 这个文件域(file scope)中，和 main.cpp 是独立的翻译单元，但是我们仍然能在 main.cpp 中访问得到静态的成员变量和成员函数。也就是说，静态的成员函数和成员变量并不具有内部链接性(internal linkage)。<br><br>静态成员函数只能访问静态成员变量。因为静态成员函数没有 this 指针，所以不能访问非静态的成员变量。这是一个例子：<br>class Example{
public:
	int var;
	static int s_var;
	static int s_func();
};
int Example::s_var = 0;
int Example::s_func(){
	var++; // invalid use of member 'Example::var' in static member function
	s_var++; 
	return s_var;
}
int main(){

	return 0;
}
<br>由于 var 不是静态成员变量，所以编译会出错。<br><br>静态成员变量相当于一个带有访问属性的全局静态变量，namespace 是类的类名。和类外的静态变量一样，它们都具有 static storage duration 。静态成员变量存储在静态存储区（通常是 .data 段或 .bss 段），但它的作用域仍然是类的作用域（命名空间）。<br>因为静态成员变量是在类的所有实例之间共享的，而类的定义通常在头文件中。如果在类内部初始化静态成员变量，这样的初始化将在每个包含这个头文件的翻译单元中重复进行，从而导致链接时出现多重定义错误。<br><br>在 local scope {} 内定义的局部 static 变量的生命周期和全局 static 变量一样。但它也具有局部的访问属性，也就是说，只有在 local scope 内部才能访问到这个静态变量。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/static-keyword-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Static Keyword in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 01 Jun 2025 03:26:09 GMT</pubDate></item><item><title><![CDATA[The Rule of Five Idiom in C++]]></title><description><![CDATA[ 
 <br><br>假设我们有这样一个类，类内只有一个 std::string 类型的成员变量和一个负责打印的成员函数。我们将他们的访问权限都变为 public，这样，我们就可以对类内的成员变量进行赋值操作了。这时，我们对类内成员变量的构造简单而直接。如下：<br>#include &lt;iostream&gt;
#include &lt;string&gt;

class Name{
public:
	std::string m_name;
	void printName(){
		std::cout &lt;&lt; m_name &lt;&lt; std::endl;
	}
};

int main(){
	Name alice;
    alice.m_name = "Alice";
    alice.printName();
	return 0;
}
<br>但为了保证类内数据的封装性，我们一般将成员变量的访问权限设置为 private。这个时候我们就不能对这些变量直接进行操作了，现在的规则是：“你只能通过类内一些特殊的函数初始化类内私有的成员变量”。在面向对象程序设计中，我们将这些特殊的函数称为构造函数。同时，我们还有一种特殊的函数叫析构函数，它会在对象的生命周期结束后自动调用用于清理对象（见<a data-tooltip-position="top" aria-label="RAII and Scope in C++" data-href="RAII and Scope in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/raii-and-scope-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">RAII</a>）。<br><br>在将成员变量设为私有后，我们需要一些特殊的函数来初始化和清理这些变量。构造函数会在类实例化对象时自动调用，用于初始化成员变量。而析构函数则会在对象的生命周期结束时自动调用，用于清理资源。通常而言，我们加入 const 是为了避免对原对象的修改。<br>#include &lt;iostream&gt;
#include &lt;string&gt;

class Name {
private:
    std::string m_name{"NO NAME"};
public:
    Name() = default;

    // Constructor that accepts std::string
    Name(const std::string&amp; name) : m_name(name) {}
    void printName() const {
        std::cout &lt;&lt; m_name &lt;&lt; std::endl;
    }
};

int main() {
    Name alice("Alice");
    Name alice2(alice); // compiler provided for free
    Name alice3 = alice;// compiler provided for free
    alice.printName();
    alice2.printName();
    alice3.printName();
    return 0;
}
<br>上面，我们添加了一个接受 std::string 类型的构造函数。但在下面的例子中，我们发现虽然我们没有显式添加接受 Name 类类型的拷贝构造函数和拷贝赋值运算符，但它们仍然能够正常工作。这是因为编译器为我们自动生成了默认的拷贝构造函数和赋值运算符。<br>    // Default copy constructor, copy assignment operator provided by compiler.
    Name(const Name&amp; other_name) : m_name(other_name.m_name){}
    Name&amp; operator=(const Name&amp; other_name) : m_name(other_name.m_name){}
<br>这些默认的拷贝构造函数和赋值运算符会逐个成员地复制对象中的每个成员，这种拷贝被称为浅拷贝。在含有成员指针的类中，这种浅拷贝可能会造成资源的二次释放。详见<a data-tooltip-position="top" aria-label="Copying and Copy Constructors in C++ > 2. Shallow Copying and Deep Copying" data-href="Copying and Copy Constructors in C++#2. Shallow Copying and Deep Copying" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/copying-and-copy-constructors-in-c++.html#2._Shallow_Copying_and_Deep_Copying" class="internal-link" target="_self" rel="noopener nofollow">浅拷贝和深拷贝</a>。<br><br>在 C++11 前，C++ 没有移动语义的概念。因此，那时我们不需要考虑移动构造函数和移动赋值运算符。如果你要实现一个类，那你就应当实现所有的三个重要函数和一个析构函数，即构造函数、析构函数、拷贝构造函数和拷贝赋值运算符。这就是“三法则”（A best practice）。<br>如果类内资源不涉及内存分配，实际上你可以依赖编译器为你提供的默认的构造函数和赋值运算符重载。简化了类的设计和实现，这种原则叫做 the rule of zero 。<br>You may be curious about why we always pass an object by reference in C++, even in the copy constructor. You might know this is to avoid copying the object. While it's true that we need to copy the object in the copy constructor, the way we achieve this is different.<br>We use a reference to avoid creating a new copy of the object on the stack of the current function. By passing a reference (or a pointer) to the existing object, the function can operate directly on the original object without the overhead of making a copy. This method ensures that we're not creating unnecessary copies, which can be expensive in terms of CPU time and memory usage.<br>If you're still feeling confused, you're welcome to learn more <a data-tooltip-position="top" aria-label="Call Stack in C++ (ENG)" data-href="Call Stack in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/call-stack-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">here</a>.<br><br><br>自 C++11 引入移动语义之后，一个类可以通过移动构造函数和重写移动赋值运算符来构造对象。"The Rule of Five" 或 “五法则” 是 C++11 后定义构造函数的指导原则。五法则规定要实现一个类，你不仅需要实现三法则（即定义析构函数、拷贝构造函数和拷贝赋值运算符），还需要在类中添加移动构造函数和移动赋值运算符，以支持移动语义。<br><br>Rule of Zero旨在简化类的设计和实现。它的核心思想是：如果一个类不需要自定义的析构函数、拷贝构造函数、拷贝赋值运算符、移动构造函数或移动赋值运算符，那么就不应该定义这些函数。相反，应该信任并依赖编译器生成的默认实现。<br>#include &lt;string&gt;
#include &lt;memory&gt;

class Widget {
private:
    int i{0};
    std::string s{};
    std::unique_ptr&lt;int&gt; pi{nullptr};

public:
    Widget() = default;
    ~Widget() = default;
    Widget(const Widget&amp;) = default;
    Widget&amp; operator=(const Widget&amp;) = default;
    Widget(Widget&amp;&amp;) = default;
    Widget&amp; operator=(Widget&amp;&amp;) = default;
};
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/the-rule-of-five-idiom-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/The Rule of Five Idiom in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 30 Jul 2025 08:57:29 GMT</pubDate></item><item><title><![CDATA[Type Deduction - Auto in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Type Deduction - Template Type Deduction" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-template-type-deduction.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Template Type Deduction</a><br><br>在类型推断的第二节，我们正式开始学习 C++ 的自动类型推断机制。C++ 有各种各样的数据类型：int 、 long 、 float 、 double 还有表示字符串的 const char* 等等。之前，每次定义一个变量的时都要带上不同的类型符，好不麻烦。在 C++11 引入了 auto 关键字来帮我们推导类型。<br>简单来说，我们在定义类型的时候不需要再考虑它是什么类型了，编译器会帮我们做这些。甚至你可以让它帮你推导自定义的类类型。我们用下面的代码举一些例子：<br>class MyClass {
public:
    MyClass(int val) : i(val) {}
    int i;
};
auto func(){ // return type is deduced as int type, since C++14
	return 0;
}
int main() {
    int a = 0;
    auto b = a; // b is deduced as int type
    auto c = 0; // c is deduced as int type
    auto d = 3.14; // d is deduced as double type

    auto obj = MyClass{10}; // obj is deduced as MyClass type
    std::cout &lt;&lt; "obj.i = " &lt;&lt; obj.i &lt;&lt; std::endl; // Outputs: obj.i = 10

    return 0;
}
<br>这里 auto 是一个占位符(placeholder)，在编译时，编译器会将自动推导变量的类型或函数的返回类型替换掉这里的 auto。<br><br>虽然上面的例子看不出 auto 类型推断的规则，但完成 Type Deduction 的第一部分后，你就实际上已经掌握了 auto 类型推导的大部分精髓了。但有一个例外，我们留在最后学习。<br>之所以说你已经掌握了 auto 类型推导的大部分精髓，是因为 auto 和模板类型推导几乎是一回事。作为 C++11 为我们提供的语法糖，虽然 auto 只提供更简单的接口，其底层机制和模板类型推导是类似的。在学习 auto 时，你完全可以想象一个模板来帮助学习。例如：<br>auto x = expr;
<br>你可以将其想像成：<br>template&lt;typename T&gt;
void func(T param);

func(expr)
<br>然后推一推 T 的类型是什么，即为 auto 推得的类型。<br>上节，我们从 ParamType 将函数模板类型推断分为三个 cases：<br>
<br>ParamType 是指针或引用类型，但不是万能引用；
<br>ParamType 是万能引用；
<br>ParamType 既不是是指针也不是引用类型。
<br>同样，当我们使用 auto 时，我们可以继承上节课的推导方法，前面的 ParamType 就是等式左边的类型，即最终推得变量的类型，如：<br>// fellow the rule 3: neither prt nor ref
auto i = 10;
const auto ci = 10;
const int&amp; a = ci; // alias
auto i2 = a;

// fellow the rule 2: universal ref type
auto&amp;&amp; uri1 = i;
auto&amp;&amp; uri2 = ci;
auto&amp;&amp; uri3 = 10;

// fellow the rule 1: non-universal ref or ptr type
auto&amp; ri1 = i;
const auto&amp; ri2 = i;
const auto* pi = &amp;i;

// the two special cases, to be said:
const char greeting[] = "Hello, world!";
auto arr1 = greeting;
auto&amp; arr2 = greeting;

void func();
auto func1 = func;
auto&amp; func2 = func;
<br>我们可以沿着上节的 rules 进行推导。答案如下：<br>// follow Rule 3: neither pointer nor reference
auto i = 10;            // (1) auto deduced as int → i: int
const auto ci = 10;     // (2) auto deduced as int → ci: const int
const int&amp; a = ci;
auto i2 = a;            // (3) auto deduced as int

// follow Rule 2: universal ref
auto&amp;&amp; uri1 = i;        // (4) i is lvalue → uri1: int&amp;
auto&amp;&amp; uri2 = ci;       // (5) ci is const lvalue → uri2: const int&amp;
auto&amp;&amp; uri3 = 10;       // (6) 10 is rvalue → uri3: int&amp;&amp;

// follow Rule 1: non-universal ref or pointer
auto&amp; ri1 = i;          // (7) i: int → ri1: int&amp;
const auto&amp; ri2 = i;    // (8) i: int → ri2: const int&amp;
const auto* pi = &amp;i;    // (9) i: int → pi: const int*

// special cases
const char greeting[] = "Hello, world!";
auto arr1 = greeting;   // (10) array decays → arr1: const char*
auto&amp; arr2 = greeting;  // (11) array bound kept → arr2: const char (&amp;)[14]

void func();
auto func1 = func;      // (12) function decays to pointer → func1: void (*)()
auto&amp; func2 = func;     // (13) reference preserves function → func2: void (&amp;)()
<br>嘿嘿，所以 auto 的行为和模板类型推断是一样的。但我们提到过有一点是不一样的，这就不得不提到 C++11 和 auto 一并引入的更安全的变量初始化方式——初始化列表（详见 <a data-href="Member Initializer List in C++ (ENG)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/member-initializer-list-in-c++-(eng).html" class="internal-link" target="_self" rel="noopener nofollow">Member Initializer List in C++ (ENG)</a>），而推出模板的 C++98 并没有初始化列表的概念，这就造成了他们唯一的不同。<br>在 C++98 时代，变量的初始化主要用两种方式：<br>int x = 10;
int y(10);

int z = 3.14; // implicit conversion happens, code compiles
<br>这两种初始化方式会纵容 implicit conversion 的发生，而且第二种初始化的语法和函数调用的语法很像，容易混淆。C++11 引入了更现代、更安全的初始化方式，即初始化列表，它的语法是这样的：<br>int x = {10};
int y{10};

int z = {3.14} // error
<br>使用初始化列表后，编译器会进行类型检查，而且不允许 implicit conversion 的发生。这样，在类型换成 auto 后，安全性就可以得到保障，比如：<br>auto d = (1, 2, 3.14); // unsafe but compiles

auto arr1 = {1, 2, 3.14}; // error happens

auto arr2 = {1, 2, 3, 4}; // okay, arr3 is a read-only initializer list type
						  // std::initializer_list&lt;int&gt;
<br>这里，我们想要初始化一个 array-like 的变量，而由于逗号运算符的关系 arr1 最后会被推导为 double 类型。由于 arr1 和 arr2 的初始化都是通过初始化列表完成的，所以 arr1 = {1, 2, 3.14} 不会编译通过，arr2 会被推导成 std::initializer_list&lt;int&gt; 类型，这是一个类似数组的只读类型。<br>在使用 auto 时，还有一点需要注意，那就是 auto 在作为函数返回值时，它的推导行为是模板类型推导而非标准的 auto 类型推导，比如：<br>auto func(){
	return {1, 2, 3};
}
<br>是编译不通过的，因为模板类型推导规则是不能处理初始化列表的。auto 的这种不一致性非常奇怪，难以理解。<br>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-auto-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Type Deduction - Auto in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 29 Jul 2025 12:49:16 GMT</pubDate></item><item><title><![CDATA[Type Deduction - Decltype in C++]]></title><description><![CDATA[ 
 <br><br>decltype 也是在 C++11 引入的关键字，跟模板推导和 auto 类型推断的不同是：decltype 会准确地告诉你表达式所声明类型的实际类型。一般而言，decltype 推导出的类型和你预料中的所一致，它会保留表达式中的 CV qualifier、还可以帮你判断表达式到底是左值还是右值，它一律会帮你分析出来。<br>我们先用一些简单的例子来看看 decltype 是如何推导变量类型的：<br>int i = 10; // i is declared int type
decltype(i) j = 10; // j has the same type as i

const int ci = 10; // ci is declared const int 
decltype(ci) cj = 10; // cj's type is const int

const int&amp; cri = 10; // cri is declared const int&amp;
decltype(cri) = 10; // thus the cri's type is const int&amp;

std::vector&lt;int&gt; v(10) = {}; // v is declared std::vector&lt;int&gt;
v[0] = 0; // v[0] is int&amp; (T&amp; operatorp[](){})
<br>是不是感觉简单明了，甚至比 auto 还简单？在实际应用上，decltype 用途最多的还是函数返回值推导。<br><br>在 auto 小节中，我们介绍了它的行为和函数模板很像，这就导致有时候它会说 auto 在推断函数返回值时忽略本应出现的引用，比方说：<br>template&lt;typename Container, typename Index&gt;
auto accessAndModify(Container&amp; c, Index i) {
	return c[i]; // should return a T&amp;, right?
}

std::vector&lt;int&gt; v(10);
accessAndModify(v, 0) = 10; // error
<br>一般而言，operator[] 都会返回 T&amp; 类型，而在上面的情况中，auto 会将返回值推断为 T，如果你对 auto 的推断规则不是很熟悉或者单纯忘记了，那这类错误就很容易出现。为了解决这个问题，要么在 auto 后面加上 &amp;，要么使用 decltype 进行更精细的类型推断，如：<br>// decltype in C++11 
template&lt;typename Container, typename Index&gt;
auto accessAndModify(Container&amp; c, Index i) -&gt; decltype(c[i]) {
	return c[i];
}

std::vector&lt;int&gt; v(10);
accessAndModify(v, 0) = 10; 
<br>这里，你会发现返回类型被放在了参数列表的后面，这种语法被称为返回类型后置 (trailing return type)，这种语法尤其适合泛型编程：<br>template&lt;typename T, typename U&gt;
auto add(T a, U b) -&gt; decltype(a + b) {
    return a + b;
}
<br>在 C++14，你还可以怎么写：<br>// decltype after C++14
template&lt;typename Container, typename Index&gt;
decltype(auto) accessAndModify(Container&amp; c, Index i) {
	return c[i];
}

std::vector&lt;int&gt; v(10);
accessAndModify(v, 0) = 10; // ok
<br>decltype(auto) 是 C++14 引入的，它是 decltype 和 auto 的结合，它用于简化原先的表达（C++11 是需要在 decltype 中填入表达式，而 C++14 只需要填 auto）。这里， auto 表示要推导的类型，而 decltype 表示在推导的过程中使用 decltype 的推导规则，即获取更精确的类型。<br><br>decltype 的类型推导已经能让我感觉到心满意足了，在最开始，我们说它“还可以帮你判断表达式到底是左值还是右值”。这就需要你对表达式做出一些修改：<br>int i = 0;  // decltype(i) is int
			// decltype((i)) is int&amp;
<br>decltype(auto) func() {
	int i = 0;
	return (i);
}
<br>decltype((auto)) 是非法的。]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-decltype-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Type Deduction - Decltype in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 28 Jul 2025 15:55:23 GMT</pubDate></item><item><title><![CDATA[Type Deduction - Template Type Deduction]]></title><description><![CDATA[ 
 <br> 从 C++98 引入模板机制开始，C++ 终于具备了对泛型编程范式的支持，其中函数模板的类型推断也成为 C++ 首个实现的类型推断机制。自 C++11 (C++14) 后，类型推断体系又进一步扩展，新增两名大将——auto 和 decltype。后来的版本中，这些特性又被不断扩展完善。<br>这些范型机制给程序员提供了便利，同时也显著提升了 C++ 程序的适应性。由于编译器会根据类型帮你生成相应的代码，所以你不需要在某一处源码修改后到处改改改。为了能够利用这些工具编写高效的代码，你需要了解编译器帮你类型推导时的行为。<br><br>作为 C++ 类型推断的鼻祖，学习模板类型推断对于后面深入理解 auto 至关重要，在学习 auto 和 decltype 之前，了解了解模板类型推断还是有意义的。这里，我们不深究类型推断到底是怎么实现的，而是讨论一下语言提供的接口究竟是怎么样的。<br>在使用函数模板时：<br>template&lt;typename T&gt;
void func(ParamType param);

// A function call
func(expr);
<br>这里的 T 是模板参数类型，ParamType 是该模板参数所构成的函数参数类型，而 expr 用于调用函数的实际表达式。编译器会根据 expr 的类型来推导出实际 T 的类型，而 ParamType 往往由 T 直接构造。我们举个例子：<br>template&lt;typename T&gt;
void func(const T&amp; param); // ParamType is const T&amp;

func(3.14); // (1)
const int&amp; i = 30;
func(i); // (2)
<br>上面的例子中，(1) 中，由于 expr 是一个 double 类型的字面量，所以 T 被推导为 double；而在 (2) 中，i（也就是 expr）的类型是 const int&amp;，然而编译器最终推导出的 T 是 const int，引用类型 &amp; 由函数参数定义决定。<br>划分 T 和 ParamType 的行为可能会反直觉，(2) 中的推导也让人迷糊，我们可能期望编译器帮我们推断的 T 和 expr 的类型严格一致，但实际上，这些想法对于接口设计而言并不友好。C++ 的模板推导可能弱化了对某些修饰符的保留，但接口实实在在地灵活通用了。所以尽管可能并不符合直觉，但的的确确是提高范型编程质量的关键。下面，我们将详细地解释其中类型推断的秘密。<br>如果确实对于严格一致的类型推导有要求，C++14 提供了直接好用的工具：decltype(auto)。详见：<a data-href="Type Deduction - Decltype in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-decltype-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Type Deduction - Decltype in C++</a>。<br><br>Scott 从 ParamType 将函数类型推断分为三个 cases：<br>
<br>ParamType 是指针或引用类型，但不是万能引用；
<br>ParamType 是万能引用；
<br>ParamType 既不是是指针也不是引用类型。
<br><br>这是最常用的情况，在这个 case 下，类型 T 是这样推得的：<br>
<br>如果 expr 是引用类型，忽略引用的部分，如果是一个指针，忽略掉小星号；
<br>将实参 expr 与形参 ParamType 进行模式匹配，找到不相交的部分作为 T 的类型修饰。
<br>这两点实际上就解除了我们前面的疑惑。假如我们有这样的例子：<br>template&lt;typename T&gt;
void func(T&amp; param); // ParamType is T&amp;

func(3.14); // (1)
const int&amp; ri = 30;
func(i); // (2)
<br>(1) 的推导结果仍然相同，然而不同于前面 (2) 的推导，由于这里 ParamType 是 T&amp; 类型，所以 i 忽略引用的其他修饰符会得到保留，因为 ParamType 没有 const 的修饰，所以 T 的类型是 const int。<br>如果 ParamType 是一个指针类型，实际的情况和引用差不多，只不过参数变成了指针，举个例子：<br>template&lt;typename T&gt;
void func(T* param); // ParamType is T*

int i = 10;
func(&amp;i); // (1) - which T is deduced to int
const int* pi = i;
func(i); // (2) - which T is deduced to const int
<br><br>如果 ParamType 是一个万能引用，那就可能没有上面的看上去那么简单直白。由于引用折叠 (referencr collapse) 的原因，使得 T&amp;&amp; 的范型引用可以既接收左值又接收右值，让编译器在收到左值和右值对象时生成对应的函数代码。我们先看看什么是引用折叠：<br><br>因为 ParamType 是一个右值引用（如 T&amp;&amp;），在模板中使用该参数时，由于引用折叠规则，当实参 expr 是左值时，编译器就会将 T 推导成左值引用，进而，ParamType 就会被折叠成左值引用类型；而实参是右值时，T 就会被推导为非引用的 fundamental types，最终 ParamType 会被推导成右值引用类型。这种机制使得 ParamType 可以完美地保留 expr 的引用性。<br>理解了上面的引用折叠，我们接着学习在第二个 case 中的类型推导，其过程如下：<br>
<br>如果 expr 是左值，T 和 ParamType 最终都会被推导成左值引用；
<br>如果 expr 是右值，就遵循 case 1 的规则。
<br>举个例子：<br>template&lt;typename T&gt;
void func(T&amp;&amp; param); // param is now a universal ref

int i = 10;
const int ci = i;
const int&amp; ri = i;

func(i); // (1) - x is lvalue, thus T is int&amp;, param's type too
func(ci); // (2) - ci is lvalue, so T is const int&amp;, param's type too
func(ri); // (3) - ri is lvalue, so T is const int&amp;, param's type too
func(10); // (4) - 10 is rvalue, so T is int, param's type is int&amp;&amp;
<br><br>如果模板函数的参数是值传递，也就是说，无论 expr 是什么，在 func(T param) 栈帧构造的时候，都会在栈帧中拷贝一份全新副本。即：<br>template&lt;typename T&gt;
void func(T param);
<br>这时，类型推导的规则如下：<br>
<br>和 Case 1 一样，如果 expr 是一个引用类型，即忽略引用；
<br>如果 expr 被 CV-qualifier 所修饰，即忽略这些修饰；
<br>最终，无论实参 expr 是什么，编译器所推得的 T 都会是原始类型，比如：<br>template&lt;typename T&gt;
void func(T param);

int i = 10;
const int ci = i;
const int&amp; ri = i;

func(i); // (1)
func(ci); // (2)
func(ri); // (3)
func(10); // (4)
<br>无论是 (1), (2), (3) 还是 (4)，T 和 ParamType 的类型都会是 int。由于是副本，所以推得类型并不带有任何的 specifier，因为不会影响原本的对象。<br>这里，指向常量的常指针的按值传参需要注意一下，在例子：<br>template&lt;typename T&gt;
void func(T param);

const char* const ptr = "Hello, world!"
func(prt);
<br>中，我们有一个指向常量的常性指针，也就是说，指针指向的区域不允许修改，同时这个指针本身也不允许修改。那么，出现了两个 const 按值传参时，是否需要将这两个 const 都忽略呢？答案是否定的，既然我们知道传进去的是指针的副本，所以对指针的常性是可以忽略的。然而，数据的常性是不可以省去的，不然就破坏了数据的只读。<br>所以最终推导的 T 和 ParamType 都将是 const char* 保留对数据的常性。<br><br>除了以上，还有两类比较特别的情况——数组参数和函数参数。<br><br>虽然数组和指针的行为类似，而且有时候还能转换（数组退化），但数组和指针还是有区分的。比方说，数组会保存大小信息，而指针不会、数组的指向不可以修改，而指针可以修改指向。<br>const char greeting[] = "Hello, world!"; // greeting is a const char[14]

const char* prt = greeting; // Array decays to ptr
<br>既然不一样，而且按值传参时，数组会退化成指针。那么在下面这种情况发生时，T 会被推导成什么类型呢？<br>template&lt;typename T&gt;
void func(T param);

func(greeting);
<br>虽然语法上你可以将参数写成 void func(int param[]) 但行为上，编译器会将其退化为 void func(int* param)。所以从语言层面来看，并不存在数组的函数参数。在上面的类型推导中，由于传入的是字符串常量，而且传入了一个数组参数和编译器的退化行为，最终推得 T 的类型是 const char*。<br>为了保留数组的类型信息，我们可以按引用传入数组，避免退化，比如：<br>template&lt;typename T&gt;
void func(T&amp; param);

func(greeting); // pass array to func by ref
<br>这时编译器推得 T 的类型就是 const char [14] 了，param 的类型将是 const char (&amp;)[14]。由于数组类型会保留数组的元素个数，所以你还可以让编译器帮你获取数组元素个数，由于这一切发生在编译期，所以你可以用 constexpr 进行优化：<br>template&lt;template T, std::size_t N&gt;
constexpr std::size_t arraySize(T (&amp;)[N]) noexcept {
	return N
}
<br><br>除了数组的指针退化外，函数类型也可能会退化成函数指针。同样的，按值传入函数对象和按引用传入函数对象看上去也不太一样。但和数组的指针退化不同的，函数的函数名本身就是指向一段代码区域的指针，本身并不额外保留任何信息，所以按值和按引用传入函数对象除了类型不太一样以外没什么区别（他们的用法都是一样的）。<br>void func(int, double); // function name

template&lt;typename T&gt;
void passByValue(T param) { // T --&gt; void(*)(int, double)
	param(1, 3.14);
}
template&lt;typename T&gt;
void passByRef(T&amp; param) { // T --&gt; void(&amp;)(int, double)
	param(1, 3.14); // the usage's the same
}

passByValue(func);
passByRef(func);
]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/type-deduction-template-type-deduction.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Type Deduction - Template Type Deduction.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 29 Jul 2025 09:09:12 GMT</pubDate></item><item><title><![CDATA[Virtual Dispatch in C++]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="Inheritance in C++" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/inheritance-in-c++.html" class="internal-link" target="_self" rel="noopener nofollow">Inheritance in C++</a><br><br><br>之前，我们学习了继承是怎么回事，还了解了派生类对基类同名函数的隐藏机制。由于函数名隐藏，你在派生类中“重写”的基类成员函数实际上并不算是“重写”。因为这时派生类看不到基类的同名函数。这里我们在深入学习一下函数名隐藏。<br>比如我们有下面的程序。在基类 Base 中，我们定义了一个 func() 和一个重载的 func(int) 。我们之后定义了两个 Base 的派生类 NoHiding 和 Derived ，NoHiding 不 “重写”基类成员函数，而 Derived 类对基类的 func() 进行重写。<br>#include &lt;iostream&gt;
class Base {
public:
    void func() {
        std::cout &lt;&lt; "Base::func()" &lt;&lt; std::endl;
    }
    void func(int i){ // Overloading of func()
	    std::cout &lt;&lt; "Base::func(int)" &lt;&lt; std::endl;
	}
};
class NoHiding : public Base {};
class Derived : public Base {
public:
	void func() { // Hiding every func() in the base class.
        std::cout &lt;&lt; "Derived::func()" &lt;&lt; std::endl;
    }
    void func(float f) {
        std::cout &lt;&lt; "Derived::func(float)" &lt;&lt; std::endl;
    }
};

int main() {
    Base b;
    b.func();
    b.func(10);
	NoHiding n;
	n.func(); // Call Base::func()
	n.func(10); // Call Base::func(int)
    Derived d;
    d.func(); // Okay, call Derived::func()
    d.func(10); // Okay, call Derived::func(float) an implicit conversion happened
    return 0;
}
<br>由于我们在派生类 Derived 中定义了和基类中同名的函数，所以基类中所有名为 func 的函数都会被隐藏。这时，我们在派生类中“重写”了 func() 并重载了一个 func(float) 。我们可能期望看到 d.func() ，但由于命名隐藏，基类的 func(int) 并不会被继承。我们观察一下输出：<br>Base::func()
Base::func(int)
Base::func()
Base::func(int)
Derived::func(float)
<br>所以命名隐藏有什么好处？我们上面在派生类中重载了 func(float) ，如果没有命名隐藏，就可能导致函数调用的歧义。如 d.func(10) 会调用基类中的 Base::func(int) 而不大可能调用派生类中的 Derived::func(float) 。<br>我们说继承的意义之一就是代码的可重用性。这样看也没有提供上面可重用性的空间呀？基类成员函数都被隐藏掉了，还怎么重用？<br>这就引出了真正的多态——虚多态（又名动态多态）。<br>// 引入 using Base::func(); 来将基类同名但参数不同的函数引入派生类的作用域中，是一种显式恢复基类重载函数可见性的手段<br>// 如果只想继承基类的某几个函数，那么就可以手动地继承保留需要版本的行为，而不暴露其他版本的接口，这就是转发：（但请注意，这种情况下，子类就不是一种父类了，）<br>class Base {
public:
    void mf1();          // ✅ 你想继承这个
    void mf1(int);       // ❌ 不希望继承这个
};

class Derived : private Base {
public:
    void mf1() { Base::mf1(); } // ✅ 手动“继承”你想要的版本
};

<br><br>所以<br>不仅仅要继承接口（纯虚函数），也要继承实现（非虚函数）<br>如果 D 类重定义了基类的非虚函数，那么就会打破 is-a 的原则（因为 D 的实现和 B 不一样了）<br>如果 D 真的很需要和 B 的实现不一样，而且 B 并不是一个接口的话，那没办法 D 就算不算 is-a B，所以就不应该继承 B<br>但话又说回来，如果这种继承关系的确需要存在，那么就让这个非虚函数虚化。（下策）<br><br>虚多态的核心就是虚函数。虚函数允许子类对该函数进行重写(override)，引入了动态多态/运行时多态的概念。还是上面的例子，我们做一点调整，去掉类 NoHiding 的继承还有派生类中的 Derived::func() ，并且让派生类重写基类的 func(int) 。<br>#include &lt;iostream&gt;
class Base {
public:
    virtual void func() {
        std::cout &lt;&lt; "Base::func()" &lt;&lt; std::endl;
    }
    virtual void func(int i){ // Overloading of func()
	    std::cout &lt;&lt; "Base::func(int)" &lt;&lt; std::endl;
	}
};
class Derived : public Base {
public:
    void func(int i) override{
        std::cout &lt;&lt; "Derived::func(int)" &lt;&lt; std::endl;
    }
};

int main() {
    Base b;
    b.func();
    b.func(10);
    Derived d;
    Base&amp; d_ref = d;
    Base* d_ptr = &amp;d;
    d_ref.func(); // Okay, call Base::func()
    d_ref.func(10);  // Okay, call Derived::func()
    
    return 0;
}
<br>输出：<br>Base::func()
Base::func(int)
Base::func()
Derived::func(int)
<br>现在，那种多态的感觉又回来了。如果在派生类中没有给出重写实现，那就会调用基类默认的实现。大大提高了程序代码的重用。<br><br>虚多态的实现很容易，你要在派生类中重写哪个基类函数，你只用声明那个基类函数为 virtual 就可以了。由于其动态绑定机制，虚多态只能配合指针或引用使用（基类指针或引用，不然可能出现问题）。如下：<br>    Base&amp; d_ref = d;
    Base* d_ptr = &amp;d;
    Base* d_ptr_heap = new(Derived);
    d_ref.func(); // Okay, call Base::func()
    d_ref.func(10);  // Okay, call Derived::func(int)
    d_ptr-&gt;func();
    d_ptr-&gt;func(10);
    d_otr_heap-&gt;func();
    d_otr_heap-&gt;func(10);
    
<br>可以看到，派生类对象并不只是在堆上创建，在栈上也可以创建派生类对象来实现动态多态。（只要满足动态绑定机制）<br>这里仍需要注意，尽管我们引入了虚多态，但命名隐藏是依然存在的。也就是说，即使基类函数是虚函数，派生类定义同名函数仍然会隐藏基类版本。这时，如果你使用派生类的指针或引用期望调用基类的虚函数，你会发现编译错误（被隐藏）。<br>    Derived&amp; d_ref = d; // Use base pointer
    //d_ref.func(); // Not okay, no matching function
    d_ref.func(10);  // Okay, call Derived::func(int)
<br>为什么？这是 C++ 的特性决定的。如果你不用指针或者引用来调用虚函数，就会发生早绑定（静态绑定）。而多态是通过晚绑定（动态绑定）实现的，所以我们常用基类指针或引用来调用虚函数。我们待会介绍什么是早绑定和晚绑定。<br><br>派生类中重写的函数必须和基类中的虚函数类型完全匹配（函数名、参数列表、常量性）。在派生类中重写虚函数的时候可以使用 final 和 override 关键字。它们各有不同的作用。<br>override&nbsp;关键字用于在派生类中重写基类中的虚函数。它告诉编译器该函数是用来重写基类中的虚函数的，如果没有正确匹配基类中的虚函数，编译器就会报错。override&nbsp;可防止因拼写错误或参数不匹配导致的意外函数隐藏。<br>final 用于控制虚函数或类继承重写的行为。如果将 final 关键字用于虚函数，则表示该虚函数的重写到此结束，如果在派生类中重写该虚函数就会导致编译错误。如：<br>class Grandpa{
public:
	virtual void func(){
		std::cout &lt;&lt; "In grandpa class." &lt;&lt; std::endl;
	}
};
class Father : public Grandpa{
public:
	void func() override final {
		std::cout &lt;&lt; "In father class." &lt;&lt; std::endl;
	}
};
class Child : public Father{
public:
	// void func(){} // The override of virtual function has been finished. 
};
<br><br>在虚多态中，基类的作用是制定一些规则（如派生类可以重写哪些函数）。但是基类不能干涉派生类重写函数的实现，这并不难理解。而如果我们想做的更绝一点，强制派生类实现基类中的虚函数（某些功能），我们就可以把这个虚函数变成纯虚函数。<br>因为纯虚函数定义了一组必须由派生类实现的函数，从而为派生类提供了一种规范和约束。因此，在 C++ 中，我们也常称纯虚函数为接口。<br>纯虚函数是通过在基类中定义虚函数，并在函数后添加 = 0 来声明的。一般而言，纯虚函数没有函数体。如：<br>class Base{
public:
	virtual void func() = 0;
};
class Derived : public Base{
	// Must implement the pure virtual function func()
	void func() final{
	// Impl...
	}
};
<br>其实接口是可以有函数体的，你可以在这里完成最初的一些初始化。<br>#include &lt;iostream&gt;

class Base{
public:
	virtual void func() = 0;
};
void Base::func(){
	std::cout &lt;&lt; "In pure virtual function" &lt;&lt; std::endl;
}
class Derived : public Base{
	// Must implement the pure virtual function func()
	void func() final{
		Base::func(); // Explicit call to Base::func()
	// Other impl...
	}
};
<br><br>当类中声明某函数为纯虚函数时，该类即为抽象类。因为往往接口不提供相关的实现，抽象类不能实例化。如果继承的派生类对接口不进行实现，那么派生类也会变为抽象类，即不能实例化。<br><br>在了解虚析构函数前，我们先写一段代码并观察其输出结果，我们会看到删除指向派生类对象的基类指针时，析构函数的调用是错误的。观察并思考一下为什么会出现这种现象。<br>#include &lt;iostream&gt;

class Base{
public:
    Base(){ std::cout &lt;&lt; "Base constructor." &lt;&lt; std::endl; }
    ~Base(){ std::cout &lt;&lt; "Base destructor." &lt;&lt; std::endl; }
};
class Derived : public Base{
public:
    Derived(){ std::cout &lt;&lt; "Derived constructor." &lt;&lt; std::endl; }
    ~Derived(){ std::cout &lt;&lt; "Derived destructor." &lt;&lt; std::endl; }
};

int main(){

    Base* base = new Base();
    delete base;
    std::cout &lt;&lt; "---------------------\n";
    Derived* derived = new Derived();
    delete derived;
    std::cout &lt;&lt; "---------------------\n";
    Base* polymorph = new Derived();
    delete polymorph;
    return 0;
}
<br>Base constructor.
Base destructor.
---------------------
Base constructor.
Derived constructor.
Derived destructor.
Base destructor.
---------------------
Base constructor.
Derived constructor.
Base destructor.
<br>输出很奇怪，当我们删除 polymorph 时，我们发现虽然我们 new 了一个 Derived 类对象，但是删除对象的时候确没有调用基类的析构函数。为什么？很明显的是，在我们删除 polymorph 时，我们并没有正确调用派生类的析构函数。这是因为非虚函数都是静态绑定的。<br><br>静态绑定也称为早绑定，因为被静态绑定的函数调用通常在编译阶段就已确定。换句话说，对于非虚函数，编译器会在编译时决定调用的具体函数。静态绑定是 C++ 的默认函数调用机制，适用于非虚函数、全局函数和静态成员函数。<br>由于多态的实现需要用到运行时的虚表信息，而静态绑定通常上仅仅依赖编译时期的类型信息来决定调用那些函数，所以在虚多态的语境下，我们看到派生类对象没有被正确的析构。<br>{
    Base* polymorph = new Derived(); // Calls Base::Base() -&gt; Derived::Derived()
} // Since the pointer type is Base*, only Base::~Base() is called. Derived's destructor is not called due to the lack of a virtual destructor in Base.
{
    Derived d; // Calls Base::Base() -&gt; Derived::Derived()
    Base&amp; polymorph = d; // A Base type reference is created to reference a Derived object.
} // Only Base::~Base() is called because the destructor is not virtual, so Derived's destructor is skipped.

<br>我们前面提到，要实现虚多态，我们需要用基类的指针或者引用。而在上面的代码构造派生类时，由于派生类继承了基类，所以会先构造基类。然而我们指向派生类对象的指针或引用是基类的指针或引用，所以在析构的时候只会析构基类对象而漏掉派生类对象。<br><br>当基类的析构函数声明为虚函数时，C++ 的虚多态会确保在删除一个指向派生类对象的基类指针时，调用的是派生类的析构函数。这是因为虚函数表会在运行时动态绑定到正确的析构函数，从而确保派生类的析构函数被调用。<br>如果你给基类的析构函数加上 virtual 关键字就好了。这样析构的时候会查去虚表，由于查虚表的过程是运行时发生的，所以称为晚绑定（动态绑定）。<br><br>那析构函数可以是纯虚的么？既然我们有多态，为什么不直接在派生类中把所有的资源给删除呢？这里我声明 Base::~Base() = 0; 为纯虚析构函数。<br>#include &lt;iostream&gt;

class Base{
public:
    Base(){ std::cout &lt;&lt; "Base constructor." &lt;&lt; std::endl; }
    virtual ~Base() = 0;
};
class Derived : public Base{
public:
    Derived(){ std::cout &lt;&lt; "Derived constructor." &lt;&lt; std::endl; }
    ~Derived(){ std::cout &lt;&lt; "Derived destructor." &lt;&lt; std::endl; }
};

int main(){

    Base* polymorph = new Derived();
    delete polymorph;
    return 0;
}
<br>编译一下，GCC 告诉我们链接出错了，因为没有找到 Base::~Base() 的定义。在前面，我们了解了纯虚函数是可以有函数体的，我们补上函数体再进行编译。<br>#include &lt;iostream&gt;

class Base{
public:
    Base(){ std::cout &lt;&lt; "Base constructor." &lt;&lt; std::endl; }
    virtual ~Base() = 0;
};
Base::~Base(){
    std::cout &lt;&lt; "Base destructor." &lt;&lt; std::endl;
}
class Derived : public Base{
public:
    Derived(){ std::cout &lt;&lt; "Derived constructor." &lt;&lt; std::endl; }
    ~Derived(){ std::cout &lt;&lt; "Derived destructor." &lt;&lt; std::endl; }
};

int main(){

    Base* polymorph = new Derived();
    delete polymorph;
    return 0;
}
<br>Ok 了，没有错误了。而且可以运行。那为什么必须要类中析构函数的定义呢？<br><br>现在，我们终于走到虚多态是如何实现的这一步了。我们现来观察一个现象：<br>#include &lt;iostream&gt;

class nonVirtual{
public:
	void func(){}
	void func(int i){}
};
class Virtual{
public:
	virtual void func(){}
	virtual void func(int i){}
};
class Derived : public Virtual{
public:
    void func(){}
    void func(int i){}
};
int main(){
	std::cout &lt;&lt; "Sizeof nonVirtual: " &lt;&lt; sizeof(nonVirtual) &lt;&lt; std::endl;
	std::cout &lt;&lt; "Sizeof Virtual: " &lt;&lt; sizeof(Virtual) &lt;&lt; std::endl;
    std::cout &lt;&lt; "Sizeof Derived: " &lt;&lt; sizeof(Derived) &lt;&lt; std::endl;
    return 0;
}
<br>输出：<br>Sizeof nonVirtual: 1
Sizeof Virtual: 8
Sizeof Derived: 8
<br>不难发现，当类里面没有数据成员，而且成员函数非虚时，类的大小为 1 字节（用于确保创建的对象有唯一的地址）。但如果定义虚函数，类的大小就会为 8 字节。这 8 字节即为虚指针（64 位机器），它是编译器为我们生成的成员变量，一般位于对象内存布局的最前面。<br>我们说虚多态由于动态绑定只能配合指针或引用使用。而动态绑定是通过虚表和虚表指针实现的。当我们使用 virtual 关键字时，C++就会帮我们创建一个 vtable 。vtable 会被存放到类定义模块的数据段中，而且所有相同类型的实例共享同一个 vtable 。<br><br>可能上面的话我们并不好理解，没关系，我们先用一个例子说明。如下，我们让将基类 Animal 定义为了一个抽象类，为其他的动物提供函数/方法接口。我们定义基类 Animal 和三个派生类，然后在 main 里面用基类指针指向 new 的派生类对象，在运行时动态调用派生类的实现。这就是动态时多态(Dynamic dispatch) 。<br>#include &lt;iostream&gt;

class Animal{
public:
    virtual void Say() {
	    std::cout &lt;&lt; "Dingggg~" &lt;&lt; std::endl;
    }
    virtual void Whoami() {
	    std::cout &lt;&lt; "I am an animal." &lt;&lt; std::endl;
	}
};
class Cat : public Animal{
    void Say() override {
        std::cout &lt;&lt; "Meowwww~" &lt;&lt; std::endl;
    }
    void Whoami() override {
	    std::cout &lt;&lt; "I am a cat." &lt;&lt; std::endl;
    }
};
class Cow : public Animal{
public:
    void Say() override {
        std::cout &lt;&lt; "Moooooo~" &lt;&lt; std::endl;
    }
};
class Pig : public Animal{
public:
    void Say() override {
        std::cout &lt;&lt; "Oinnnnk~" &lt;&lt; std::endl;
    }
};
int main(){

    Animal* animal = new Cat();
    animal-&gt;Say();
    animal-&gt;Whoami();
    Animal* animal2 = new Cow();
    animal2-&gt;Say();
    animal2-&gt;Whoami();
    
    delete animal;
    delete animal2;
    return 0;
}
<br>Meowwww~
I am a cat.
Moooooo~
I am an animal.
<br>派生类继承基类时，如果没有重载虚函数，虚表中的指针仍然指向基类的虚函数实现。也就造成了我们所看到的 I am an animal. 。虚表(vtable) 的存在是实现动态多态性和默认基类实现的核心所在。通过虚表，可以在运行时决定调用哪个具体的函数实现，我们现在就来了解它。<br><br>上面我们观察了一些现象，我们下面就来看看虚表和我们的代码在内存中是怎么样的。我们用一张图来说明一下。<br><img alt="vtable.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/vtable.png"><br>在这个例子中，每个类都有一个自己的虚表，某个类的所有实例都会共享一个虚表，当类被实例化时，实例会有一个指向其类的虚指针（指向虚表的指针）。<br>虚表中包含类的所有虚函数的指针。我们的基类&nbsp;Animal&nbsp;有两个虚函数&nbsp;Say()&nbsp;和&nbsp;Whoami()，所以&nbsp;Animal&nbsp;类的虚表中会有两个指针分别指向&nbsp;Say()&nbsp;和&nbsp;Whoami()&nbsp;的实现。对于派生类中没有重载基类的某个虚函数，那么它的虚表中会包含指向基类&nbsp;Animal&nbsp;中相应虚函数的指针。<br>从上图中能清楚地看到：<br>
<br>Cat&nbsp;类的虚表会有指向虚函数&nbsp;Cat::Say()&nbsp;和&nbsp;Cat::Whoami()&nbsp;的指针。
<br>Cow&nbsp;类的虚表会有指向虚函数&nbsp;Cow::Say()&nbsp;和&nbsp;Animal::Whoami()&nbsp;的指针。
<br>Pig&nbsp;类的虚表会有指向虚函数&nbsp;Pig::Say()&nbsp;和&nbsp;Animal::Whoami()&nbsp;的指针。
<br>请留意：这里并没有画栈这个内存中极其重要的一部分。在堆中的实例需要通过栈上的指针来进行寻址，完整的寻址过程如下：栈上指针&nbsp;-&gt;&nbsp;实例中的虚指针&nbsp;-&gt;&nbsp;虚表中的函数指针&nbsp;-&gt;&nbsp;虚函数。这也是动态多态性较慢的原因之一（也就是为什么也叫动态多态为运行时多态）。<br><br>我们先来看看内存布局：<br>Mapped address spaces:

          Start Addr           End Addr       Size     Offset  Perms
      0x555555554000     0x555555555000     0x1000        0x0  r--p   
      0x555555555000     0x555555556000     0x1000     0x1000  r-xp   .text
      0x555555556000     0x555555557000     0x1000     0x2000  r--p   .rodata
      0x555555557000     0x555555558000     0x1000     0x2000  r--p   .rodata
      0x555555558000     0x555555559000     0x1000     0x3000  rw-p   .data&amp;bss
      0x555555559000     0x55555557a000    0x21000        0x0  rw-p   [heap]
      0x7ffff7800000     0x7ffff7828000    0x28000        0x0  r--p   libc.so.6
      0x7ffff7828000     0x7ffff79bd000   0x195000    0x28000  r-xp   libc.so.6
      0x7ffff79bd000     0x7ffff7a15000    0x58000   0x1bd000  r--p   libc.so.6
      0x7ffff7a15000     0x7ffff7a16000     0x1000   0x215000  ---p   libc.so.6
      0x7ffff7a16000     0x7ffff7a1a000     0x4000   0x215000  r--p   libc.so.6
      0x7ffff7a1a000     0x7ffff7a1c000     0x2000   0x219000  rw-p   libc.so.6
      0x7ffff7a1c000     0x7ffff7a29000     0xd000        0x0  rw-p   
      0x7ffff7c00000     0x7ffff7c9a000    0x9a000        0x0  r--p   libstdc++.so
      0x7ffff7c9a000     0x7ffff7dab000   0x111000    0x9a000  r-xp   libstdc++.so
      0x7ffff7dab000     0x7ffff7e1a000    0x6f000   0x1ab000  r--p   libstdc++.so
      0x7ffff7e1a000     0x7ffff7e1b000     0x1000   0x21a000  ---p   libstdc++.so
      0x7ffff7e1b000     0x7ffff7e26000     0xb000   0x21a000  r--p   libstdc++.so
      0x7ffff7e26000     0x7ffff7e29000     0x3000   0x225000  rw-p   libstdc++.so
      0x7ffff7e29000     0x7ffff7e2c000     0x3000        0x0  rw-p   
      0x7ffff7ea1000     0x7ffff7ea5000     0x4000        0x0  rw-p   
      0x7ffff7ea5000     0x7ffff7ea8000     0x3000        0x0  r--p   libgcc_s.so.1
      0x7ffff7ea8000     0x7ffff7ebf000    0x17000     0x3000  r-xp   libgcc_s.so.1
      0x7ffff7ebf000     0x7ffff7ec3000     0x4000    0x1a000  r--p   libgcc_s.so.1
      0x7ffff7ec3000     0x7ffff7ec4000     0x1000    0x1d000  r--p   libgcc_s.so.1
      0x7ffff7ec4000     0x7ffff7ec5000     0x1000    0x1e000  rw-p   libgcc_s.so.1
      0x7ffff7ec5000     0x7ffff7ed3000     0xe000        0x0  r--p   libm.so.6
      0x7ffff7ed3000     0x7ffff7f4f000    0x7c000     0xe000  r-xp   libm.so.6
      0x7ffff7f4f000     0x7ffff7faa000    0x5b000    0x8a000  r--p   libm.so.6
      0x7ffff7faa000     0x7ffff7fab000     0x1000    0xe4000  r--p   libm.so.6
      0x7ffff7fab000     0x7ffff7fac000     0x1000    0xe5000  rw-p   libm.so.6
      0x7ffff7fbb000     0x7ffff7fbd000     0x2000        0x0  rw-p   
      0x7ffff7fbd000     0x7ffff7fc1000     0x4000        0x0  r--p   [vvar]
      0x7ffff7fc1000     0x7ffff7fc3000     0x2000        0x0  r-xp   [vdso]
      0x7ffff7fc3000     0x7ffff7fc5000     0x2000        0x0  r--p   x86-64.so.2
      0x7ffff7fc5000     0x7ffff7fef000    0x2a000     0x2000  r-xp   x86-64.so.2
      0x7ffff7fef000     0x7ffff7ffa000     0xb000    0x2c000  r--p   x86-64.so.2
      0x7ffff7ffb000     0x7ffff7ffd000     0x2000    0x37000  r--p   x86-64.so.2
      0x7ffff7ffd000     0x7ffff7fff000     0x2000    0x39000  rw-p   x86-64.so.2
      0x7ffffffdd000     0x7ffffffff000    0x22000        0x0  rw-p   [stack]
  0xffffffffff600000 0xffffffffff601000     0x1000        0x0  --xp   [vsyscall]
<br>打个断点，看看栈上指针 animal2 指向哪里：<br>(gdb) break 40
Breakpoint 2 at 0x55555555528a: file main.c, line 41.
(gdb) run
(gdb) print animal2
$6 = (Animal *) 0x55555556b2e0
<br>这段空间是堆空间。我们创建的对象 Cow 就是堆上创建的，这个位置是对象的位置。由于例子中的类没有其他成员变量，所以它的在堆上只需要 8 字节存放虚指针，我们将堆上的元数据块一并打印出来。<br>(gdb) x/4xg 0x55555556b2d0
0x55555556b2d0: 0x0000000000000000      0x0000000000000021
0x55555556b2e0: 0x0000555555557cf0      0x0000000000000000
<br>这里，上面一行（16 字节）是元数据，元数据中的 0x21 表示实际上堆内存申请了 32 字节（包括元数据）。虽然虚表指针只需要 8 字节，但由于 16 字节的对其需要，实际上申请了 32 字节。<br>animal2 指向 Cow 对象的虚指针应当指向虚表。而虚表是在 .rodata 段的。这里，我们看虚表指针是 0x555555557cf0 ，这个位置刚好是 .rodata 段所在的位置。<br>Cow() 的虚表，就长这个样子：<br>(gdb) x/4xg 0x555555557ce0

0x555555557ce0 &lt;_ZTV3Cow&gt;:      0x0000000000000000      0x0000555555557d40
0x555555557cf0 &lt;_ZTV3Cow+16&gt;:   0x0000555555555428      0x000055555555536e
<br>实际上，它和其他几个类的虚表是挨在一起的：<br>0x555555557d00 &lt;_ZTV3Cat&gt;:      0x0000000000000000      0x0000555555557d58
0x555555557d10 &lt;_ZTV3Cat+16&gt;:   0x00005555555553ac      0x00005555555553ea

0x555555557d20 &lt;_ZTV6Animal&gt;:   0x0000000000000000      0x0000555555557d70
0x555555557d30 &lt;_ZTV6Animal+16&gt;:0x0000555555555330      0x000055555555536e

0x555555557d40 &lt;_ZTI3Cow&gt;:      0x00007ffff7e1dc30      0x000055555555603b
0x555555557d50 &lt;_ZTI3Cow+16&gt;:   0x0000555555557d70      0x00007ffff7e1dc30

0x555555557d60 &lt;_ZTI3Cat+8&gt;:    0x0000555555556040      0x0000555555557d70
0x555555557d70 &lt;_ZTI6Animal&gt;:   0x00007ffff7e1cfa0      0x0000555555556048
<br>我们来分析 Cow 类的虚表。他有四个条目，我们先来看前两个：0x000000000000 用于占位，0x555555557d40 是 RTTI 指针，指向 Cow 的类型信息（_ZTI3Cow）。后面的两个条目就是虚函数指针，第一个指向 Cow::Say()，第二个条目指向 Animal::Whoami()。<br>回过头来再来看看这张图。我们有一个基类，三个派生类。其中，大家都会说自己的话。但是 Cow 和 Pig 并不知道自己是什么。所以它们复用基类的 void Animal::Whoami() 函数。<br><img alt="vtable.png" src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/vtable.png"><br>而且有没有发现，如果你不构造某个类对象，那它就不会生成虚表。这里，我们就没有 Pig() 的虚表。<br><br>他有四个指针条目，指向不同的位置。第一个条目是  0x00007ffff7e1dc30 。查看一下内存布局，这里是动态库 libstdc++ 所在的地方。存放着父类的类型信息。<br>第二个条目是 0x55555555603b ，指向 .rodata ，这是 Cow 的类型名称地址。<br>(gdb) x/s 0x000055555555603b
0x55555555603b &lt;_ZTS3Cow&gt;:      "3Cow"
<br>第三个条目是 0x0000555555557d70，存放着基类的类型信息。是基类的 RTTI 指针。<br>0x555555557d70 &lt;_ZTI6Animal&gt;:   0x00007ffff7e1cfa0      0x0000555555556048
<br>这里的第四个条目是其他父类信息。由于 Cow 只有一个父类，所以和第一个条目一致。<br><br>在 C++ 中，构造函数是不能被声明为虚函数的。因为虚多态依赖虚表，而虚表需要虚指针才能发挥作用。在构造函数执行时，对象尚未完全构造，虚指针还不存在。这时，要是构造函数是虚函数，那它是如何查找的虚函数表呢？<br>而且，虚表需要构造类对象才能产生。如果不构造类对象，就不会产生该类的虚表。<br><br>静态成员函数类似于带作用域的全局函数。它们属于类本身，而非具体的对象。即使你不创建对象，你依然可以使用静态成员函数。在上头我们提到虚多态依赖动态绑定，而动态绑定又依赖于虚表和虚指针。假如我们不构造对象，没有虚指针，如此，静态函数是如何实现动态绑定的？<br>（AI 总是偏向说静态成员函数没有 implicit this 。实际上大同小异，this 指针需要指向的还是构造对象时产生的虚指针。既然无法属于某个对象，那它自然也不能拥有自己的虚指针。）<br><br><br><br><a data-href="Virtual Inheritance in C++ (NC)" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-inheritance-in-c++-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Virtual Inheritance in C++ (NC)</a>]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/virtual-dispatch-in-c++.html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Virtual Dispatch in C++.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 11 Aug 2025 11:59:22 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/vtable.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-c-plus-plus-series/pics/vtable.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Volatile Specifier in C++ (Questioning)]]></title><description><![CDATA[ 
 <br>volatile 的作用到底是什么？我知道 volatile 声明的变量不会被编译器优化到寄存器中去。意思说用 volatile 声明的变量只能从内存中取，然后放到内存中。然后使用 volatile 会 back-off 编译器的优化。C++ 经常把 const 和 volatile 合并称为 CV qualifier。<br>硬件开发的程序中，经常会出现 volatile 这个修饰符，网上给出的解释是告诉编译器该变量可能会在程序外被改变，因此不应该对变量进行优化？理解不了。由于 volatile 会 back-off 编译器的优化，所以很容易想明白的应用场景就是一些需要测试代码性能的场景，我们想避免编译器对代码的优化（更何况不同的编译器生产的指令还可能不一样），以确保编译器能够生成代码运行时的真实行为和生成的机器指令。<br>还有对 volatile 的误解，认为其和并发程序有关，提供线程安全。实际上，这只是无稽之谈，有些人将不把 volatile 的变量暂存进 cache 误解成不会发生数据竞争，但 volatile 并不对其做任何保证。（而且 volaile 也不会对程序乱序执行有任何的 restrictions）]]></description><link>https://congzhi.wiki/congzhi's-c-plus-plus-series/volatile-specifier-in-c++-(questioning).html</link><guid isPermaLink="false">Congzhi's C Plus Plus Series/Volatile Specifier in C++ (Questioning).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 22 Jul 2025 21:14:27 GMT</pubDate></item><item><title><![CDATA[1. Introduction to Operating Systems]]></title><description><![CDATA[ 
 <br>第一遍重写完成<br><br><br>操作系统是计算机系统不可或缺的一部分，它是连接用户和硬件的桥梁，负责协调资源并优化计算机的使用体验。无论是个人电脑、手机还是现代的车机系统，操作系统都扮演着核心的角色。每当我们使用这些设备时，操作系统正在背后默默地完成资源调度和任务管理工作。<br>那什么是操作系统？好像一时间很难回答出来，好像它们天然的存在在我们的手机/电脑上。如果你感到困惑，不用担心，因为这个问题就是我们本阶段要回答的问题。因为操作系统是计算机系统的一部分，所以，咱们先来看看什么是计算机系统。<br><br>计算机系统是一个宏观的概念，由计算机硬件和计算机软件共同构成。硬件为计算机系统提供了基础设施 (infrastructure) ；软件赋予计算机系统功能，使其能够完成各种任务并满足用户需求。<br><br>当你的电脑或手机处于关机状态时，所能看到的就是计算机硬件啦。现代计算机主要遵循 John von Neumann 架构（指令和数据共享同一内存）或其变种 Harvard 架构（指令和数据分开存储）。这些架构定义了计算机由一组特定组件通过特定方式连接而成。这些内容在计算机组成原理中对此有详细介绍，我们这里简单地回顾一下。<br><br>在计算机组成原理中，我们学到，von Neumann 计算机由五大核心部件构成。分别是：运算器、控制器、存储器/内存、输入部件和输出部件构成。即要想组成一台计算机，我们就需要有：<br>
<br>Central Processing Unit: 包括算术逻辑单元 (ALU)、控制单元 (Control Unit) 和一组寄存器。其中的控制单元负责协调 ALU 与其他部件。
<br>Memory: 由众多存储单元组成，用于保存指令和数据。
<br>Input/Output Devices: 输入输出设备，我们统称为外设，用于将数据和指令传递给 CPU 去运算或是接收 CPU 运算后的结果。鼠标、键盘、屏幕都是常见的外设。
<br>Bus: 总线用于连接上述组件并实现它们之间的通信。分为数据总线、地址总线和控制总线。
<br>如下，我们展示了计算机硬件。我们把没有安装任何软件的计算机称为裸机，想一想，假设这台计算机没有安装任何软件，我们该如何使用裸机完成一些任务？<br><img alt="computer_hardware.png" src="https://congzhi.wiki/congzhi's-os-series/pics/computer_hardware.png"><br><br>在使用裸机前，我们先来看看计算机的工作流。von Neumann 架构将计算机分为五大部件，其中，CU 负责协调和控制其他四个部件完成各自的任务。所以计算机的工作流程实际上是由控制器指挥其他部件按顺序执行指令。<br>由于 von Neumann 架构的计算机是一种基于存储程序 (stored-program) 的设计思想的架构，即指令和数据存储在同一个存储器中。指令的处理流程通常包括以下四个步骤：<br>
<br>输入：从外部设备接收数据或指令。
<br>存储：将接收到的数据或指令保存到存储器中。（涉及 memory architecture 我们将在 <a data-href="12. Memory Management" href="https://congzhi.wiki/congzhi's-os-series/12.-memory-management.html" class="internal-link" target="_self" rel="noopener nofollow">12. Memory Management</a> 来介绍）
<br>计算：CPU 从存储器中读取数据并由 ALU 对当前指令进行逻辑或算术运算。
<br>输出：将运算结果传递给外部设备反馈给用户。
<br>CPU 需要处理数据，输入设备就负责将外部信息输入到计算机系统中。常见的输入设备包括键盘、鼠标、扫描仪等。此外，现代计算机还包括触摸屏、语音识别系统和摄像头等高级输入设备，用于更加多样化和直观的用户交互。<br>当 CPU 要处理某些数据时，它会从 cache 或内存中读取并处理这些数据。如果 cache 和内存没有相关的数据或指令，系统还可能会在硬盘中寻找相关的数据。在并行化计算时代，除了 CPU，系统在一些如图形处理、机器学习等并行计算的应用场景中还会依赖 GPU (Graphics Processing Unit)。<br>当数据处理完成之后，我们想得到处理后的结果。这时，系统就会让输出设备将计算机处理后的信息输出给用户。常见的输出设备包括显示器、打印机、扬声器等。<br><br>我们现在知道了计算机工作流程，我们再来深入一下，看看 CPU 如何执行机器语言指令。如果你要做菜，第一步你需要从冰箱里面先拿原材料，然后处理原材料，烹制菜肴，最后装盘上桌。CPU 执行指令和做菜一样，你要先从内存里面取指令和数据，经过解码，CPU 执行指令，最后写回结果。<br>这整个过程称为指令执行周期，也简称为指令周期。一般由取指令、指令解码、执行、结果写回四部分组成（有的还包括访存周期）。在指令周期中，你可能经常见到以下几个寄存器：PC 寄存器 (Program Counter, AKA Instruction Pointer)、MAR 寄存器 (Memory Address Register)、MDR 寄存器 (Memory Data Register, AKA Memory Buffer Register)、IR 寄存器 (Instruction Register)。<br>我们以一个 X+Y 的加法指令做例子，其指令执行周期如下：<br>
<img alt="Pasted image 20240405005008.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240405005008.png"><br><br>CU 负责从内存中获取下一条要执行的指令。它通过 PC 得到当前指令的地址，从内存中读取指令。MDR 的大小就决定了CPU 一次性能够 fetch 多少指令。之后，指令会被加载到 IR 中去处理（解码+执行）。完成这一系列操作后，CU 就会更新当前 PC 中的值（进行 PC+1 的操作），以指向下一条要取 fetch 的指令地址。<br>取指令阶段顺序：PC (In charge of CU) -&gt; MAR -&gt; (MEMORY) -&gt; MDR -&gt; IR -&gt; (Instruction executes)<br><br>在指令被加载到 IR 后，控制单元就会解码指令，根据其操作码来确定它是一条什么类型的指令（本例中说加法指令，即需要两个操作数）。后识别出操作数位于寄存器 A 和寄存器 B。解码阶段确定了需要执行的具体操作及涉及的数据。<br><br>指令解码后，根据 CU 的控制信号，ALU 执行实际加法操作。它从寄存器读取值，执行加法操作并将结果暂存。<br><br>在操作完成后，CU 一般会指示将 ALU 的计算结果写回寄存器。<br><br>至此，我们已经大致了解了一条计算机指令是如何执行的了，尽管这个过程涵盖了取指、解码、执行和写回等多个步骤，但在我们的视角下，这一切几乎透明不可见。所有的逻辑都被封装在一片微小的芯片中了，呈现在我们眼前的，仅仅是排列整齐的针脚们。<br>我们将这种把复杂机制封装于内部，只暴露简洁接口的设计思想称为——抽象 (abstruction)，这是计算机科学中最核心的理念之一。<br><br>作为计算机硬件的核心，CPU 通过众多封装的管脚与外界进行交互。假如我们想使用裸机（没有操作系统的计算机）进行一个加法运算，所需要的步骤看似简单：我们只要把被加数放到一个寄存器中，把加数放在另一个寄存器中，然后执行 CPU 的加法指令，等待一瞬间，我们可以在加数寄存器上观察到输出结果了。<br>我们说过，这一看似轻而易举的过程的背后实际上是万亿计逻辑电路的协作。既然不用关心 CPU 内部到底发生了什么，我们来正式操作一台裸机。<br><br>如果你想在裸机环境中编程，你就必须使用机器语言，而且每次都要设置寄存器的值。机器语言是基于硬件的最低级别编程语言，是 CPU 唯一能够直接理解并执行的语言。<br>作为使用裸机的用户，我们并不关心门电路怎么实现加法指令，我们只需要使用 CPU 提供的加法指令就好了。但我们确实需要关心一件事情：我们希望机器码在厂商后续的升级型号中也能继续使用，不然每次得到新的 CPU 我们都需要编写新的机器代码。为此，芯片厂商就会通过指令集体系结构 (ISA) 对底层晶体管电路进行规约抽象。<br>例如，8086 及后续的芯片型号使用的架构就是 x86 ，由于 ISA 总是向下兼容的，因而你可以在后续任何版本的 x86 架构芯片上运行相同的机器语言代码。也就是说，ISA 封装了底层的硬件，为裸机程序员提供了一个统一的抽象接口。我们又看到了，封装和抽象的思想。<br><br>光说不做假把式，我们来使用一下那个年代的裸机。假想我们有一颗诞生于 1978 年的 8086 微处理器芯片，得益于芯片内部对底层逻辑电路的封装，要完成一个加法运算（1+2），我们只需要想办法让芯片的 IR 寄存器先后呈现这样的电平状态：<br>10111000 00000001 00000000
10111011 00000010 00000000
00000001 11011000
<br>这些指令代表的含义如下：<br>
<br>第一行指令是将一个立即数 1 放到 AX 寄存器中；
<br>第二行指令是将一个立即数 2 放到 BX 寄存器中；
<br>第三行指令是加法指令，即 AX = AX+BX。
<br>我们前面提到过，因为 von Neumann 架构的计算机都是存储程序方式工作的计算机。所以我们需要将这三条指令连续的放到内存中，然后想办法一步步地加载这三条指令。因为 PC 寄存器会自动地进行指令 + 1，所以我们只需要想办法让 CPU 取到第一条指令。虽然 CPU 对逻辑的封装固然方便，但是这样子还是好麻烦。<br><br>为了让机器自动地帮我们做事，软件应运而生。计算机软件是运行在计算机硬件上的一系列程序和数据的集合。软件告诉计算机硬件需要先这样，在那样......，从而实现自动化操作。<br>我们前面说，硬件提供了计算机系统的基础设施，但硬件裸机的使用体验很差，没有人会死板到直接与硬件打交道。通过编写软件，让 CPU 自动地执行软件中的一条条指令，并在适当的时刻与外部设备交互，如接收用户输入、控制输出设备等。有了这层抽象，计算机好像没那么难用了。根据其作用，计算机软件又可分为系统软件和应用软件。<br><br>操作系统就是一层软件，帮助我们管理抽象计算机的硬件资源并提供友好可靠的 API 和交互界面。作为用户，我们只要使用系统为我们提供的接口就好了。所以，我们可以说操作系统是封装硬件的软件，作用是为应用软件提供服务。操作系统提供了对硬件资源的抽象和管理，使用户不再需要直接与计算机硬件对接，替代了人工与裸机硬件的交互。<br>由于屏蔽了硬件资源和硬件的复杂性，操作系统必须为应用程序提供特殊的 API 接口，以申请和管理硬件资源。我们把这些 API 叫做系统调用，使应用程序可以通过操作系统提供的标准化接口访问硬件资源。简化了应用程序开发，还通过权限管理和硬件抽象层提高了系统的安全性和稳定性。<br><br>除了操作系统，应用程序也是计算机软件一大重要的组成部分。即使经过了系统的封装，计算机依然不算好用，但人们对于软件开发的热忱一点不减。各式各样的计算机软件层出不穷，我们的生活中充斥着让人感到新奇和让人兴奋的应用软件。<br>我们利用软件来请求计算机为我们解决一些问题或获得一些需求，不同的软件定义着不同的解决用户问题或获取需求的方式。根据软件功能，我们还能够进一步地划分（办公、娱乐......）。<br><br>在上一节中，我们从一个整体视角简单地描述了计算机系统的构成与操作系统的地位。那么，操作系统究竟是什么？它具体扮演着怎样的角色？为了更清晰地回答这些问题，我们将从用户视角和系统视角两个角度进行探讨。<br><br>前面，我们尝试模拟使用二进制机器语言直接和计算机硬件进行交互，即便只是完成加法运算，也可见这种方式的复杂性和低效性。用户希望计算机的使用变得更加简单、直观，而有了操作系统对硬件功能的封装，机器的易用性和使用体验得以大幅提升。<br><img alt="os_user_view.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/os_user_view.jpg"><br>从<a data-tooltip-position="top" aria-label="2. Evolution of Operating Systems" data-href="2. Evolution of Operating Systems" href="https://congzhi.wiki/congzhi's-os-series/2.-evolution-of-operating-systems.html" class="internal-link" target="_self" rel="noopener nofollow">计算机的发展史</a>可以看出，人机交互（用户和操作系统的接口）的方式随着科技的进步而不断演化。从最早的打孔纸带，到 CLI 命令行交互界面，再到 GUI 图形化的交互界面，直至如今的触屏交互方式，人机交互变得越来越便捷。未来，语音交互甚至脑机接口会成为新兴的交互方式么？<br>在个人电脑 PC 的时代，一个人独享一台电脑。而在 1970s，计算机仍是稀缺货，同时被多个用户使用。所以计算机还需要为每个用户提供一种自己独占整个计算机的错觉。<br>除了易用性，计算机的性能也是使用体验中很重要的一个部分。而裸机的 CPU 和硬件资源又改变不了，所以，操作系统还应通过优化资源调度策略来最大化 CPU 和硬件资源的利用率。为了提升用户的使用体验，现代操作系统中的调度算法一般会优先保障用户交互相关的任务，如图形界面的平滑操作，而将其他后台任务的优先级适当降低。<br>需要注意的是，并非所有计算机系统都以用户体验为核心。比如说嵌入式计算机系统（如车机系统）可能更注重实时性需求，甚至会以牺牲一定的用户体验为代价，确保关键任务的按时完成。<br><br>从系统的角度看，操作系统就是一组专门设计用于与硬件交互的软件，负责管理所有硬件资源。从而，任何的应用程序都需要通过操作系统提供的系统调用 API 来请求并使用硬件资源。通过这种抽象，操作系统屏蔽了硬件的复杂性，简化了应用程序的开发，同时提高系统的稳定性和安全性。<br>从这种角度看，操作系统实际上就是计算机系统的资源管理器 (resource allocator)，协调系统内的各种资源，例如占用的CPU时间、内存空间、存储空间和I/O设备的分配等。操作系统需要最大化硬件资源的利用率，还要通过权限管理与访问控制，确保资源的安全共享并防止资源冲突。<br>这种资源调度能力使操作系统成为计算机系统运行的核心，不仅仅需要协调控制 I/O 设备，还需要对用户程序的执行进行控制和管理，以防止用户程序出现错误或者对计算机资源的越界访问（防止应用程序直接操作硬件导致系统崩溃或数据损坏）。当应用程序真正想要访问系统资源时，应用程序就需要通过系统调用 API 想操作系统提出相关的请求，让操作系统代为完成。<br><br>所以我们如何去定义什么是操作系统？如何准确地定义它的角色和功能？上面我们已经看到，在不同的视角中，操作系统的定义也是不同的。在用户眼里，操作系统就是让机器易于使用的软件（通过 CLI 和 GUI）；而从系统的层面来看，操作系统就是封装硬件，通过系统调用为上层应用提供服务的抽象层。<br>在许多教科书中，你会看到这样的定义：“操作系统是管理软硬件资源，为应用提供服务的系统软件”。这实际上也是从系统的层面上来定义的。而操作系统的核心目标是构建一个“用户友好”的计算机系统，使计算机能够高效地解决用户的问题和需求。<br>虽然纯计算机硬件也能直接执行计算任务，但我们也看到硬件远远无法满足用户对易用性和便捷性的要求。因此，我们想要通过软件自动地来帮助实现这些目标。而你可以发现许多软件程序都有一些共性的操作，例如对内存的操作、I/O 的操作等。为了统一管理和实现这些功能，我们将它们集成到一个软件中，即内核 (Kernel) 。内核的出现把计算机软件划分成了系统软件和应用软件。<br>有了操作系统提供的对下层硬件的抽象，不仅能够满足用户对易用性和便捷性的需求，而且通过在计算机系统中加入一层 indirection ，应用程序想要使用硬件资源必须经过过操作系统内核的管理。提高了整个系统的安全性。<br>通过上面的学习，我们应该能够明白操作系统的职责之所在。OS最基本的职责就是为我们提供一些抽象，让系统方便使用（user perspective）。除此之外，操作系统还应该管理资源，让系统的性能得到最大的利用（system perspective）。 <br><br><br>当我们想要创建自己的软件时，我们就需要将想法告诉计算机。而计算机无法理解人类的语言，计算机能够理解的只有特定架构下的二进制 01 指令。所以我们就需要通过一些方式将我们的想法变成计算机能够理解的机器语言。这节课，我们来学习这一切如何实现。<br><br>在之前的学习中，我们了解到，与硬件交互需要使用机器语言。机器语言以二进制（01）表示，是计算机中直接与高低电平对应的语言。虽然它可以直接控制 CPU 和硬件资源，但有些太过于反人类，每次使用都要查阅相关的手册，而且还十分容易出错。<br>尽管能用二进制代码直接操纵包含数以亿计的晶体管已经可以称之为奇迹了（虽然 8086 只有大约 29000 个晶体管），但我们可能见到这种毫无章法的 01 二进制就烦。为了解决这一问题，人们发明了更易读、容易记忆的汇编语言。汇编使用助记符来表示不同的操作、寄存器等。比如说，你可以用 ADD 来指代加法操作。<br>汇编如何和机器语言对应上的呢？咱们回到之前的加法运算程序中：<br>10111000 00000001 00000000
10111011 00000010 00000000
00000001 11011000
<br>机器语言的指令是具有一定的结构的，一般由操作码 (Opcode) 和操作数 (Operands) 构成。指示 CPU 应该执行的操作类型，例如“数据移动”或是“加法”等。操作数用于指定指令操作需要的数据或存储位置，如寄存器或内存地址。<br>第一条指令是将一个立即数”移动到“特定的寄存器中，我们把称为 AX ，这是一个通用寄存器。其中，前五位表示移动指令，后三位用于指定移动到哪个寄存器。之后的 00000001 00000000 是用小端方式表示立即数 1 。也就是说，这一条指令的作用是把立即数 1 移动到 AX 寄存器中。<br><br>第二条指令和第一条指令差不多。<br>第三条指令是一个加法指令，其中操作码是 00000001 ，表示一个加法操作。后面的一个字节表示将两个寄存器中的数进行相加。即 11 011 000 。11 表示的就是操作数是两个寄存器。<br>我们将上面的机器语言符号化，用 MOV AX 表示原来的 10111 000，用 ADD 来指代之前的加法指令操作码 00000001。不同的寄存器（000 - 111）也用字母，如 AX BX等来表示。这样，一一对应的，我们就得到了下面的汇编语言指令：<br>MOV AX, 1      ; AX = 1
MOV BX, 2      ; BX = 2
ADD AX, BX     ; AX = AX + BX → AX = 3
<br>这样，是不是容易理解多了？<br>从机器语言到汇编的转变是计算机语言发展中的重大节点。通过符号化的助记符，程序员可以更好更直观地编写代码了。但仍然，汇编还是不够好。因为计算机的架构不同，使用的机器语言和汇编语言仍然是不同的（汇编一一对应机器码）。这是由 ISA 所决定的（如 x86, ARM, RISC-V, MIPS等）。<br><br>高级语言是汇编的封装和抽象。高级语言增加了代码在不同架构平台上的可移植性，屏蔽了底层细节，使得同一段代码可以在不同架构的机器下运行。这是通过高级语言编译器实现的，编译器会将高级语言程序编译成特定平台的汇编语言，再由汇编器将汇编代码转换成机器码供计算机读取。<br>最初的 Unix 系统就是用汇编语言编写的，然而汇编语言是 machine-specific 的，不支持不同平台的移植。虽然第一个高级语言 FORTRAN 的出现代表着编程语言有了更高层次的抽象，但最开始仍未解决可移植性问题。即你可能需要在不同的平台上写不同的程序。<br>C 语言的出现改变了这一局面。C 语言设计的初衷之一就是为了实现代码的可移植性。它通过提供一个接近底层硬件的抽象层，使得程序员可以编写在不同硬件平台上运行的代码，而不需要对每个平台进行大量的修改。<br><br><br><br><br>在前两节课中，我们初步介绍了抽象的概念——抽象是计算机科学发展的基石。CPU 通过暴露管脚，将晶体管的硬件封装为一个抽象的计算单元；操作系统则进一步对这些封装的硬件（如 CPU、内存、I/O 等）进行抽象，为应用程序提供接口。这种设计让开发者能够忽略硬件的复杂性，而将精力放在实现业务逻辑和用户交互上。<br>随后，我们学习了汇编语言和高级语言。汇编语言对机器语言进行抽象，为开发者提供了更友好的编程接口。而高级语言则进一步抽象底层细节，大幅提升了代码在不同平台上的可移植性，让开发者能够更轻松地与硬件交互。<br>通过抽象，复杂的底层细节得以简化，使得计算机系统能够更高效地工作，同时也让开发者能够专注于解决实际问题。在理解了抽象的概念后，本课将探讨操作系统需要管理的资源及其管理方式。简单来说，操作系统必须管理以下几大核心资源：<br>
<br>处理器 (CPU)
<br>内存空间 (Memory)
<br>文件存储 (File Storage)
<br>I/O 外设 (I/O Devices)
<br><br>如果一段程序不能被 CPU 所执行，那即使这段程序功能在强大，它依旧没有任何意义。在操作系统中，进程指的是正在执行的程序。比如，你现在正在浏览的网页、运行的微信应用，以及正在播放的 QQ 音乐，都属于独立的进程。它们的共同点是：这些程序实例都在一台计算机上运行（即由 CPU 执行）。<br>我们把可以在机器上运行的程序称为可执行程序，这些程序通常经过操作系统的封装（Windows 下的 .exe 格式，Linux 下的 .elf 格式）。在磁盘上存储的可执行程序需要被载入内存后，才能转化为进程并开始执行。<br>简单来说，进程可以被理解为程序在 CPU 上运行的一个实例。静态存储在磁盘上的程序是“被动的静态实体”，而进程则是“主动的动态实体”，因为它代表了程序正在运行的状态。所以同样的程序载入两次内存，创建的是两个不同的进程。<br>为了完成任务，运行中的程序——即进程需要操作系统分配必要的资源，例如：<br>
<br>CPU 资源：用于执行程序中的指令。
<br>内存资源：用于载入程序代码段和数据段，以支持程序正常运行。此外，运行中的程序也会动态的申请和释放内存。
<br>I/O 设备资源：例如把内容显示到屏幕上或接收用户输入数据。
<br>文件资源：用于读取配置文件或持久化保存处理的数据。
<br>当我们启动一个程序时，操作系统会将程序载入内存并为其分配所需的资源。在程序的运行过程中，进程也可能会动态向操作系统申请额外资源，确保任务能够持续完成。而当进程终止时，操作系统会需要回收该进程占用的所有资源，并重新分配给其他任务。<br>在操作系统中，为了管理和调度进程的运行，操作系统需要支持以下功能：<br>
<br>进程的创建和删除：动态地在系统中生成新的进程，同时支持对执行完毕进程的清理和删除。
<br>进程的挂起与恢复：提供暂停进程在 CPU 上运行的机制，并在需要时恢复进程的执行状态。
<br>CPU 上的进程调度：根据调度算法将进程分配到 CPU 上运行，实现对系统资源的高效利用。
<br>进程间通信的机制：提供进程之间的协同工作、资源共享和任务协调，完成更复杂的任务。
<br>进程间同步的机制：确保多个进程在共享资源时不会发生冲突（进程 A 读的同时进程 B 写）。
<br>这些功能我们将会在 PROCESS MANAGEMENT 的部分进行介绍。<br><br><br>CPU 只能从内存中加载指令执行，所以任何需要运行的程序就需要首先加载到内存中。在大多数情况下，计算机都会从一块可读可写的内存中读取并运行程序，我们称之为主存，也叫随机存储器 (Random Access Memory, RAM)。主存一般采用动态随机存储器 (DRAM) 的半导体技术实现。<br>RAM 是一种易失性 (volatile) 存储器，即一旦断电，RAM 中存储的数据就会消失。所以除了主存，计算机系统中还需要其他类型的存储器来保存关键数据，即使在断电时也不丢失。比如，计算机加电后运行的第一个程序，我们称之为启动程序 (bootstrap program)，一般存储在一块非易失性存储器中，如闪存 (Flash) 和电可擦写可编程只读存储器 (EEPROM) 。<br>内存由一系列的字节构成，每个字节都拥有其唯一的地址。不同指令集架构下的 CPU 可能通过不同的汇编指令和内存进行交互。比如在 x86 汇编中，常常通过 MOV 指令来实现加载和存储操作。如：<br>MOV AX, 10
MOV BX, [0x1000]
ADD AX, BX
MOV [0x1000], AX
<br>这段代码将一个立即数 10 赋予 AX，然后将内存地址 0x1000 的数据加载到寄存器 BX 中。相加两个寄存器并将结果存储在 AX 中，最后将结果写回 0x1000 中。<br>而在 ARM 架构下的汇编中，CPU 通过 LDR 和 STR 指令和内存进行交互。LDR 就是将主存中的数据加载到 CPU 中的某个寄存器里面，STR 就是将寄存器中的内存存储到主存中。我们在上面看到，x86 是通过 MOV 指令来实现这两种操作的。ARM 汇编中，我们可以用下面的代码实现同样的功能：<br>LDR R1, [0x1000]
ADD R1, R1, #10
STR R1, [0x1000]
<br>其效果和 x86 汇编是一样的。<br>之前，我们接触到了 CPU 指令周期，在 von Neumann 架构的机器下，CPU执行的每条指令都会经历这样的指令周期，包括：取指令周期，解码周期，执行周期和写回周期（可能没有）。在指令的取指令周期中，CPU就会从内存中加载指令。<br>从上面的描述中，我们窥得在主存计算机系统中的核心地位，但是 DRAM 半导体的性质赋予了主存一些难言之隐。在大多数情况下，我们都希望自己的劳动成果能够永久地保存下来。但主存是易失性的存储器，只要断电，内容就会永久丢失掉。而且主存一般情况下都很小，不能够存储我们想要的所有程序和数据。<br><br>为了解决主存的难言之隐，大部分的计算机系统都会提供一个多级的存储器结构来优化存储性能并扩展主存功能。比如，我们现在有二级存储 (secondary storage) 来作为主存的扩展。我们可以将需要永久保存的程序和数据放到二级存储器中进行永久保存。<br><img alt="Storage_Device_Hierarchy.png" src="https://congzhi.wiki/congzhi's-os-series/pics/storage_device_hierarchy.png"><br><br>二级存储器通常由 hard-disk drives (HDDs) 或 solid-state drives (SSDs) 这类非易失性的 (nonvolatile) 存储介质组成。大多数程序都会存储在二级存储器中等待加载进内存中执行。虽然二级存储器容量很大，而且具有非易失性，但二级存储器通常都很慢。<br><br>非易失性的存储器除了二级存储器之外还有三级存储器。三级存储器结构通常由光盘和磁带构成，你可以想象它们有多慢，所以三级存储器一般只用作程序或数据的备份。<br><br>无论是计算机系统还是存储层次结构中，主存始终处于核心位置。主存就相当于一个共享的仓库，为 CPU 和 I/O 设备提供快速的访问。在取指令周期中，CPU 从主存中读取指令。我们先前提到程序和数据都是存储在二级存储器中的，但主存是 CPU 唯一能够直接访问的大容量存储器，如果 CPU 要执行某个程序，就必须首先被加载到主存中。<br>在早期的计算机中，主存容量有限，这意味着内存可能只能确保一个程序的执行。为了让程序能够运行，操作系统会采用将程序加载到固定物理地址（绝对地址）。程序直接使用物理地址来访问内存。CPU 也通过物理地址取指令和数据。在程序执行结束后，操作系统会释放其占用的内存空间，供其他程序使用。<br>之后，随着内存的扩大和 CPU 性能的提升，为了充分利用资源并支持多程序系统，降低计算机对用户的响应，使得计算机需要将好多个进程加载进内存中。也就诞生了最早的对内存管理的需求。为实现有效的内存管理，诞生了许多不同的内存管理模式，并且大多都并依赖特定硬件的支持。<br>为了支持多程序并发，后续系统引入了静态重定位 (static relocation) ，通过依赖一些特殊的寄存器（如基址寄存器和界限寄存器）来实现地址的转换。这个时期的系统开始使用逻辑地址(Logical Address) 的概念。这时的内存管理仍然非常简单，操作系统只需要：<br>
<br>跟踪物理内存状态：记录空闲/已用内存区域。
<br>动态分配/回收内存：通过一些内存分配算法来分配物理内存。
<br>进程隔离与保护：通过寄存器实现简单的进程隔离。
<br>交换技术 (Swapping)：将闲置进程换出到二级存储器上以便腾出更多的内存空间。
<br><img alt="memory_layout_multiprogramming.png" src="https://congzhi.wiki/congzhi's-os-series/pics/memory_layout_multiprogramming.png"><br>为了进一步优化内存的使用效率，同时保障系统的稳定性和安全性。现代操作系统引入了更复杂的内存管理机制，例如，现代计算机系统都引入了内存管理单元 (MMU) 来实现如虚拟内存 (Virtual Memory) 和内存保护 (Memory Protection) 等更复杂的内存管理机制。此处略过。<br>我们后续将在 Memory Management 部分详细介绍操作系统是如何进行内存管理的。<br><br>除了非易失性，主存还有一个问题，即相比较于 CPU 而言的速度太慢了。在 70-80 年代的早期计算机中，主存的速度和 CPU 较为匹配。但随着半导体技术的突破，主存和 CPU 性能开始扩大，逐渐落后于 CPU 的吞吐要求。<br>（比如 1980 年 8086 CPU 主频 5MHz（200ns），DRAM 访问延迟约 200ns，基本匹配；而 2020 年的 i9-10900K 主频 5.3GHz（0.19ns），DDR4 内存延迟约 50ns，差距达&nbsp;263 倍。）<br>这种速度差异就意味着 CPU 在等待主存响应时会浪费大量资源。为了弥补这一性能差距，我们需要一种速度能匹配 CPU 的存储介质。要这么多层的存储介质的好处就是让每次 CPU 在取指令/数据时，都优先从更快的存储介质中寻找，这就是缓存 (Caching) 所做的事情。<br><br>之前我们了解到了二级存储器的概念，在一定程度上解决了内存易失性的问题。我们会将程序和数据存放在二级存储器上，如果 CPU 在内存中找不到相关的信息，系统就会从二级存储器中寻找并将需要的信息全部或部分的加载到内存中。所以，你可以将内存理解为二级存储器的“缓存”。<br>为了匹配 CPU 的速度，我们需要引入一种更快的存储介质，这样在 CPU 执行指令时，率先从这种更快的介质中寻找需要的数据或指令。如果未找到，则从内存中加载数据，并将数据存入这种告诉存储介质中方便后续使用。我们把这种介质称为缓存 (Cache) 。<br>缓存是用一种叫 SRAM (Static Random Access Memory) 的技术实现的，它有以下特点：<br>
<br>高速度：访问速度只需要约 5 个 CPU 时钟周期，相比之下，主存需要上百个时钟周期。
<br>小容量：大小通常在 MB 级别，相比之下，主存大小通常是 GB 级别的。
<br>现代计算机系统中，缓存通常集成在 CPU 芯片上，并且采用分层设计，以进一步优化性能。<br><img alt="speed_of_storage_hierarchy.png" src="https://congzhi.wiki/congzhi's-os-series/pics/speed_of_storage_hierarchy.png"><br>缓存有 CPU 和内置的 MMU 单元进行管理，虽然这一层次对于操作系统而言不可见，但是系统程序员必须了解这一存储层次。比如，操作系统的调度策略就需要考虑到缓存这一层次的存储结构。在 CPU SCHEDULING 阶段和 MEMORY MANAGEMENT 阶段，我们还会接触到这一存储层次。<br><br>在操作系统中，I/O 管理是核心模块。在某种意义上，I/O 系统的运行效率和稳定性直接地决定了整个系统的可用性，因为 I/O 的管理涉及到于用户交互设备的工作（如键盘、鼠标、显示器等）。为了确保系统的可靠性和性能，操作系统需要对 IO 进行管理。<br>此外，操作系统的一大职责就是对底层硬件的封装。这意味着操作系统需要封装并隐藏 I/O 设备的细节，为上层应用提供统一的接口，从而简化开发与使用。这些 I/O 设备的复杂细节由操作系统的 I/O 子系统负责管理。I/O 子系统主要包含以下部分：<br>
<br>内存管理组件：包括缓冲 (Buffering)、缓存 (Caching) 和假脱机 (SPOOLing)。用于优化 I/O 的数据传输速度。
<br>统一接口：为不同的设备提供通用的设备驱动接口，使得上层软件无需关心硬件细节。
<br>设备管理：通过设备驱动程序管理和区分特定的硬件设备，屏蔽硬件差异。
<br>I/O 调度：优化设备请求处理的顺序，提高资源利用率和吞吐量。
<br>中断驱动：通过中断请求设备并通知操作系统设备完成任务，避免 CPU 的等待。
<br>中断我们会在 INTERRUPTS AND SYSTEM CALLS 进行介绍，操作系统 I/O 子系统会和大存储管理、文件系统管理在 I/O SUBSYSTEM &amp; MASS STORAGE &amp; FILE-SYSTEMS 这三个阶段进行详细地介绍。<br><br>在学习内存管理的时候，我们穿插了一些层次化存储结构的内容。我们看到，在现代计算机中，计算机系统必须提供二级存储层次作为主存的后备。二级存储器是一类特殊的 I/O 设备，通常采用 HDDs 和 SSDs 这样的存储介质。它们的速度较主存慢得多，且数据的交互方式类似于 I/O 操作。<br>为了高效管理二级存储，操作系统需要提供如下的管理功能：<br>
<br>磁盘分区：将物理存储设备划分成多个逻辑分区。
<br>分区挂载：将分区与操作系统的目录结构连接，以便用户和程序可以访问分区中的数据。
<br>空闲空间管理：记录哪些磁盘区域是空闲的，并动态分配空闲空间以存储新数据。
<br>存储分配：根据文件或数据块的大小分配磁盘空间，尽量减少碎片化，提高存储效率。
<br>磁盘调度：多个请求同时发生时，决定磁盘访问的顺序，减少磁盘寻道时间。
<br>存储保护：确保文件和分区的安全性，防止未经授权的访问或数据损坏。
<br>除了二级存储器，计算机系统中可能还存在三级存储结构（如磁带驱动器、光盘驱动器等），这一层次对于性能而言并不关键。但如果系统还包含这一层次的存储，操作系统也需要提供类似的管理功能，如：<br>
<br>挂载和卸载
<br>数据迁移
<br><br>文件系统操作系统在大存储系统管理的基础上实现的核心模块。借助二级/三级存储器（也叫块设备）非易失性的特性，文件系统实现了对程序和数据的持久化保存 (Persistence)。文件系统一般会封装对二级存储器的大存储管理，为用户提供了更高级的接口，方便操作和管理。<br>操作系统会将物理存储设备上存放的数据抽象为逻辑上的存储单元，也就是我们常见的文件 (File)，文件是一个包含相联信息的集合。通常而言，文件用于表示程序或数据（可能是数字、字母或二进制类型）。文件有很多类型，一般根据后缀名来区分不同类型格式的文件。<br>为了更好地管理二级存储器上的文件，文件系统通常会用目录 (Directories) 来管理文件。此外，文件系统还会将逻辑上的文件与存储文件的底层的物理存储介质（如块设备）进行映射，管理其逻辑与物理结构。<br>为了高效管理二级存储器上的文件，文件系统提供了以下核心功能：<br>
<br>创建并删除文件：允许用户在文件系统中创建、删除文件，确保存储资源的动态管理。
<br>创建并删除目录：目录（文件夹）用于组织文件，形成层次化的结构以方便存储和检索。
<br>文件和目录操作：提供操作文件和目录的操作，如读取、写入、重命名、复制等。
<br>文件和存储设备的映射：负责将文件的逻辑地址映射到底层存储介质上的物理块。
<br>文件持久化备份：支持将内存中的数据备份到非易失性存储设备上，持久化存储数据。
<br><br><br>之前的学习中，我们明确指出了操作系统的核心作用，即封装底层硬件、为上层应用软件提供服务的系统程序。通过引入间接层，操作系统使得用户能够更方便地使用计算机，同时不需要关心复杂的硬件细节。此外，系统的安全性也得到了保障，因为每个程序必须通过系统调用来申请资源。<br>在本节课中，我们将学习操作系统如何通过抽象与虚拟化为应用程序提供服务，并优化资源使用。同时，我们还会回答一个关键问题：操作系统为应用程序提供的资源在应用的视角下是怎么样的？<br><br>抽象是操作系统设计中一个重要且核心的概念。前两课中，我们已经隐约地感受到抽象在不同层次的应用。在第一节课中，我们了解到 CPU 是对晶体管硬件的封装抽象、操作系统是对 CPU 内存及 I/O 等计算机硬件资源进行封装抽象。在第二节，我们又从编程语言的角度理解抽象。<br>抽象的意义在于：隐藏底层的复杂性，只向上层暴露最关键的信息。抽象可以简化上层的开发过程，并为系统架构的扩展性提供了支持。抽象的结果就是接口，它可以连接不同层次的系统组件，支持更高级的应用。根据功能与连接对象的不同，接口可以划分为以下几类：<br>
<br>硬件与硬件之间的接口：比如主板上的各种端口。用于连接内存、CPU和I/O设备。
<br>软件与软件之间的接口：比如程序之间用来交换数据的应用程序编程接口 (API) 。
<br>硬件和软件之间的接口：比如操作系统通过硬件抽象层 (HAL) 将底层硬件细节封装。
<br>我们在这里关注的是应用-操作系统-硬件之间的抽象，所以我们下面先简单介绍一下经过操作系统硬件抽象层抽象后的硬件资源在操作系统眼中是怎么样的。<br><br>操作系统硬件抽象层是内核的一部分，主要提供针对底层硬件的标准化接口。操作系统的作用之一就是封装底层硬件，内核的 HAL 就是系统与硬件进行直接交互的部分。使操作系统能够通过硬件驱动程序与具体的 CPU、内存和外设交互，而无需关心硬件实现的差异。<br>经过 HAL 的封装，操作系统的其他子模块（如调度器、内存管理、IO 子系统）可以使用统一的接口与下层的硬件进行交互，而不用担心平台架构的不同（ARM 还是 x86）。由此，HAL 的存在赋予了操作系统在不同架构平台上的可移植性。在不同的平台下，只需要更换相应的 HAL 模块就好了。<br><br>在 HAL 屏蔽硬件的细节的基础上，操作系统的其他资源管理模块就可以无视硬件细节，将 HAL 封装的系统硬件资源进行二次抽象来为应用程序提供更高级的 API 接口。二次抽象的作用往往就是对资源进行统一管理并提升系统的安全性。<br>以内存管理为例，内存管理模块会通过分页 (Paging) 机制将物理内存虚拟化成虚拟内存，为应用程序提供连续的虚拟地址空间。从而应用开发者就不需要关心实际物理内存的布局了。<br>由于虚拟内存需要通过操作系统内存管理模块将虚拟内存转化成物理内存，之后由 HAL 进行实际的分配和管理。所以间接的确保了内存资源的安全性。此外，资源管理模块通常还会实现更为高级的功能，如写时复制、内存隔离等高级功能。<br><br>在进程的视角下，操作系统通过虚拟化技术将物理资源转换为一致的虚拟资源供进程使用。尽管进程在运行时可以获得 CPU 资源、内存资源、I/O 资源，但经过资源管理模块的二次抽象，这些资源都将是虚拟的，什么意思呢？<br><br>上面我们提到了虚拟内存，简单来说，进程的视角下，它自认为得到的是一个连续、完整的内存空间。而在实际的物理内存中，这些部分可能是分散存储的，甚至与其他进程共享。<br><br>从进程的视角来看，每个进程也都认为自己独占 CPU。然而，这实际上是通过操作系统调度器实现的虚拟化。操作系统通过轮转调度 (Round-Robin) 等调度算法，将物理 CPU 的使用权在多个进程之间快速切换。<br>这种调度策略称为分时策略 (time-sharing)，操作系统将 CPU 的运行时间划分为多个时间片，每个进程在一个时间片内运行，时间片耗尽后切换到下一个进程。通过这种方式，操作系统为每个进程提供了虚拟 CPU 的假象，使其感觉自己持续运行。<br><br>此外，操作系统还会通过设备驱动程序和 HAL，将复杂的设备交互封装为标准化的系统调用接口。在进程的视角下，I/O 操作简单而直接（read(), write 系统调用），根本不需要了解 I/O 是怎么运作的。<br><br>经过上面的学习，我们了解到应用程序使用的资源只是操作系统通过虚拟化为应用程序提供的一种“幻象”。这层“幻象”是操作系统将底层物理资源（如 CPU、内存和 I/O 设备）通过抽象与虚拟化技术封装后呈现的。<br>作为使用计算机的用户，我们对机器的接触也停留在这一层“幻象”上。由于操作系统的封装和抽象，我们是看不到硬件的细节和物理资源是如何分配的。操作系统会帮我们将这一层“幻象”转换成机器上的物理资源。所以，我们可以说操作系统为应用程序和用户提供了一台“虚拟的机器”。<br><br>在上小节，我们讨论了资源虚拟化的概念，通过操作系统各个资源管理模块对物理硬件进行抽象和封装，使得应用程序能够简化对底层硬件的使用。而除了资源虚拟化，操作系统一般还提供一种更高级的虚拟化技术 (Virtualization)，即提供在一个物理主机上运行多个操作系统的能力。<br>这种虚拟化一般通过一层虚拟化软件来实现，一般称为虚拟机管理器 (Hypervisor)。Hypervisor 会抽象底层的硬件资源，为每个操作系统实例（虚拟机）提供独立的运行环境。这种虚拟化技术和资源虚拟化很类似，我们将在 ADVANCED TOPICS 中进行详细地介绍这种虚拟化技术。<br><br><br>在考研 408 科目的参考书之一——《计算机操作系统》（汤小丹）中，将操作系统的特性归纳为“四大特征”（即并发、共享、虚拟和异步）。我们介绍过了虚拟（资源虚拟化），本节课我们就来介绍其他的三个特征。<br><br>在操作系统中，共享是资源管理的核心目标之一。通过共享，我们期望多个应用程序可以同时使用有限的系统资源，以提升资源利用率。共享特性往往基于资源虚拟化技术。操作系统通过将物理硬件资源（如 CPU、内存和 I/O）虚拟化，为每个进程提供一个独占所有资源的“幻象”。然而，实际情况是，这些资源被多个应用程序所共享。<br><br>内存是程序运行的基础资源。在操作系统中，主存通过虚拟内存技术被抽象为一个独立、连续的地址空间，供每个进程使用。实际上，进程看到的虚拟内存可能与其他应用共享相同的物理内存页。在实际应用中，共享内存可以用于应用程序之间的通信，还有通过复用相同的代码段来实现内存共享的动态链接库。<br><br>I/O 设备也是典型的共享资源。例如，多个任务可能需要使用同一个打印机，或者多个用户同时访问文件系统。操作系统就需要通过合理的 I/O 调度和资源管理，确保 I/O 的有序访问。<br><br>通过调度器的对物理 CPU 的虚拟化，每个进程会认为自己得到了独立的 CPU，但实际上得到的是一段 CPU 的时间片。操作系统的任务调度器通过快速切换任务，使得每个任务都能在极短的短时间内获得 CPU 的使用权。从而，在用户看来，多个任务似乎在同时进行。<br>通过分时，一个物理的 CPU 核心可以被多个进程在逻辑上“同时”使用，实现一种在宏观上同时运行，但在微观上时间片轮流交替的效果，这就是并发 (Concurrency)。<br><br>异步操作指在执行某个操作时，不需要等待该操作完成，而是可以继续执行其他任务。操作完成后，会通过某种机制通知执行者。这种机制在操作系统中尤其重要，特别是用于管理慢速 I/O 操作。异步机制使得 CPU 不需要和慢速 I/O 一直打交道，提高了 CPU 的运行效率。<br>操作系统通过中断和信号机制来实现异步操作。当某个 I/O 操作完成时，硬件会发送一个中断信号给 CPU，CPU 会暂停当前的任务，转而处理这个中断信号。处理完中断信号后，CPU会恢复之前的任务。这样，进程就不需要一直等待 I/O 操作的完成，而是可以在 I/O 操作完成时被通知。<br>异步操作不仅限于 I/O 操作，还可以应用于其他需要等待的操作，比如网络通信、定时任务等。通过异步操作，操作系统能够更高效地利用资源，提高系统的响应速度和吞吐量。<br><br><br>多个特权级别用于区分程序的运行权限；<br>
通过用户帐户和权限管理隔离不同用户；<br>
使用虚拟地址空间隔离不同的应用程序；<br>
...]]></description><link>https://congzhi.wiki/congzhi's-os-series/1.-introduction-to-operating-systems.html</link><guid isPermaLink="false">Congzhi's OS Series/1. Introduction to Operating Systems.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 19 Aug 2025 17:53:11 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/computer_hardware.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/computer_hardware.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2. Evolution of Operating Systems]]></title><description><![CDATA[ 
 <br>第一遍重写进行中<br><br><br>回顾 <a data-href="1. Introduction to Operating Systems" href="https://congzhi.wiki/congzhi's-os-series/1.-introduction-to-operating-systems.html" class="internal-link" target="_self" rel="noopener nofollow">1. Introduction to Operating Systems</a>，我们知道了操作系统作为一种系统软件，负责封装底层硬件的复杂细节，为用户提供简洁高效的交互方式。但这是站在今天的视角对现代操作系统从上至下的一种审视。在计算机系统发展的过程中，这些抽象是一步一步建立的，并不是一蹴而就的。<br>操作系统的革新不单单体现在人机交互模式的逐步改进（Bare machine -&gt; CLI -&gt; GUI-&gt; Touch Interface），也受到半导体技术发展和硬件种类增加的驱动（mainframe -&gt; minicomputer -&gt; PC -&gt; handheld）。这些硬件的演进不仅带来了计算能力的提升，也为操作系统设计开辟了新的方向。<br>在本阶段，我们将结合不同时代的计算机系统，深入探讨那些具有深远影响力的计算机系统和操作系统。每个时代都具有许多富有创造力的想法和技术突破。也正是这些伟大的创新推动了计算机科学的发展，它们既可能引领硬件的诞生与革新，也可能促成操作系统的变迁与进步。<br>通过这些历史阶段的梳理，我们将分析硬件与操作系统如何相辅相成：硬件的进步为操作系统提供了发挥作用的平台，而操作系统的变迁反过来推动了硬件的广泛应用与普及。本阶段，我们也将讨论操作系统和硬件如何不断创新并解决新的技术挑战，共同塑造现代的计算机系统。<br><br>回顾计算机系统的发展历程，从最初动辄占地 30 平米的真空管计算机，到如今比鸡蛋还小的智能手表，我们不难发现计算设备不仅趋于小型化，同时交互方式也逐渐智能化。这种变化主要得益于半导体工艺的突破和操作系统功能的丰富。<br>根据设备的体积和功能特点，我们可以将计算机的发展阶段划分为以下几类：大型机 (mainframes)、小型机 (minicomputers)、台式机 (desktop computers)、掌上电脑 (handheld computers) 和 智能设备 (smart devices)。<br>随着制造工艺的不断进步，计算机的体积与性能之间的关系发生了变化。在早期，由于技术限制，机器体积越大，性能往往越强。但在半导体工艺提升后，小型化芯片能够承载更高的计算能力，这为操作系统功能的迁移提供了支持。<br>许多操作系统的功能最初设计用于大型机，然后随着半导体技术的发展才逐步下放至体积更小的设备（同体积晶片能实现更强大的计算性能），实现了更广泛的应用场景。下图展示了不同时期操作系统功能从大型机向小型设备迁移的趋势。<br><img alt="migration_of_OS_concepts_and_features.png" src="https://congzhi.wiki/congzhi's-os-series/pics/migration_of_os_concepts_and_features.png"><br>在本阶段的后续课程中，我们以每 10 年作为一个时期来介绍这个时代的计算机系统以及其提供的系统级服务，即支持怎么样的操作系统。<br><br><br>在 1940 年代之前，通用的存储程序的 von Neumann 架构的计算机还未诞生。这时的计算机被称为专用计算机，因为它们只能完成一些特定任务，通常依赖齿轮等机械结构来完成计算。<br>在 1945 年，John von Neumann 提出了现代计算机的基本架构，即“存储程序”思想。强调将程序和数据放在存储器中，使得计算机可用通过读取存储器中的指令来自动执行任务，而无需人工干预。为后来通用计算机的出现奠定了理论基础。<br>在第二次世界大战时期，军事上复杂计算的需求也直接地推动了计算机科学的发展。无论是世界上第一台通用的电子计算机——ENIAC，还是最早的 von Neumann 架构的计算机——EDVAC，最早都是用于计算炮弹弹道和研发核武器提供支持（虽然没它们有直接用在二战中）。<br><br>在计算机系统发展的早期阶段（1940s），尽管 von Neumann 架构的通用计算机得到了成功运行，但这个时期的计算机都有一些共同点：<br>
<br>操作原始：使用开关、插拔板 (plugboards) 和打孔纸带 (punch-cards) 与计算机交互。
<br>体积庞大：使用上万个真空管、电阻和电容，使得计算机体积庞大，能耗极高。
<br>成本高昂：由于真空管、精密机械部件和复杂的设计，使得制造成本高昂。
<br>可靠性低：真空管的故障率高，极易损坏。由于线路裸漏，一个虫子 (bug) 就可能导致短路。
<br>编程复杂：程序员需要使用机器语言编程，通过物理插拔版连接逻辑单元。
<br>运行较慢：低可靠性和人工操作方式使得计算机大部分时间都处于空闲状态。
<br>用途单一：仍主要用于军事研究和科学计算。
<br>由于这个时期的计算机通常体积庞大，因此也被称为大型机 (mainframes) ，大型机这个名称之后被沿用了下来。直到今天，我们都会将体积庞大的计算机称为大型机 ，但不同的是，受限于工艺，那个年代将计算机做得像房子一样大是迫不得已。<br><img alt="mainframe_computer.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/mainframe_computer.jpg"><br>因为没有操作系统，所以用户需要直接操作硬件。通过控制台 (console) 上的指示灯、按钮和电缆插孔来调试操作机器。我们把这种人机交互方式称为手工操作方式。在这个时期，人们用纸带来存储输出结果和程序代码，避免代码的重复输入。<br><br>虽然使用真空管的计算机用现代的眼光来看非常慢（100 kHz），但仍然比人类计算快多了。但我们仍然有一个问题：早期的计算机系统一次性只能服务一个任务，每次任务计算完成后都需要物理重构硬件（重新在控制台设置开关、重连电缆），这可能耗费数小时甚至数天。<br>由于计算机在每次任务完成后都必须重新调整硬件，所以即使 CPU 的运算速度远快于人工计算（每秒可执行约 5000 次加法），但大量时间仍被手工操作占用，导致计算机的空闲率非常非常高，真正用于计算的时间极为有限。<br>这一低效的工作模式直接催生了 50 年代批处理系统的出现，使计算机的利用率得以显著提升。<br><br><br>40 年代，大多数计算机系统仍然依赖人工切换任务，效率极低。而且每次计算完成后都需要人工调整硬件，导致计算机的空闲时间远远多于实际计算时间。人们希望计算机每次任务完成后都能够自动地重置状态并加载下一任务，而不依赖人工干预。这一思想最终推动了批处理系统的出现。<br>50 - 60 年代是批处理的时代。50 年代的单道批处理（如 IBM 的 IBSYS）的出现减少了 CPU 的空闲时间，一定程度上提升了 CPU 的利用率。而至 60 年代，多道批处理 (Multiprogramming Batch Processing) 的多任务调度使得系统能够同时容纳调度多个程序，CPU 利用率一度提升到 50% - 60%（如 IBM OS/360）。<br>尽管晶体管技术在 1947 年已被发明，但由于 1950 年代尚未实现大规模商用化，计算机仍然依赖真空管进行计算。这就导致 1950 年代的计算机仍然处于真空管计算机的第一代计算机，体积庞大、功耗高且故障率高，因此这个时期仍然是大型机主导的时代。<br>1960 年代，晶体管与 SLT 混合集成电路（如 IBM System/360）的普及标志着第二代计算机的成熟。随着硬件成本下降和晶体管的广泛应用，计算机的体积得以进一步缩小，同时性能和可靠性得到大幅提升。在这一时期，小型机(Minicomputers) 开始出现，例如 1965 年出现的 DEC PDP-8，它比传统大型机更小（冰箱大小）、更经济（1.8万美元），推动了计算机的普及。<br><br>到了 1950 年代，计算机开始不再局限于军事用途，但普通人仍然是接触不到的，这个时期的计算机仍非常昂贵，其受众是军队、大型公司和大型的科研单位。在之后的很长一段时间内（直到1970 年代），你都需要将你自己想运行的程序提交给大型机的操作员代为处理（高校场景）。<br><br>相比较于 40 年代手工插拔电缆复位的方式，50 年代的大型机开始支持用户通过控制台直接操作计算机，极大地提升了使用的便利性。比如 1952 年推出的 IBM 701，操作员可以直接在其控制台上输入命令来控制计算机的运行，不需要重新布线。<br>下面是一张当年的高端计算机 IBM 701 的控制台照片。<br><img alt="IBM_701_console.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/ibm_701_console.jpg"><br>现代计算机的处理器、内存等大多都集成再一块主板上。但在 50 年代的计算机上，由于仍然使用电子管，所以和 1940 年代计算机一样，是由几大部分构成的。以 IBM 701 为例，硬件单元就有：<br>
<br>IBM 701：计算控制单元，也就是CPU。
<br>IBM 706：使用 Williams Tube 的经典存储单元，是系统的内存。
<br>IBM 711：打孔卡阅读器，用于数据输入，每分钟可以阅读 150 张打孔卡片。
<br>IBM 716：打印机。
<br>IBM 721：打孔卡记录机，用于输出计算机处理好的数据，每分钟可以记录 100 张卡片。
<br>IBM 726/727：磁带机，用于输入/输出磁带存储数据。
<br>IBM 731：磁鼓存储器，用于辅助存储数据。
<br>IBM 736/741：电源框架，提供计算机运行所需的电力。
<br>IBM 740：阴极射线管输出记录器，用于显示计算结果。
<br>IBM 746：电源分配单元，管理整个系统的电力分配。
<br>IBM 753：磁带控制单元，可控制 最多 10 台 IBM 727 磁带机。
<br>在 1953 年， John Backus 为 IBM 701 开发了 Speedcoding 的工具，支持通过打孔卡片/磁带连续地加载任务队列。这种能够连续加载多个程序的功能是批处理的雏形，但仍不属于自动批处理系统，因为每次任务结束后仍需要依赖人工调度，手动启停。<br><br>1954 年，IBM 推出了 IBM 704，这是首个支持磁芯存储和浮点运算的计算机。如此强悍的性能，加上 Speedcoding 的经验直接促成了首个被广泛应用的高级语言——FORTRAN 的诞生。高级语言的发展也催生了后面 FORTRAN Monitor System，为首个批处理系统 IBSYS (IBM Batch System) 奠定了基础。<br><img alt="IBM_704.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/ibm_704.jpg"><br>在 1954 年，假如我在使用一台 IBM 704，当我想运行一段 FORTRAN 代码时，我就需要完成如下的步骤：<br>
<br>加载 FORTRAN 编译器磁带
<br>运行编译器，将高级语言源程序变为汇编
<br>卸载 FORTRAN 编译器磁带
<br>加载汇编磁带
<br>运行汇编器，将汇编语言源程序转换为机器语言的二进制目标程序
<br>加载目标程序
<br>运行目标程序
<br>每一步都需要操作员加载、运行、卸载程序。每当操作员在命令台上操作时，CPU 就会空闲。而 IBM 704 每月租金大约 15000 美元，为了让投资有意义一点，就需要提高 CPU 的利用率，不让 CPU 空闲下来，我们就得让机器自己尽可能不停的加载运行程序。<br><br>为了减少机器的空闲时间，人们开发出了自动作业排序 (Automatic Job Sequencing)，它不需要人工干预，会自动组织和执行多个任务，从而提高计算机的利用率。这种思想创造了早期的操作系统——IBSYS 批处理系统的出现。<br>批处理系统最大的好处就是减少了人工操作的时间。假设我有两个 FORTRAN 程序、一个 COBOL 程序，我希望在 IBM 704 上编译它们为汇编代码。我会先将这些程序按类型分成两个批次，以免重复加载程序。在没有批处理系统的情况下，我必须逐步完成以下过程：<br>
<br>人工加载 FORTRAN 编译器，编译这两个 FORTRAN 程序后卸载。
<br>人工加载 COBOL 编译器，编译一个 COBAL 程序后卸载。
<br>而有了批处理系统，相同类型的任务可以按批次自动执行，一批次执行完毕后机器自动执行二批次任务。由于这时仍需手动提交打孔卡或磁带，所以操作员需要提交任务前把相同类型的任务整理在一起再提交给批处理系统，以减少相同程序的重复加载：<br>
<br>人按一定格式提交打孔卡或磁带，之后机器自动调度任务。
<br>机器自动加载 FORTRAN 编译器，所有 FORTRAN 代码在同一批次中编译。
<br>卸载 FORTRAN 编译器并自动加载 COBOL 编译器。
<br>所有 COBOL 代码在同一批次中编译，同样机器自动处理整个流程；
<br>计算机自动调度任务，操作员无需手动干预每个步骤。<br><br>其实 IBM 704 本身并不支持批处理，后期 IBM 704 引入了 IBSYS，才使其能够执行批处理任务。即多个作业可以预先存入打孔卡/磁带中，计算机自动按顺序的执行程序；同时操作员无需手动干预，计算机自动地调度作业。<br>早期批处理系统仅仅是一段常驻内存的小型程序，称为监控程序 (Monitor)，大概只有 4KB 大小。最早的时候，每次机器开机后都要人工地加载 IBSYS 这样的监控程序永驻到内存上。如果你提交了一批任务，监控程序就会帮你逐个地加载并执行任务。当一个任务执行完毕，CPU 控制权就会重新回到监控程序上并调度下一个任务在 CPU 上运行。<br>常驻的监控程序通常由三部分组成：<br>
<br>控制卡解释器：决定系统应该加载哪个程序，例如 FORTRAN 编译器或汇编器。
<br>加载器：负责将程序或编译器加载到内存中，使其可以运行。
<br>设备驱动器：负责与输入/输出设备（如打孔卡机、磁带机、打印机）进行交互。使计算机能够自动地读取用户提交的数据并存储计算结果。节省了操作员手工操作的时间。
<br><img alt="resident_monitor.png" src="https://congzhi.wiki/congzhi's-os-series/pics/resident_monitor.png"><br>通过引入控制卡 (Control cards)，监控程序就能够知道需要运行什么程序。以下图为例，我们有这几类控制卡片：<br>
<br>$JOB：标记新作业的开始，通常包含作业的会计信息。
<br>$LOAD：指示系统加载程序或编译器到内存中。
<br>$END：标记当前作业的结束，用于清理资源。
<br>$FTN：加载 FORTRAN 编译器，用于编译 FORTRAN 源代码。
<br>$ASM：加载汇编器，将汇编代码转换为机器代码。
<br>$RUN：执行已编译的二进制目标程序。
<br><img alt="batch_control_cards.png" src="https://congzhi.wiki/congzhi's-os-series/pics/batch_control_cards.png"><br>许多系统会使用 $ 作为控制卡的标识。IBM 的的作业控制语言 (JCL) 使用 // 作为控制卡的标识。<br><br>即使请来当时世界上最优秀的操作员来操作计算机，人类仍然是要远慢于计算机的运算速度。所以当时的人们开发出批处理软件程序来让计算机自动地执行任务，期间不需要人类干预。这大大地提升了 CPU 的使用率。<br>然而，即便如此，CPU 仍然面临瓶颈。其主要原因在于机械 I/O 设备的速度限制。在这一时期，计算机每秒已经能够执行数千条指令，但最快的打孔卡阅读机 仍然只能以每分钟几十到几百张卡的速度读取数据，这使得 I/O 成为计算的主要瓶颈。CPU 的利用率可能仍然不到 10% 。<br>一个解决方案是研发更加快速的 I/O 。这个时代的人们但是短短几年后，晶体管的时代就全面的到来了，CPU 性能增加的速度甚至要快于 I/O 性能增加的速度，差距越来越大。<br><br>当时一个流行的解决方法就是在读卡器和 CPU 之间加入一层更快速的 IO 设备——磁带机。操作员使用专用设备（如IBM 729磁带机）将卡片批量转存到磁带，此过程是离线拷贝的，不占用主机资源。之后批处理监控程序全速读取磁带数据在 CPU 上面执行，同时另一台磁带机可以执行下一批任务的离线拷贝工作。（类似于 DMA，与 CPU 并行工作）<br><img alt="offline_io.png" src="https://congzhi.wiki/congzhi's-os-series/pics/offline_io.png"><br>通过配合使用多个磁带机，我们就已经可以让 CPU 处于一个还不错的忙碌状态了。但是如果作业运行的过程中需要使用 I/O 就会造成 CPU 的空闲。<br><br><br>Multiprogramming Batch Processing<br><a data-tooltip-position="top" aria-label="http://www.righto.com/2019/04/iconic-consoles-of-ibm-system360.html" rel="noopener nofollow" class="external-link" href="http://www.righto.com/2019/04/iconic-consoles-of-ibm-system360.html" target="_blank">Iconic consoles of the IBM System/360 mainframes, 55 years old</a><br>
批处理系统(Batch Operating System，BOS) 将CPU的利用率提升了一大截，用户可以将要执行的任务一次性全部打包交给机器。之后，用户不需直接操作计算机硬件就能提交作业。可以将BOS视作一位管理员，它负责接收所有用户提交的作业。BOS会将需求相似的作业分组成批次，然后批量发送到计算机上执行。作业完成后，BOS负责将结果返回给用户。<br>OS/360 是1964年IBM为其当时全新的System/360大型计算机开发的批处理操作系统它标志着企业级计算和批处理操作系统的早期发展。OS/360的直接后续版本包括System/370的操作系统，然后是MVS(Multiple Virtual Storage)，最后发展为现代的 z/OS。(最早的指令集架构思想)<br>
<br>IBM650(1954)：使用磁鼓来存储程序和数据，但用户仍然需要通过物理介质（如punchcard）来输入程序和数据。
<br><br><br>在BPOS中，机器允许多个用户同时将任务(jobs)加载进任务队列(job queue)中，这时，系统把需求相似的任务分成不同批次(batch)然后按顺序送入CPU中运行。在任务执行过程中，任务不仅仅需要使用CPU，还会访问磁盘或外设。在任务访问磁盘或I/O时，CPU会处于空闲(idle)状态。这样会无故浪费很多CPU资源。<br>试想，如果允许内存加载多个作业，我们可以通过特定策略来提高资源的利用率。这就是多道程序设计的由来。在BPOS中，作业需要一个一个按顺序完成。但是Multiprogramming OS允许同时存在多个作业处于活动状态。当一个任务等待I/O时，操作系统可以调度另一个任务使用CPU。这样，即使某些任务在等待I/O操作，CPU也能保持忙碌状态，从而大大提高了效率和吞吐量。<br>
<img alt="Pasted image 20240912161341.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240912161341.png"><br>
多道程序系统其实就是容许多”道“程序同时存在于内存中，通过调度算法来使CPU尽量忙碌，提高CPU的利用率。在多道程序系统中，我们需要理解下面的概念：<br>
<br>“道数”：内存中存放作业的数量。
<br>技术支撑：中断、通道
<br>优点：<br>
<br>资源利用率更高
<br>响应时间变短<br>
缺点
<br>复杂性的提升
<br>
<br><a data-tooltip-position="top" aria-label="https://wixette.github.io/8800-simulator/" rel="noopener nofollow" class="external-link" href="https://wixette.github.io/8800-simulator/" target="_blank">Altair 8800</a>：通过拨动面板上的开关来输入机器语言程序。<br>
<a data-tooltip-position="top" aria-label="https://www.gatesnotes.com/meet-bill/source-code/reader/microsoft-original-source-code" rel="noopener nofollow" class="external-link" href="https://www.gatesnotes.com/meet-bill/source-code/reader/microsoft-original-source-code" target="_blank">Celebrating 50 years of Microsoft | Bill Gates</a>
<br><br><br>
Multi-tasking is a logical extension of multiprogramming.
<br>（<br>
进程隔离的概念这时候开始出现？避免进程之间的信息泄露。而且确保在一个进程crash时其他进程仍然可以继续运行<br>最早的进程隔离出现在 1962 的 Atlas 超级计算机上<br>
）<br>多道程序设计是多任务系统的前身，多道程序设计解决了CPU利用率低下的问题，但毕竟人类需要直接与计算机交互。从而促使了多任务系统的诞生。<br>多任务操作系统是一种能够“同时”处理多个任务的操作系统。这种类型的操作系统通过高效地分配CPU时间和其他资源，使得用户几乎可以同时运行多个任务，这些任务共享共同的CPU、内存等资源。极大的提高了系统的响应时间和用户体验。<br><img alt="Pasted image 20240912162841.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240912162841.png"><br>Multitasking OS 也叫 time sharing systems (分时系统)。这种操作系统将CPU时间分成很小的片段，称为时间片(time slices) 或 时间量(quantum)，并将这些时间片轮流分配给各个任务，从而实现多任务的并发执行 。这种方法的关键在于时间片的长度通常非常短(通常是毫秒级别)，足够短到让用户无法感觉到任务之间的切换。多个任务或多个用户感觉都独占CPU和内存，而实际上他们是在分享CPU和内存。<br>公认的第一个分时操作系统是 1961 年 MIT 开发的CTSS(Compatible Time-Sharing System)。CTSS的开发标志着计算机技术的一个重要转折点，使得多个用户能够通过各自的终端同时使用同一台计。算机资源。这一概念彻底改变了计算机的使用方式，为后来的操作系统，如Multics(多路信息计算服务)，以及今天我们使用的现代多任务操作系统奠定了基础。<br>应当引入并发的概念。。。<br><br><br>在现实中，我们有三种不同类型的并发：进程的并发、线程的并发和I/O复用。前两者是最好理解的，我们比较了进程和线程的优劣，对于进程的并发来说，每个进程都有自己独立的虚拟空间和资源，隔离性高，可以提供更好的安全性和稳定性；而进程切换的开销使得进程并发可能会影响系统资源。<br>线程的并发则减少了进程切换带来的系统开销，线程切换时仅仅保留执行时所需要的少量资源，其他的资源都由进程进行管理。而且线程减少了进程间通信的使用。然而，线程之间互相影响增加了并发控制的复杂性，稳定性相对差一点。<br>I/O复用我们将在学习完I/O系统后单独开一个阶段学习。I/O复用指的是多个I/O操作”同时“被单个进程/线程”同时“所处理，而不需要为每个I/O操作创建一个独立的进程/线程。常见的I/O复用技术有select、poll和epoll<br>tcp1981<br>
TCP最早是包含IP头字段的<br>4004 -<br>
1972 - intel releases the first 8-bits microprocessor-8008 - successor - 8080 - 1974<br>
真空管（提供0/1的表示） - ENIAC 巨大的根本原因（真空管还会大量产热，这也是为什么早期计算机功耗如此之高且吸引虫子Bug的原因）<br>自从真空管发明出以来（30余年），变化很小（附图）<br>第一个晶体管（solid-state transistors）1940s于at&amp;t bell labs<br>transistor scale: 找不到“Intel Technology - Architecture All Access Transistor Technology Intel Technology [_PELtLdh87Y - 517x273 - 7m03s].png”。 <br>moore's law - doubling performance while reducing the power comsuption<br>
1995 ASCI Red - 1Tflop performance need 10,000 microprocessors and 500kW<br>
of power<br>
2005, just 10 years later, Intel XEON PHI 1Tflop with 80 CPUs （signle conputer motherboard and with 67W]]></description><link>https://congzhi.wiki/congzhi's-os-series/2.-evolution-of-operating-systems.html</link><guid isPermaLink="false">Congzhi's OS Series/2. Evolution of Operating Systems.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 23 Jul 2025 17:39:44 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/migration_of_os_concepts_and_features.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/migration_of_os_concepts_and_features.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[8. Threads and Concurrency]]></title><description><![CDATA[ 
 <br>第一遍重写中<br><br><br>
Thread is short for thread of execution.
<br><br>我们已经了解过了进程的概念，也简单地了解过了什么是并发。当前，消费级的 CPU 大多都是多核心的，在某一时刻，我们可以有 CPU 核心数个进程同时运行在 CPU 上。而在单核 CPU 的时代，我们只能通过系统分时在机器上模拟多个进程“同时”运行。我们再来回温一下什么是分时和并发。<br><br>要实现并发，系统就需要是分时的。分时就是系统将CPU时间分为一段一段的CPU时间片。通过快速的切换任务，当时间片越来越小时，在宏观上用户和程序就会就会感觉像是独占了CPU。在时间片小到人类不能察觉时，程序就在微观上交替执行，宏观上”同时“执行了，这就是并发。<br><img alt="Pasted image 20241203220443.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241203220443.jpg"><br><br>提到了并发，与之对应地，我们往往会想到并行的概念。并行我们很好理解，同时地做很多件事。初学时我常会将这两个概念搞混，因为在宏观上，它们提供的效果太类似了。但只要把视角转向微观，我们就会明白它们的不同。<br>并发是”同时“做多件事，但并行是同时做多件事。他们在宏观上好似都拥有同时，但在微观上只有并行是同时做的。并发关注结构，而并行关注执行，并发提供了解决问题的结构方法，可能支持并行化（多核），但也不必然（单核）。<br><img alt="threads_parallelism_concurrency.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/threads_parallelism_concurrency.jpg"><br>在上面的图示中，展示了两个核心上四个线程（你可以看成是进程）的并行并发执行。<br><br>并发为我们带来许多优点，我们现在可以在单处理器上允许多个程序，实现微观上虽然是交替执行，但宏观上”并行“执行的特点。这种特性提升了CPU的利用率，从而带给我们更好的性能表现。<br>缺点同样显而易见，调度器需要瞻前顾后，增加了系统调度的复杂性，多个进程可能会相互作用，互相争夺资源，所以避免并发线程导致 inconsistent states 是我们需要关心的一大问题。除此之外，频繁调度所产生的上下文切换开销也是我们要关心的。<br><br>假如我们有一个 HTTP 服务器，在没有线程的时候，每一个客户端的连接都将对应一个进程的创建。在高并发的场景下，每秒都可能有成百上千个客户端需要与服务器建立连接。在 HTTP 服务器上，可能出现频繁地创建、切换和销毁进程，这样带来的开销可能是服务器不可承受之重。更何况进程间通信也会为系统带来不小的开销。<br>那么，我们是否有更好地方法来降低系统开销，实现一种机制来避免进程操作给我们带来的系统开销呢？在剖析进程时，我们发现进程中有很多部件，而执行程序是在栈区中执行的。为了减少进程操作开销，人们将进程中负责执行程序的部分独立出来称为线程，而进程仅作为资源管理的单位为提供线程其所需要的资源。<br>从而，我们可以在服务器进程中，用多线程的方式同时服务多个客户端，减少了进程开销。<br><img alt="Pasted image 20241204003952.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204003952.jpg"><br>线程是执行线程的简写，它是一系列顺序任务流，这些任务流可被CPU调度。由于线程的调度和操作相比进程系统开销要小很多，所以现在的操作系统使用线程作为CPU调度的基本单位，即线程是最小的可调度单位。而进程，作为资源管理的基本单位，可以被看作是线程的容器。<br><br>为了让这个最小的可调度单位能够正常的工作，我们需要给线程分配必要的资源。每个线程都需要独立的 TCB ，包括寄存器组、PC寄存器和堆栈指针等。此外，还需要给每个线程分配栈空间以确保能够调度执行。这些线程会共享同一个进程中的代码段、数据段和文件等资源。由于线程的轻量，我们也称其为轻量级进程(Light-weight process)，这样的轻量型为减少系统开销帮了不少忙。<br><img alt="Pasted image 20241203215622.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241203215622.jpg"><br>由于线程所占用的资源很少，所以创建和销毁线程要比进程快得多（10×），而且上下文切换的时间也更快。又因为所有线程共享进程的资源，所以同一进程中线程间交流并不需要IPC。要使程序运行起来，一个进程就至少需要有一个线程，叫做主线程(main thread)。<br>此外，线程间共享进程资源还为我们带来另一个好处——高缓存命中率（关于缓存亲和性的内容将在<a data-tooltip-position="top" aria-label="9. CPU Scheduling > 第三课 Multiple-Processor Scheduling" data-href="9. CPU Scheduling#第三课 Multiple-Processor Scheduling" href="https://congzhi.wiki/congzhi's-os-series/9.-cpu-scheduling.html#第三课_Multiple-Processor_Scheduling" class="internal-link" target="_self" rel="noopener nofollow">CPU调度</a>阶段中学习到）。由于线程的轻量型，线程切换通常不需要将一些上下文重新加载到缓存中，所以缓存命中率高。<br><br><br>线程和进程一样有各种各样的状态，操作系统为了对线程进行管理和调度，在线程创建的时候会为线程创建一个TCB来存放线程执行相关的信息。TCB数据结构中的数据通常包括：<br>
<br>线程ID：用于唯一标识系统内的线程；
<br>线程状态：如运行、等待、就绪等；
<br>寄存器内容：保存线程的上下文信息，确保线程能够恢复执行；
<br>优先级：用于调度策略；
<br>线程特定的数据：每个线程私有的数据区；
<br>指向PCB的指针；
<br>指向不同内存区域的指针：text,data,heap and stack；
<br>资源信息指针。
<br>...
<br>为了使线程执行相独立，线程的栈空间和上下文信息是线程独享的，与其他线程相独立。虽然线程TCB中有指向资源的指针，但这些资源都是共享的，线程并不作为资源的管理者。<br>在Linux中，进程和线程都使用task_struct数据结构来描述它们的状态和信息。但同一进程内的线程间共享进程内的地址空间和资源。我们也有线程组的概念，同一进程内的主线程和其他线程组成一个线程组，组号tgid即为主线程的线程标识符。<br><br>和进程一样，每个独立的线程也都有自己的状态。我们之前介绍的进程模型有七种状态，线程也有自己的五态模型。由于线程并不是资源的调度单位，我们不用考虑线程在内存上的换入和换出，因此在线程模型中不会看到挂起态。线程的五态有：new、ready、running、waiting、terminate。<br>线程的状态和进程的状态息息相关，如果进程因等待I/O操作或其他资源而阻塞，那么所有线程也会进入阻塞状态。当进程处于就绪状态时，虽然它的线程已经准备好运行，但进程没有被调度到CPU上，因此所有线程暂时不能使用CPU资源。这就是进程被称为最基本的资源调度单位的原因。<br>如果所有的线程都被阻塞，而进程处在就绪态呢？由与进程的运行需要依托至少一个线程的可运行状态，所有即使进程能够获取资源也不可以执行任何任务。<br><br>线程上下文切换是操作系统从一个线程的执行状态切换到另一个线程的执行状态的过程。这个过程的过程和进程上下文切换的过程类似。与进程的上下文切换相比，线程的上下文信息更小，所以切换效率更高。<br><br>在同一进程下，由于线程之间共享进程资源，其上下文切换通常不涉及存储块的交换，所以线程切换的局部性更好（保留缓存内容，可能不需要刷新MMU）。<br><br>我们刚才比较同一进程内的线程切换，由于进程内的线程共享了大部分进程资源，因而开销相对较低。而跨进程的线程切换则不太一样了。由于跨进程的线程没有共享的进程资源，而且进程与进程之间的内存空间和安全环境的完全隔离，所以跨进程间的线程切换的系统开销要大得多。<br><br>在<a data-tooltip-position="top" aria-label="4. Interrupts and System Calls > 第七课 Interrupt Context" data-href="4. Interrupts and System Calls#第七课 Interrupt Context" href="https://congzhi.wiki/congzhi's-os-series/4.-interrupts-and-system-calls.html#第七课_Interrupt_Context" class="internal-link" target="_self" rel="noopener nofollow">阶段-5</a>，我们接触到了内核栈，我们用内核栈来存储中断上下文。那问题来了，进程的上下文 PCB 和线程的上下文 TCB 在哪里存储呢（task_struct in Linux）？它们在内核堆中存储。内核堆和用户进程的堆空间一样，是一种动态内核数据结构。<br>我们没有接触 CPU 调度的内容。简单起见，你需要理解——当线程切换时，内核需要负责保存当前线程的 TCB 并恢复调度线程的 TCB 。由于这一过程在内核中进行，所以硬件和系统会先将中断上下文保存到内核栈中。然后操作系统保存剩余的上下文（TCB）到内核堆，调度程序选择一个线程，恢复其 TCB 并通过 iret 返回用户态或内核态执行。<br><br>线程的这种轻量型可以带给我们很多好处，但也能够给程序的执行带去不少的烦恼。下面，我们将比较线程和进程在各个方面上的不同，然我们得以对两者有更好的理解。<br><br>在它们的创建方式上，我们讨论进程fork()的创建方式。在POSIX thread库中，线程通过pthread_create()来创建。由于进程是资源调度的基本单位，所以在进程创建的过程中，除了创建一个主线程之外，还要将父进程的所有资源映像拷贝到进程自己的内存空间中。相比之下，线程不需要进行资源的完整复制，而是共享同一进程的资源，因而线程的创建更小。<br>由于线程共享进程中的资源，因而线程的内存开销和切换开销要小很多。你可以将进程和线程理解为大石头和小石头，操作系统搬小石头肯定更加地容易。在数据的共享上，进程会使用繁琐的IPC机制，在同一进程内的线程不需要考虑这些，因为它们的资源是共享的。<br><br>事事都有其两面性，看过线程光鲜亮丽的一部分，我们接下来学习线程阴暗的一面。相比进程之间彼此隔离，由于线程间资源共享可能导致一系列问题。比如不当的调度导致的资源竞和死锁问题。我们说线程栈是独立的，但不像进程空间那样隔离。理论上，一个线程可能访问另一个线程的栈空间，这是非常危险的。可能导致程序的崩溃。<br><br>在前面，我们说多个线程可能属于一个进程，这些线程共享进程中的数据。这种数据共享是多线程编程带给我们的一个好处。但在某些情况下，每个线程可能需要自己的某些数据副本。我们将这种数据称为线程局部存储。<br>一个线程的TLS仅对这一个线程可见，这破坏了一定的共享性，但会带来很多好处（比如避免可能导致的资源竞争和死锁问题）。为了在线程的生存周期内可用，TLS通常声明为静态的。在C语言和C++中，可以使用 __thread (编译器提供)或 thread_local (C++11)来声明TLS变量。<br>如果我们想为一个线程分配一个唯一的标识符，我们可以这样声明：<br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;

__thread int threadID = 0;

void* printThreadID(void* arg) {
    threadID = (int)(long)arg;
    printf("Thread ID: %d\n", threadID);
    return NULL;
}

int main() {
    pthread_t t1, t2, t3;

    pthread_create(&amp;t1, NULL, printThreadID, (void*)(long)1);
    pthread_create(&amp;t2, NULL, printThreadID, (void*)(long)2);
    pthread_create(&amp;t3, NULL, printThreadID, (void*)(long)3);
    
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    pthread_join(t3, NULL);

    return 0;
}
<br>#include &lt;iostream&gt;
#include &lt;thread&gt;

thread_local int threadID = 0;

void printThreadID(int id) {
    threadID = id;
    std::cout &lt;&lt; "Thread ID: " &lt;&lt; threadID &lt;&lt; std::endl;
}

int main() {
    std::thread t1(printThreadID, 1);
    std::thread t2(printThreadID, 2);
    std::thread t3(printThreadID, 3);

    t1.join();
    t2.join();
    t3.join();

    return 0;
}
<br><br>现在，我们对线程也有了一定的理解。我们对比了进程和线程上下文切换开销。由于线程的轻量型，我们将线程作为 CPU 调度的基本单位，而进程仅作为资源管理的基本单位。<br>然而，线程究竟是内核所创建并进行调度的。在用户空间，我们所能够接触到的并不是我们所讲到的线程，而是另一种线程——用户级线程。内核调度的线程我们称为——内核级线程。<br><br>以上，我们简单地了解了内核级线程(Kernel-Level Threads) 和用户级线程(User-Level Threads) 。内核级线程是由操作系统直接管理的线程。而用户级线程是由用户态的线程库（如 POSIX Thread 库）所管理的，由于用户级线程的创建、切换和销毁都在用户态中进行。因此内核级线程并不知晓用户级线程的存在。<br><img alt="Pasted image 20241204011034.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204011034.jpg"><br>无论是内核级线程还是用户级线程都有自己的线程资源（栈、上下文信息等），但是内核级线程的 TCB 是被操作系统内核管理的，而用户级线程的 TCB 被用户空间的线程库程序所管理。<br><br>用户级线程是对内核级线程的模拟。在前面学习的线程中，我们说（内核级）线程是CPU基本的调度单位，这里的线程实际上就是内核级线程。因为内核的线程调度器是通过内核级线程的 TCB 对线程进行调度的。而在用户空间创建的用户级线程不会被操作系统所察觉，因而系统没有办法对用户级线程进行调度。<br>由于内核并不会记录 ULTs 的资源和上下文，所以 ULTs 并不能参与 CPU 的调度。ULTs 的运行建立在运行在 CPU 的 KLTs 之上。虽然看上去 ULTs 好像并不大方便高效，但这种线程事实上能够带给我们许多好处。<br>由于 ULTs 的操作都是在用户态进行的，所以 ULTs 切换时不需要考虑系统调用的开销。一般情况下，用户级线程的 TCB 也相较内核线程小得多，所以上下文切换的开销可以非常低。由此，ULTs 的创建、切换等操作并不需要内核的帮助，速度可以相较 KLTs 快很多。<br>但是这种线程的缺点也显而易见，多个 ULTs 可能运行在一个 KLT 上，没有实际上线程的并发，性能可能并不够好。如果一个 KLT 对应多个 ULTs，每个 ULT 可能只会获得  的线程性能。下面，我们将介绍三种用户级线程和内核级线程的设计模型：多对一模型、一对一模型、多对多模型。<br><br>多对一模型中，一个 KLT 上要支持多个 ULTs 的执行。我们前面提到过，ULTs 的切换开销很低，所以这种模型中的上下文切换非常快。但由于这种模型只有一个 KLT，这就意味着一时间在这些 ULTs 只能有一个在 CPU 核心上执行。这种模型在多核处理器上并不能发挥并行执行的优势。<br><img alt="Pasted image 20241204011053.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204011053.jpg"><br>而且，如果某个时刻其中一个ULT调用了阻塞的系统调用（使用I/O），那么唯一的那个内核级线程就会阻塞。进而，整个进程会被阻塞，剩下的那些用户级线程就也随之阻塞。即一个 ULT 的阻塞导致了所有 ULTs 的阻塞。<br><br>一对一模型是最简单的模型，即一个 ULT 都对应着一个 KLT ，每个 ULT 事实上变成了一个独立的调度单位（因为 ULT 的线程操作就对应着 KLT 的线程操作）。这种模型和我们学习的线程是对应的。使用一对一模型后，我们就不必担心一个 ULT 的阻塞导致整个进程阻塞的事件发生。而且，使用一对一模型后，这些 ULTs 可以在多处理器上并发地运行。<br><img alt="Pasted image 20241204011045.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204011045.jpg"><br>由于这种模型的一一对应，我们实际上失去了 ULTs 给我们带来的好处。不能再享受到多用户级线程带来的低开销线程上下文切换。意味着你每创建一个用户级线程，背后都对应着一个支持 ULT 的系统级线程，所有的调度都将在内核的调度器下完成。<br>虽然看上去有这么些缺点，但现在的许多操作系统仍然采用一对一的模型。一方面这种模型较好实现，在多核处理器的时代也能够有较好的性能来支持庞大数量的内核级线程数量。<br><br>在多对多模型上，M 个用户级线程被固定的 N 个内核级线程所支持。相比上两种模型，多对多模型更加灵活，性能看上去也更好。在多对多模型中，ULT 可以不再绑定到特定的 KLT 上了。我们可以在用户级的线程库中实现一些调度机制，比如当某个 KLT 被阻塞，我们可以让其余的 ULTs 可以迁移到其他的 KLTs 上，当然，这就意味着可怜的 cache 命中率。<br><img alt="Pasted image 20241204011100.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204011100.jpg"><br>历史上， M:N 模型曾在 FreeBSD5 上得到过支持，但由于调度的复杂性和维护成本低限制，在 FreeBSD7 默认转为 1:1 模型，并从 FreeBSD8 起完全弃用 M:N 模型。<br>而随着 Go 语言进入人们的视野，这一模型重获新生。Go 原生的 GPM (Goroutine, Processor, Machine) 调度架构，通过在用户空间低成本调度海量 Goroutines（也就是 ULTs），显著降低了线程创建和切换的开销，让 M:N 模型在高并发的场景下展现其优越性。详见<a data-href="GPM Model of Go" href="https://congzhi.wiki/let's-go/gpm-model-of-go.html" class="internal-link" target="_self" rel="noopener nofollow">GPM Model of Go</a>。<br><br><img alt="Pasted image 20241204011158.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241204011158.jpg"><br><br>我们已经了解了不同的线程模型，那么用户态线程库中创建的线程是如何变成可调度的内核线程的呢？对于多对多或两级线程模型的系统实现中，通常会引入一个中间层，称为轻量级进程(LWP)。对于用户线程库来说，LWP 相当于一个虚拟的处理器，应用程序可以在其上进行调度和运行。<br><br>每个 LWP 都会关联到一个内核线程，操作系统通过调度这些内核线程来控制它们在物理处理器上的执行。如果一个内核线程进入阻塞状态（例如等待 I/O 操作完成），相应的 LWP 也会阻塞，从而导致附加到该 LWP 的用户线程无法继续执行。<br>为了保证程序的高效运行，应用程序通常需要多个 LWP 。对于不同的应用，所需要的 LWP 数量也各不相同。下面我们举例 CPU bound 和 IO bound 的应用场景：<br>
<br>对于 CPU 密集型应用：假设一个在单处理器上运行的 CPU 密集型应用程序。在这种情况下，一次只能运行一个线程，因此一个 LWP 就足够了。
<br>对于 IO 密集型应用：可能需要多个 LWP。因为每个并发阻塞的系统调用都需要单独的 LWP。例如，假设一个程序发出了 5 个独立的文件读取请求，但系统只分配了 4 个 LWP，那么第 5 个请求必须等待某个 LWP 释放后才能继续。
<br><img alt="Pasted image 20250113014249.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250113014249.jpg"><br><br>用于用户线程库和内核之间通信的方案称为调度激活。其工作原理如下：内核为应用程序提供一组 LWP 作为虚拟处理器，用户线程库可以将用户线程调度到可用的虚拟处理器上运行。<br>如果有关键事件发生，内核会通过上行调用(Upcall) 来通知用户线程库。比如，当用户线程即将被阻塞时，上行调用就会被触发，通知应用用户线程库保存阻塞线程的状态，并调度其他线程运行。当阻塞事件结束时，内核再次触发上行调用，通知用户线程库程序恢复之前阻塞的线程。<br>上行调用由用户线程库的处理程序负责，且该处理程序必须运行在虚拟处理器（LWP）上。通过这种机制，调度激活实现了用户态与内核态的高效协作。<br>在一对一线程模型中，每个用户级线程直接对应一个内核级线程，因此不需要 LWP 作为中间层。在 1:1 模型中，用户级线程会直接映射到内核级线程上。内核负责调度所有的内核级线程，从而实现线程管理的简化和高效性。也就是说，1:1 模型中用户线程的 TCB 完全由内核负责创建和维护。<br><br>对于用户而言，我们没发直接创建一个内核级线程。我们能做的只有通过一些封装了系统调用的线程库来创建一个线程。这些封装接口大多运行在高级语言层面。常见的线程库有 POSIX threads,  Windows threads, Java threads 等。<br><br>POSIX threads(pthreads) 标准仅仅定义 API 接口的实现，并不规定底层的实现。区别于不同的系统实现，pthreads 创建的线程可以是用户线程也可以是内核线程。因为现代主流的系统都采用 1:1 模型，通过 pthreads 库创建的 TCB 完全由内核创建维护。所以 pthreads 实现的是内核线程。<br><br>Windows threads 明确设计了 1:1 的线程模型。每个 TCB 都存储在一个内核对象表里，所有调度都由内核控制，无用户级线程实现。<br><br>以上两个线程库都是平台相关的。即使 C++/Rust 等这类高级语言拥有自己的标准线程库，所编译后的代码仍然是平台相关的。而 Java threads 库是平台无关的。<br>由于 Java “一次编写，处处运行（Write Once, Run Anywhere）” 的设计理念， Java 会通过引入虚拟机（JVM）来实现对底层系统的封装。简单来说，Java 会将源程序编译为 Java 字节码，在运行字节码程序时，Java 会启动一个 JVM 来在不同操作系统上解释或编译运行。<br>所以宿主机上的线程是什么类型， Java thread API 提供的线程就是什么类型的。<br><br><br>POSIX 线程库提供了许多系统调用用于管理线程和线程属性。我们下面一步一步的来介绍这些系统调用。本阶段，我们着重于学习线程管理和线程属性相关的 POSIX 线程库中的系统调用。<br><br>pthread 是 POSIX 标准线程的缩写，它的标准定义在 IEEE 1003.1c 中，规范了 UNIX 系统中的线程行为。这些规范促成了代码在不同平台上（类 Unix 系统）的可移植性。常用管理线程相关系统调用有：<br>#include &lt;pthread.h&gt;

pthread_create();           // Create a new thread
pthread_exit();             // Terminate the calling thread
pthread_join();             // Wait for a specific thread to exit
pthread_detach();           // Detach a thread
pthread_yield();            // Yield the processor to another thread
pthread_cancel();           // Send a cancellation request to a thread
pthread_testcancel();       // Test for pending cancellation requests
<br>常用的线程属性相关的系统调用有：<br>#include &lt;pthread.h&gt;

pthread_attr_init();        // Initialize thread attributes object
pthread_attr_destroy();     // Destroy thread attributes object
pthread_attr_setdetachstate(); // Set the detach state attribute
pthread_attr_getdetachstate(); // Get the detach state attribute
pthread_attr_setstacksize();   // Set the stack size attribute
pthread_attr_getstacksize();   // Get the stack size attribute
pthread_attr_setstackaddr();   // Set the stack address attribute
pthread_attr_getstackaddr();   // Get the stack address attribute
pthread_attr_setscope();       // Set the contention scope attribute
pthread_attr_getscope();       // Get the contention scope attribute
pthread_attr_setschedparam();  // Set the scheduling parameters attribute
pthread_attr_getschedparam();  // Get the scheduling parameters attribute
<br><br>我们先从管理 POSIX thread 的相关系统调用上学起。我们先来看线程创建的系统调用。<br><br>当要创建一个新的线程时，我们会用到pthread_create系统调用，其函数原型如下：<br>int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);
/* 
Parameters:
	1. thread: Pointer to a pthread_t variable that will hold the thread ID.
	2. attr: Pointer to a pthread_attr_t structure that specifies thread attributes (can be NULL for default attributes).
	3. start_routine: Function pointer to the function to be executed by the thread.
	4. arg: Argument to be passed to the start_routine function.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>这个函数有 4 个参数，需要接受一个 pthread_t 类型的线程类型变量、一个 pthread_attr_t 类型的线程属性、指向一个可调用对象的函数指针和需要传递的参数。本小节我们不需要关注第二个参数，线程属性是为了更小粒度的控制线程，我们留在下小节介绍。<br><br>在创建线程时，通过设置start_routine的函数指针，我们可以让创建好的线程去执行相关的start&nbsp;routine。在编写start_routine函数时，我们需要遵循以下规则：<br>void *(*start_routine) (void *)
/* Rules to obey:
Return value must be a (void*) pointer.
Parameter only can be one void* type pointer, can be a function pointer or else.
*/
<br>下面展示如何使用pthread_create创建一个线程并打印 "Hello from thread!\\n" ：<br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;

void* printHello(void* arg) { // This is a start routine
    printf("Hello from thread!\n");
    return NULL;
}

int main() {
    pthread_t thread; // To store the thread ID

    if (pthread_create(&amp;thread, NULL, printHello, NULL) != 0) {
        perror("Failed to create thread");
        return 1;
    }
    if (pthread_join(thread, NULL) != 0) {
        perror("Failed to join thread");
        return 1;
    }
    printf("Hello from main!\n");
    return 0;
}
<br>当调用 pthread_create 后，新的线程将会创建并开始执行 start_routine 参数所指向的函数。创建完成之后，我们可以用pthread_t类型的变量来操作特定的线程。通过pthread_t变量，我们可以进行线程的管理和控制，比如调用pthread_join等操作来等待线程结束。<br><br>当我们需要等待一个线程结束时，就会用到pthread_join()系统调用。pthread_join()会阻塞主线程（或其他线程）等待指定线程完成后再继续执行，它的函数原型如下：<br>int pthread_join(pthread_t thread, void **retval);
/* 
Parameters:
	1. thread: Thread ID of the thread to wait for.
	2. retval: Pointer to a location where the thread's return value will be stored (can be NULL if not needed).

Return value: Returns 0 on success, non-zero on failure.
*/
<br>为什么一个线程要等待另一个线程呢（主要是主线程等待子线程）？和我们前面学习过的进程 wait() 系统调用类似。主线程需要 pthread_join() 来读取返回值并回收子线程的资源。避免资源泄漏。<br>而且如果主线程没有等待子线程完成或者没有将子线程分离，主线程先行退出，那么所有未分离的子线程会被强制退出。这时，操作系统会强制性地回收子线程所占有的资源，可能导致数据完整性问题并带来同步问题。这里，你需要关心的问题是资源没有得到正确的释放。<br><br>线程有创建就有终止，当我们使用pthread_exit()系统调用时，线程就会终止执行并返回一个值给调用者。函数原型如下：<br>void pthread_exit(void *retval);
/* 
Parameters:
	1. retval: Pointer to the return value of the thread.

This function does not return.
*/
<br>pthread_exit()&nbsp;可以确保线程在退出时正确清理资源（pthread_cleanup_push），并将返回值传递给任何等待它的线程，例如通过&nbsp;pthread_join()&nbsp;函数等待的线程。<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

// Function to be executed by the thread
void* thread_function(void* arg) {
    int *ret_val = (int*)malloc(sizeof(int));
    *ret_val = 42; // Set the return value to 42
    pthread_exit((void*)ret_val); // Exit the thread and return the value
}

int main() {
    pthread_t thread;
    int result;
    void *retval;

    // Create a new thread
    result = pthread_create(&amp;thread, NULL, thread_function, NULL);
    if (result != 0) {
        // Handle error
        return -1;
    }

    // Wait for the specific thread to exit and get the return value
    result = pthread_join(thread, &amp;retval);
    if (result != 0) {
        // Handle error
        return -1;
    }

    // Print the return value
    printf("Thread returned value: %d\n", *(int*)retval);

    // Free the allocated memory
    free(retval);

    return 0;
}
<br><br>我们用pthread_detach&nbsp;系统调用将线程设置为分离状态，使线程结束时资源能被自动回收。要设置守护线程等后台线程就需要将线程设置为分离状态。它的函数原型如下：<br>int pthread_detach(pthread_t thread);
/* 
Parameters:
	1. thread: Thread ID of the thread to detach.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>我们前面提到，主线程的退出会导致所有未分离线程的强制退出。而当线程被设置为分离状态后，主线程的退出就不会再影响分离后线程的运行了，这时分离的子线程会在后台继续运行，并在完成后由操作系统自动释放资源（当所有子线程完成后，操作系统会终止整个进程并回收资源）。<br><br>当有需要让线程让出 CPU 给其他线程时，就会用到 pthread_yield 系统调用。需要注意的是，这个出让是系统层面上的。其函数原型如下：<br>int pthread_yield(void);
/* Explanation:
This function yields the processor to another thread.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>pthread_yield()会让线程主动放弃 CPU 。当线程调用pthread_yield()后，它将处于就绪状态，并让操作系统的调度程序选择运行其他就绪线程。一旦其他线程完成或被调度程序切换，原线程可以重新获得CPU时间并继续执行。<br><br>在前面，我们学习了线程会共享进程的地址空间、进程的一系列资源。但是要使得线程能够正常运行，线程还需要拥有自己独立的 TCB、寄存器组和栈空间。前面的两个由操作系统帮我们管理，作为程序员，你可以在创建子线程时规定一些线程的属性信息。包括：<br>
<br>栈大小(stack size)：定义线程的栈大小空间，确保线程运行过程中不会发生栈溢出。你可以通过pthread_attr_setstacksize函数设置栈大小。
<br>调度参数(scheduling parameters)：设置线程的优先级，以决定线程的执行顺序。可以通过pthread_attr_setschedparam函数设置调度参数。
<br>线程状态(thread state)：可以设置线程是分离状态还是可连接状态。分离状态的线程在终止后会自动释放资源，而可连接状态的线程需要通过pthread_join来回收资源。可以通过pthread_attr_setdetachstate函数设置线程状态。
<br><br>在上小结 pthread_create() 中，我们见到了存放线程属性的结构 thread_attr_t 。所有关于线程属性的系统调用都是围绕着这个结构所展开的。简化版的结构体原型如下：<br>typedef struct {
    int detachstate;          // Thread detach state (PTHREAD_CREATE_JOINABLE or PTHREAD_CREATE_DETACHED)
    int scope;                // Contention scope (PTHREAD_SCOPE_SYSTEM or PTHREAD_SCOPE_PROCESS)
    size_t stacksize;         // Thread stack size
    void *stackaddr;          // Thread stack address
    
    struct sched_param schedparam; // Scheduling parameters (priority, etc.)
    // Other fields specific to the implementation
} pthread_attr_t;
<br>下面是一个例子：<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

// Thread start routine
void* startRoutine(void* arg) {
    // Thread work here
    return NULL;
}

int main() {
    pthread_attr_t attr;
    pthread_t thread;
    int result;

    // Initialize the thread attributes object
    pthread_attr_init(&amp;attr);

    // Specific settings for thread attributes
    // Example: set the detach state
    pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE);

    // Create a thread with the specified attributes
    result = pthread_create(&amp;thread, &amp;attr, startRoutine, NULL);
    if (result != 0) {
        perror("Failed to create thread");
        return 1;
    }

    /*
    Do something here
    */

    // Wait for the thread to terminate
    result = pthread_join(thread, NULL);
    if (result != 0) {
        perror("Failed to join thread");
        return 1;
    }
    // Destroy the thread attributes object
    pthread_attr_destroy(&amp;attr);

    return 0;
}
<br><br>pthread_attr_init函数用于初始化一个线程属性对象，使其具有默认属性。初始化为默认属性有许多好处，我们希望避免未初始化属性导致的不确定行为，减少潜在的错误，提高查询的可靠性。pthread_attr_init系统调用完成后，我们就可以设置特定化一些的线程属性。<br>系统调用的原型如下：<br>int pthread_attr_init(pthread_attr_t *attr);
/* 
Parameters:
	1. attr: Pointer to a pthread_attr_t structure to be initialized.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>默认属性一般如下：<br>Thread attributes: 
	- Detach state = PTHREAD_CREATE_JOINABLE 
	- Scope = PTHREAD_SCOPE_SYSTEM 
	- Inherit scheduler = PTHREAD_INHERIT_SCHED 
	- Scheduling policy = SCHED_OTHER 
	- Scheduling priority = 0 
	- Guard size = 4096 bytes 
	- Stack address = 0x40196000 
	- Stack size = 0x201000 bytes
<br><br>pthread_attr_destroy&nbsp;用于销毁一个线程属性对象并释放其占用的资源。函数原型如下：<br>int pthread_attr_destroy(pthread_attr_t *attr);
/* 
Parameters:
	1. attr: Pointer to a pthread_attr_t structure to be destroyed.

Return value: Returns 0 on success, non-zero on failure.
*/
<br><br>#include &lt;pthread.h&gt;

int pthread_attr_setdetachstate(pthread_attr_t *attr, int detachstate);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. detachstate: Detach state to be set (PTHREAD_CREATE_JOINABLE or PTHREAD_CREATE_DETACHED).

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_getdetachstate(const pthread_attr_t *attr, int *detachstate);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. detachstate: Pointer to an integer where the detach state will be stored.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. stacksize: Stack size to be set.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_getstacksize(const pthread_attr_t *attr, size_t *stacksize);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. stacksize: Pointer to a size_t where the stack size will be stored.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_setstackaddr(pthread_attr_t *attr, void *stackaddr);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. stackaddr: Stack address to be set.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_getstackaddr(const pthread_attr_t *attr, void **stackaddr);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. stackaddr: Pointer to a void* where the stack address will be stored.

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_setscope(pthread_attr_t *attr, int scope);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. scope: Contention scope to be set (PTHREAD_SCOPE_SYSTEM or PTHREAD_SCOPE_PROCESS).

Return value: Returns 0 on success, non-zero on failure.
*/
<br>int pthread_attr_getscope(const pthread_attr_t *attr, int *scope);
/* 
Parameters:
	1. attr: Pointer to the thread attributes object.
	2. scope: Pointer to an integer where the contention scope will be stored.

Return value: Returns 0 on success, non-zero on failure.
*/
<br><br>线程取消是一种机制，允许线程在完成其工作之前被另一个线程终止掉。其中，我们将要取消的线程叫做 target 。要取消某一线程，我们需要先用 pthread_cancel 发送取消请求给目标线程。之后，目标线程一般会在取消点检查取消请求，检查到取消请求后终止线程。（延迟取消）<br>线程可以设置自己的取消状态和取消类型，来决定如何相应取消请求。取消类型有：<br>
<br>异步取消(Asynchronous Cancellation)：线程可以随时被取消。（风险较大）
<br>延迟取消(Deferred Cancellation)：线程在到达取消点时检查取消请求。
<br><br>线程可以使用&nbsp;pthread_setcanceltype&nbsp;系统调用来设置自己的取消类型<br>int pthread_setcanceltype(int type, int *oldtype);
/* 
Parameters:
	1. type: Specifies the new cancelability type for the thread. It can be one of the following:
	   - PTHREAD_CANCEL_DEFERRED: The thread will respond to cancellation requests at cancellation points (default).
	   - PTHREAD_CANCEL_ASYNCHRONOUS: The thread will respond to cancellation requests immediately.
	2. oldtype: Pointer to an integer where the previous cancelability type will be stored. Can be NULL if the previous type is not needed.

Return value: Returns 0 on success, non-zero on failure.
*/
<br><br>我们用&nbsp;pthread_cancel&nbsp;系统调用取消一个正在运行的线程。与 pthread_exit 不同的是， pthread_cancel 通常是其他线程调用。<br>int pthread_cancel(pthread_t thread);
/* 
Parameters:
	1. thread: Thread ID of the thread to be canceled.

Return value: Returns 0 on success, non-zero on failure.
*/
<br><br>pthread_testcancel&nbsp;系统调用可以在调用线程中创建一个取消点，使线程能够响应取消请求。<br>void pthread_testcancel(void);
/* 
Explanation: This function creates a cancellation point in the calling thread. If a cancellation request is pending, the thread will be canceled.

This function does not return a value.
*/
<br><br>我们常常使用pthread_testcancel作为取消点来检查是否有取消请求，当检测到请求时，目标线程就会终止线程。然而，pthread 中的库函数还会作为潜在取消点(Potential&nbsp;Cancellation&nbsp;Points)。<br>潜在取消点指的是线程在执行这些操作时，可以检查并响应取消请求的地方。这些点通常是在系统调用或库函数内部，它们也会检查是否有取消请求，以确保线程能够及时响应取消请求。常见的潜在取消点有：<br>
<br>pthread_join：等待线程终止。如果在等待过程中收到取消请求，当前线程会响应并退出。
<br>pthread_testcancel：显式检查取消请求的位置。插入此调用可以设置明确的取消点。
<br>pthread_cond_wait：等待条件变量。如果在等待过程中收到取消请求，线程会响应取消。
<br>read&nbsp;和&nbsp;write：许多I/O操作，如文件读写，也会作为取消点。
<br>sleep：休眠函数在终止时也会检查取消请求。
<br><br>我们主要使用 pthread_cancel 系统调用来发送取消请求，这是 POSIX 标准中提供的线程取消机制。通过 pthread_cancel ，可以发送取消请求并使目标线程在取消点检查和相应取消请求。下面是一个简单的例子：<br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;

void* startRoutine(void* arg) {
    while (1) {
        printf("Thread running\n");
        pthread_testcancel(); // Set a cancellation point, check for cancel request
        sleep(1);
    }
    pthread_exit(NULL);
}

int main() {
    pthread_t thread;

    pthread_create(&amp;thread, NULL, startRoutine, NULL);
    sleep(3); // Blocking for 3s before send cancellation.
    pthread_cancel(thread); // Send cancellation.
    pthread_join(thread, NULL);
    printf("Main thread ends\n");
    return 0;
}
<br><br>如果取消线程的时候线程仍然占有资源怎么办？为了避免资源泄漏，我们可以用下面的系统调用来设置一个 cleanup 句柄。确保每次线程取消时都会调用 cleanup&nbsp;routine 来清理线程，释放资源。<br>#include &lt;pthread.h&gt;

// Register cleanup handler with argument.
void pthread_cleanup_push(void (*routine)(void*), void *argument);
/*
1. routine: Pointer to the cleanup handler function.
2. argument: Argument to be passed to the cleanup handler function.
*/
<br>// Run if execute is non-zero.
void pthread_cleanup_pop(int execute);
/*
1. execute: If non-zero, the cleanup handler is executed.
*/
<br>注意，pthread_cleanup_push&nbsp;和&nbsp;pthread_cleanup_pop&nbsp;必须在同一作用域下成对使用。如果使用了一个，就必须使用另一个。如果没有一个，那么两个都不要出现。<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;

void cleanupHandler(void *array) {
	void** a = (void**) array;
	if(*a != NULL){
	    free(*a);
	    printf("Array cleaned up.\n");
	}
}

void* startRoutine(void* arg) {
    int* array = (int*)malloc(sizeof(int) * 10); // 先分配内存
    pthread_cleanup_push(cleanupHandler, (void*)&amp;array); 

    while (1) {
        printf("Thread running\n");
        pthread_testcancel();
        sleep(1);
    }

    pthread_cleanup_pop(1); // To pair
    pthread_exit(NULL);
}

int main() {
    pthread_t thread;
    pthread_create(&amp;thread, NULL, startRoutine, NULL);
    sleep(3);
    pthread_cancel(thread);
    pthread_join(thread, NULL);
    printf("Main thread ends\n");
    return 0;
}
<br>void** a = (void**) array;的必要性：使用(void**)提供了指针的间接访问，通过传递&nbsp;void*&nbsp;参数到清理函数，如果需要修改或检查传入的内存指针指向的内容，可以将其转换为&nbsp;void**。通过类型转换确保指针类型匹配，使得我们能够访问并释放原指针指向的内存。<br>如果cancellation在pthread_cleanup_push(cleanupHandler, array);之前就发生了怎么办？我们可以用下面的方法：<br>int* array = NULL;
pthread_cleanup_push(cleanupHandler, &amp;array); 
array = (int*)malloc(sizeof(int) * 10);
<br><br><br><br>在上一个阶段的结尾，我们简要讨论了多核处理器的调度问题。对于大多数系统来说，多核处理器和优化调度确实能够显著加快系统的运行速度。想象一下，如果有100个人在排队吃面条，显然两家面馆一起营业的效率会比只有一家面馆要高一倍。<br><br><br><br>This depends on the nature of the task (n&lt;core number and in an ideal scenario)<br>Fully parallelized: n*Threads = n*Speed<br>
Partitally parallelized: n*Threads = N*Speed (1&lt;N&lt;n)<br>
Cannot be parallelized: n*Threads = 1*Speed<br><br>阿姆达尔定律(Amdahl's&nbsp;Law) 是计算机科学中的一个公式，用来预测系统在添加多个处理器后的最大可能加速比。阿姆达尔定律表达了一个程序中可以被并行化的部分和不能被并行化的部分，以及在添加更多处理器后系统性能的提升受限于那部分不可并行的计算。<br><br><br>
<br>&nbsp;是使用&nbsp;n&nbsp;个处理器时的加速比。
<br> 是可以并行化的程序部分比例。
<br>&nbsp;是不能并行化的程序部分比例。
<br>&nbsp;是处理器数量。
<br><br><br><br><br>上述课程的学习完毕后，我们应当对线程有了一定了解了。简单来说，线程就是进程的执行流，执行流是什么我们马上会介绍。还记得我们进程番外篇学习的 IPC 机制么？通过IPC，我们创建多个进程共同解决一个问题，但这样做除了资源的浪费，还不得不考虑进程间通信带来的开销。但是引入线程后，上述两个我们最关系的问题迎刃而解。<br>在多核处理器的背景下，创建多个线程的好处是显而易见的——节省资源。线程虽然有独立的TCB，但是线程没有独立的进程虚拟地址空间。这就为线程带来很多相对进程而言的优点，这也是为什么线程能够打赢进程。当项目的代码量很大的时候，fork()创建的子进程的代码区会浪费很大一部分珍贵的内存空间，本质上还是父子进程共享”同一段“代码。但使用线程后，双引号就可以去掉了。<br>而且相比进程，线程为我们带来的优点有：<br>
<br>对线程的操作更快（创建、切换、销毁等）；
<br>TCB 更轻量；
<br>线程间数据直接共享，免去繁琐的 IPC 操作。<br>
但是有优点就会有缺点，比如：
<br>因为进程之间彼此隔离，因而进程的稳定性更好。
<br><br>在理解栈空间之前，我们可以先去看看进程代码是如何执行的——详见 《<a data-href="进程的一生——从出生到死亡 (Abandoned)" href="https://congzhi.wiki/some-notes/进程的一生——从出生到死亡-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">进程的一生——从出生到死亡 (Abandoned)</a>》。知道了进程的执行逻辑之后，我们会注意到进程虚拟空间中 “栈” 这个名词的分量。栈是一个很重要的概念，代码的功能是在函数中执行的，而函数的执行依赖在栈空间中创造的一个个栈帧来实现。因此程序的执行和栈空间密不可分，即栈空间就是独立的运行上下文。<br>当一个进程拥有多个线程时，每个线程共享代码段、数据段等资源。每个线程创建的时候操作系统会为这个线程分配单独的栈空间资源。比如，一个线程的栈空间从A到B，另一个线程的栈空间从B到C（A&lt;B&lt;C），以此类推。每个线程的代码在各自的栈空间内运行。<br>我们可以用下面的代码来进行线程栈空间的初始化。<br>#include &lt;thread&gt;
#include &lt;iostream&gt;

void m_Func(){
}

int main(){
    std::size_t stack_size = 1024*1024; // 1MB
    std::thread t(std::thread(func), std::move(stack_size));
    t.join();
    return 0;
}
<br>我们说线程之间数据共享，其实不仅仅是指数据段中的数据。理论上如果知道其他线程栈中局部变量在栈帧中的位置，也可以对这些数据进行操作。因此，线程中的资源是高度共享的。如果线程甲创建的栈帧覆盖线程乙的栈空间，就有可能导致进程的终止。（Threads share all segments except the stack, but a thread can still access the stack of another thread.）<br><br>在当下的日常生活中，无论是手机、电脑、工作站或是服务器都采用多核处理器架构。这是因为相比于执着地将单核登峰造极（在单核心上堆料），多加一个核心性价比要来的更好。由此，多核处理器成为了绝对的主流。但核心也不是越多越好的，要发挥多核处理器的性能优势，不仅仅需要操作系统合理的调度，同样也需要我们开发人员编写多线程的程序以供操作系统调度。<br><br>在Windows操作系统中，线程是进程的基本执行单元。每个进程可以包含一个或多个线程，这些线程共享进程的资源（如内存空间、文件句柄等）。当用户创建一个线程时，Windows会在内核中创建一个对应的内核级线程。Windows采用我们前面所说的1:1线程模型，即每个用户级线程对应一个内核级线程。这种模型的优点是线程管理和调度由操作系统内核负责，简化了开发者的工作。<br>在Windows中，同一台主机上，不同进程中的线程ID可能会重复，因为线程ID在进程内是唯一的。<br><br>在Linux操作系统中，线程和进程的概念则更加模糊。Linux使用轻量级进程(Lightweight Process, LWP)来实现线程，每个线程在内核中都是一个task。Linux通过pthread_create()或clone()系统调用创建线程，clone()允许创建一个共享资源的task（线程）。每个task都有一个唯一的struct task_struct数据结构，用于管理和调度。与Windows不同，Linux的线程模型更灵活，可以通过clone()的参数指定共享哪些资源。<br>在Linux中，所有属于同一进程的线程共享相同的线程组ID（TGID），这个TGID实际上就是进程的PID。而由于进程和线程都是为task，每个任务都有唯一的TID。所以同一台主机上的所有的线程ID都是唯一的。<br><br>C++11的多线程库提供了一组标准的API，用于创建和管理线程、同步线程操作等。这些API包括std::thread、std::mutex、std::condition_variable等。由于这些API是基于pthread标准设计的，因此它们在不同操作系统上的实现是相似的。<br><br>Windows操作系统本身提供了丰富的线程API，如CreateThread、WaitForSingleObject等。C++11的多线程库在Windows上实现时，底层会调用这些Windows API。由于Windows采用1:1线程模型，因此每个C++创建的线程都会对应一个内核级线程。<br>Linux操作系统主要使用pthread库来实现多线程。C++11的多线程库在Linux上实现时，底层调用了pthread API，如pthread_create、pthread_join等。Linux的线程模型基于轻量级进程（LWP），C++程序中创建的每个线程都会对应一个task_struct。<br><br>当程序加载进内存，内核会创建该程序的主线程，其 start routine 的入口在 main() 的开始。在 main() 这个主线程下，我们可以使用 thread 类创建多个子线程。过程如下：<br>#include &lt;iostream&gt;
#include &lt;thread&gt;
static int i = 0;
void hello() {
	i++;
	std::cout &lt;&lt; "Hello there! I am No." &lt;&lt; i &lt;&lt; std::endl;
}
int main() {

	std::thread thread_1(hello);
	std::thread thread_2(hello);
	std::thread thread_3(hello);
	std::thread thread_4(hello);
	std::thread thread_5(hello);
	std::thread thread_6(hello);
	std::thread thread_7(hello);
	std::thread thread_8(hello);
	std::thread thread_9(hello);
	std::thread thread_10(hello);

	return 0;
}
<br>通过thread类就可以创建线程类对象，我们需要给thread类的构造函数传递可调用对象的参数，这里使用函数作为参数。但这个程序会出现bug，这是因为：<br>
<br>主线程没有等待子线程而先行退出；
<br>并没有实现对共享变量i的互斥访问。
<br>因而会导致如下的问题出现：<br><img alt="Pasted image 20240905160146.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240905160146.png"><br>du@DVM:~/Desktop$ ./thread 
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.6
terminate called without an active exception
Hello there! I am No.7
Aborted (core dumped)
<br><br>程序执行对应着新进程的创建，在进程中，main函数就是主线程，而在主线程执行时，我们可以创建另外的线程，让这些线程并行独立执行。我们需要注意的是，如果主线程在子线程完成之前结束，程序会调用std::terminate，导致所有未完成的子线程被强制终止，从而引发abort()。<br><br>为了避免主线程先于子线程结束，我们用join()函数来阻塞主线程等待子线程结束后再返回。join()确保主线程在子线程结束后回收其资源，避免资源泄漏。<br>#include &lt;iostream&gt;
#include &lt;thread&gt;
static int i = 0;
void hello() {
	i++;
	std::cout &lt;&lt; "Hello there! I am No." &lt;&lt; i &lt;&lt; std::endl;
}
int main() {

	std::thread thread_1(hello);
	std::thread thread_2(hello);
	std::thread thread_3(hello);
	std::thread thread_4(hello);
	std::thread thread_5(hello);
	thread_1.join();
	thread_2.join();
	thread_3.join();
	thread_4.join();
	thread_5.join();
	std::cout &lt;&lt; "Main thread say byebye!" &lt;&lt; std::endl;
	return 0;
}
<br>运行结果如下：<br>Hello there! I am No.2
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Main thread say byebye!
<br>join()函数适用于需要确保子线程完成其任务后再继续主线程工作的情况。<br><br>另一种情况是我们想让子线程与主线程分离，主线程从此无法控制子线程，子线程被C++的运行库接管。主进程不需要等待子线程结束在退出，运行库会在这些子线程运行结束后自动清理资源。由此，detach()函数适用于不需要等待子线程完成的任务，例如后台任务或守护线程。<br>#include &lt;iostream&gt;
#include &lt;thread&gt;
static int i = 0;
void hello() {
	i++;
	std::cout &lt;&lt; "Hello there! I am No." &lt;&lt; i &lt;&lt; std::endl;
}
int main() {

	std::thread thread_1(hello);
	std::thread thread_2(hello);
	std::thread thread_3(hello);
	std::thread thread_4(hello);
	std::thread thread_5(hello);
	thread_1.detach();
	thread_2.detach();
	thread_3.detach();
	thread_4.detach();
	thread_5.detach();
	std::cout &lt;&lt; "Main thread say byebye!" &lt;&lt; std::endl;
	return 0;
}
<br>运行结果如下：<br>Main thread say byebye!
Hello there! I am No.1
<br>我们看到主线程早早就结束了，有的子线程甚至还来不及在屏幕上向我们打招呼。<br><br>有三种方式在在子线程中传递参数：1. 值传递；2. 引用传递；3. 指针传递。一般来说使用 detach() 函数时尽量不要传递指针，还不要使用隐式类型转换。<br><br>普通类型在传递子线程函数参数时，我们可以直接使用值传递。当我们使用值传递时，函数收到的是变量的副本，也就是说，函数内部的变量和元素的变量是两个独立的变量了，修改函数内部的变量并不会影响到原始的变量。所以，当使用值传递时，主线程可以放心的退出。<br><br>在C++的 std::thread 中，直接传递引用参数需要使用 std::ref 来包装引用，否则 std::thread 会尝试复制参数，这会导致编译错误或未定义行为。示例如下：<br>#include &lt;iostream&gt;
#include &lt;thread&gt;

void hello(int&amp; i) {
//void hello(const int i){
    i++;
    std::cout &lt;&lt; "Hello there! I am No." &lt;&lt; i &lt;&lt; std::endl;
}

int main() {
    int i = 0;

    std::thread thread_1(hello, std::ref(i));
//  std::thread thread_1(hello, i);
    thread_1.join(); 

    std::cout &lt;&lt; "Main thread: i = " &lt;&lt; i &lt;&lt; std::endl;
    return 0;
}
<br>使用普通的引用传递会调用一次复制构造函数，导致函数无法对引用对象进行修改，于是我们有std::ref，它可以使子线程在传递参数时不再调用复制构造函数。<br><br>每个函数的调用和返回都是伴随着栈内存中栈帧的创建和销毁。如果函数内申请了一段堆内存空间，我们就需要在函数返回之前（栈帧销毁前）将这段堆内存给释放掉，因为栈内存是堆内存的唯一寻址方式。所以，当主线程退出而子线程仍在运行且访问传递的指针时，就可能会导致指针悬挂的问题（因为内存已经释放掉了）。<br>再者，多个线程同时访问指针指向的数据。期间若是涉及到了数据的写操作，我们还不得不考虑数据一致性的问题，可能需要额外的同步机制来确保数据的安全问题。<br>在现代C++中，我们可以使用智能指针来避免内存释放的问题，但是数据一致性还是我们需要考虑的。下面是使用std::shared_ptr的示例，即使主线程退出，我们仍然可以正确管理内存。<br>#include &lt;iostream&gt;
#include &lt;thread&gt;
#include &lt;memory&gt;
#include &lt;chrono&gt;

void hello(std::shared_ptr&lt;int&gt; ptr) {
    (*ptr)++;
    std::cout &lt;&lt; "Hello there! I am No." &lt;&lt; *ptr &lt;&lt; std::endl;
}

int main() {
    auto ptr = std::make_shared&lt;int&gt;(0);

    std::thread thread_1(hello, ptr);
    std::thread thread_2(hello, ptr);
    std::thread thread_3(hello, ptr);
    std::thread thread_4(hello, ptr);
    std::thread thread_5(hello, ptr);

    thread_1.detach();
    thread_2.detach();
    thread_3.detach();
    thread_4.detach();
    thread_5.detach();

    std::this_thread::sleep_for(std::chrono::seconds(2)); // 确保子线程有时间完成

    std::cout &lt;&lt; "Main thread say byebye!" &lt;&lt; std::endl;
    return 0;
}
<br>运行结果如下：<br>Hello there! I am No.2
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Main thread say byebye!
<br><br>你可能已经注意到，我们上面代码的运行结果并不符合我们所想象的那样。虽然我们用join()确保子线程运行完成，但是我们仍然看到这种情况：<br>Hello there! I am No.2
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Hello there! I am No.5
Main thread say byebye!
<br>这是由于操作系统的线程调度机制和竞态条件导致的。线程的执行顺序由操作系统的调度器决定，可能导致输出顺序的不确定性。为了解决这种问题，我们可以使用一些同步机制，如互斥锁（mutex）、条件变量(condition&nbsp;variable) 和 原子操作(atomic operation) 等。我们将在后面的阶段介绍这些技术。<br><br><br>
协程本身并不作为操作系统中的内容。协程是用户级的并发编程模型。这里仅作为补充。协程并不算新鲜词，Melvin Conway 在 1958 年就提出了协程的概念，并应用于汇编程序中。
<br>协程并不算是操作系统的内容，因为它是在用户态实现的。引入协程的概念就是为了避免线程切换的开销，作为更小型的并发控制流，协程随即应运而生。因为协程是用户级别的并发编程模型，所以协程切换时不需要从用户态陷入内核态，系统内核对协程的存在也一无所知。<br>在本阶段，我们提到了内核调度的最小单位是内核级的线程。现代的操作系统中，一个用户级线程往往对应着一个内核级线程，当我们在用户空间创建一个用户空间线程之后，我们可以说我们创建了一个内核可以调用的真•线程。而协程往往在一个线程中运行，所以协程并不提高程序的性能。<br>协程表面上看与用户级线程很相似，但在本质的实现上是不同的。用户级线程是通过线程库（如 POSIX Threads）在用户态提供的一种线程机制，依靠用户线程库完成调度。而协程是一种更轻量级的编程抽象，可以由高级语言的运行时系统（如 C++、Go 等）或用户自定义逻辑来实现。<br><br>协程和函数很类似，你可以说协程就是一个能暂停并继续执行的函数。当函数中出现co_await、co_yield或co_return的其中一项时，该函数就可被看作为一个协程。下图展示了函数和协程的调用过程。当函数被调用时，函数就会开始执行，直到遇到return或到达函数末尾。而协程可以执行一部分程序后挂起，等待再次调度。协程实现了在单个线程内的并发执行。<br><img alt="Pasted image 20250112233950.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250112233950.png"><br>
由于协程在用户空间实现并发，我们可以控制代码的执行顺序，从而减少多任务调度造成的竞争问题（特别是cooperative multitasking）。由于协程在单个线程内运行，并且可以在执行过程中暂停和恢复，因此我们可以避免传统多线程编程中常见的竞争条件和锁定问题。在某些情况下，仍然需要使用同步机制来确保一致性。<br><br><br>为了实现协程从挂起恢复后继续运行，协程也需要保存上下文状态信息，这些上下文状态信息被存放在coroutine frame中。根据coroutine frame的实现方式，我们有两类协程：stackfull coroutine和stackless coroutine（C++20 使用的是 stackless coroutine）。Stackfull coroutine会将coroutine的数据和c-routine frame存放在stack中，而stackless coroutine的实现方式则会把coroutine存放在堆上。<br>这些上下文信息会有编译器代为我们管理。当协程的生命周期结束时，协程帧就会被销毁。<br><br>在C++中，我们有三个关键字：co_await、co_yield和co_return。这三个关键字得以让协程与外界进行交互。协程可以被暂停/挂起和恢复执行。其中，co_await和co_yield可以挂起协程，而co_return用于返回。它们有不同的行为，协程状态与这三个关键字也息息相关。<br><br>包含协程三个关键字之一的函数就可以被看作是一个协程。但需要注意的是，在C++中，当我们创建协程时，协程的返回类型必须是一个特定的类型，该类型需要包含一个名为promise_type的嵌套类型（和std::promise没关系）。所以下面返回类型为int的协程代码在编译时就会报错：<br>int foo() {std::suspend_always{};}
<br>promise_type是一个类对象（struct或class，而且必须名为promise_type），定义并控制协程的行为和生命周期管理。返回值类型就是对这个promise_type的包装，如下：<br>struct co_return {
    struct promise_type {
		// Something needs to be done here...
    };
};
co_return foo() {}
<br><br>在promise_type中，需要至少包含以下的方法get_return_object、initial_suspend、final_suspend、return_void（或return_value）和unhandled_exception。我们将一步一步的对这五个方法进行说明。最小的协程返回对象如下：<br>struct co_return {
    struct promise_type {
        co_return get_return_object() { return {}; }
        std::suspend_never initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void return_void() {}
        void unhandled_exception() { std::terminate(); }
    };
};
<br><br>这是最先开始执行的方法，get_return_object方法构造协程的返回类型（这里是co_return）并返回promise_type的父类型，也就是协程的返回类型。这里的return {};表示返回默认构造的co_return。<br><br>这个方法在协程开始执行之前被调用，返回一个可等待对象，决定协程是否在开始时挂起。这里，返回的可等待对象可以是std::suspend_never或std::suspend_always。我们这里使用前者，表示协程在开始时不会挂起，而是立即执行。return {};返回默认构造的std::suspend_never。<br>根据不同的启动方式，协程可以被分为Lazily started coroutines和eagerly started coroutines。我们例子中给出的是eagerly started coroutines，这些协程在创建时就会立即开始执行。如果我们在这里返回的可等待对象是std::suspend_always那么我们就会创建lazily started coroutine。<br><br>与 initial_suspend 类似，final_suspend 方法在协程结束时被调用，它同样返回一个可等待对象，决定协程是否在结束时挂起。这里我们使用 std::suspend_never 。 final_suspend 是一个 non-throwing method，这就是为什么通常使用 noexcept 关键字。<br><br>这两个方法用于处理协程的返回值。如果协程没有返回值，则使用return_void；如果协程返回值，则使用return_value。在例子中，我们不设置任何返回值，所以使用return_void。<br><br> unhandled_exception 方法用于协程中的异常处理，在协程中抛出未捕获的异常时被调用。在一些的例子中，我们不需要做任何的异常处理。例子中，我们使用 std::terminate() 来终止协程。<br><br>C++标准提供了两个常见的 awaiters，std::suspend_always 和 std::suspend_never。当使用 std::suspend_always 时，，协程会在遇到 co_await 时立即挂起。使用 std::suspend_never 时，协程会在遇到 co_await 时继续执行，不会挂起。<br><br><br>#include &lt;iostream&gt;
#include &lt;coroutine&gt;

struct co_return {
    struct promise_type {
        co_return get_return_object() { return {}; }
        std::suspend_never initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void return_void() {}
        void unhandled_exception() { std::terminate(); }
    };
};

co_return coroutine_foo() {
    std::cout &lt;&lt; "Hello ";
    co_await std::suspend_always{};
    std::cout &lt;&lt; "world!" &lt;&lt; std::endl;
}

int main() {
	co_return cofoo = coroutine_foo();
	cofoo();
    return 0;
}
<br><br>#include &lt;iostream&gt;
#include &lt;coroutine&gt;

struct co_return {
    struct promise_type {
        co_return get_return_object() { return co_return{std::coroutine_handle&lt;promise_type&gt;::form_promise(*this)}; }
        std::suspend_never initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void return_void() {}
        void unhandled_exception() { std::terminate(); }
    };
    std::coroutine_handle&lt;&gt; handle;
    co_return(std::coroutine_handle&lt;&gt; handle_): handle{handle_}{ }
    operator std::coroutine_handle&lt;promise_type&gt;() const {return handle_}
};

co_return coroutine_foo() {
    std::cout &lt;&lt; "1. Hello! \n";
    co_await std::suspend_always{};
    std::cout &lt;&lt; "2. Again! \n";
    co_awiat std::suspend_always{};
    std::cout &lt;&lt; "3. Another Hello! \n";
    co_awiat std::suspend_always{};
    std::cout &lt;&lt; "4. Another another Hello! \n";
}

int main() {
	co_return cofoo = coroutine_foo();
	cofoo.handle.resume();
	cofoo.handle(); // Calls std::coroutine_handle&lt;&gt;::operator();
	std::cout &lt;&lt; std::boolalpha &lt;&lt; cofoo.handle.done() &lt;&lt; std::endl; // Check if coroutine is executed.
	cofoo();
    return 0;
}
<br>学不明白。。。课程链接如下：<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/playlist?list=PL2EnPlznFzmhKDBfE0lqMAWyr74LZsFVY" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/playlist?list=PL2EnPlznFzmhKDBfE0lqMAWyr74LZsFVY" target="_blank">Coroutines</a>]]></description><link>https://congzhi.wiki/congzhi's-os-series/8.-threads-and-concurrency.html</link><guid isPermaLink="false">Congzhi's OS Series/8. Threads and Concurrency.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 12 Aug 2025 22:09:22 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241203220443.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241203220443.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[9. CPU Scheduling]]></title><description><![CDATA[ 
 <br>第一遍重写中<br><br><br>当今的现代操作系统都调度内核级的线程，但许多资料仍然使用如“作业调度”或“进程调度”这样的名词。在本阶段的内容中，不区分相关名词的具体含义。无论是作业调度、进程调度还是线程调度，都是指操作系统在不同时间段内分配CPU资源给不同任务的过程。<br><br>在OS中，调度(Scheduling) 是指管理和分配计算机处理器给不同进程的过程。调度通常分为：长程调度(Long-term scheduling)、中程调度(Medium-term scheduling)、短程调度(Short-term scheduling) 和 I/O调度(I/O scheduling)。<br><img alt="Pasted image 20240528012310.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240528012310.png"><br><br>长程调度，或称高级调度(high-level scheduling) 和 作业调度(job scheduling)，决定了哪些作业(jobs)进入系统（内存）并加入到就绪队列准备运行。涉及到进程从new state到ready state的转换。通常在批处理系统中使用，用于控制系统的负载和作业的流入速度。长程调度的主要目标是保证系统资源的合理利用，避免系统的过载。<br>我们使用的PC中，长程调度并不常见。作为用户，我们可以决定去运行哪个程序。有时候可能会有per-user的限制（比如 100 processes/user）。除了批处理，长程调度在服务器中也很常见。当服务器满载时，新加入的客户端可能会收到服务器的提示信息（比如游戏服务器的排队提醒）。<br>在Android等移动操作系统上，进程调度可能会非常激进。当系统资源紧张时，操作系统会优先保证前台应用的运行，而将后台进程杀掉以释放内存和CPU资源。这种机制被称为“杀后台”。<br><br>中程调度，或称中级调度(mid-level scheduling)，负责进程在内存和外存之间的调度，如换入和换出。这种调度用于内存管理，以便内存不足时将不活跃的进程暂时换出，从而腾出内存空间给其他进程使用。中程调度的主要目标是优化内存使用，提高系统的整体性能。<br>相比于长程调度和短程调度，中程调度的频率很低，因为I/O操作很费时间。甚至在有的操作系统中，当内存不足时会强制终止任务的执行。（Android）<br><br>短程调度，或称低级调度(low-level scheduling) 和 CPU调度(CPU scheduling)。长程调度/中程调度可能很久才发生一次，单短程调度可是时时刻刻都在发生。短程调度就相当于是“我们现在要做什么？”。阻塞、运行、预备态都是在短程调度中发生的。在短程调度中，调度器(dispatcher)会使用一些调度算法管理选择哪个进程可以占用CPU。<br>在多任务操作系统中，（短程）调度器负责不断地将进程放在CPU中执行，以确保系统的高效运行。调度程序选择下一个要在CPU中运行的进程后，调度器就会执行相关的上下文切换，然后跳转到适当的内存位置执行相应的指令。<br>
<img alt="Pasted image 20250112184625.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250112184625.png"><br>由调度器所消耗的时间成为调度延迟。这种延迟可能会变成系统的性能瓶颈，所以设计调度器时应当确保其时间复杂度尽可能的低。<br><br>详见I/O系统。<br><br>不同的CPU调度算法具有不同属性，一个特定算法的选择可能会对某一个或几个进程更有利。为了选择以应对特定情境的算法，我们需要考虑各个准则综合评估。这些准则包括：<br>
<br>CPU使用率(CPU utilization)<br>
CPU利用率是指CPU十几倍使用的事件百分比。对于一个实际系统，CPU利用率不宜太低，一般为40%（轻负载系统）到90%（重负载系统）。
<br>吞吐量(Throughput)<br>
吞吐量时系统一个单元时间内进程完成的数量。
<br>周转事件(Turnaround time)<br>
周转时间是进程从提交到完成的这段时间。周转时间是所有时间段之和，包括等待进入内存、就绪队列中的等待、CPU中执行和I/O执行。
<br>响应事件(Responce time)<br>
响应时间是指从提交请求到系统首次响应的时间(例如:当你进行键盘输入，从按下一个键到屏幕显示这个字符的时间)。对于交互式系统，低响应时间至关重要，因为它直接影响用户体验。调度策略应确保系统对用户请求的响应尽可能快。
<br>等待事件(Waiting time)<br>
等待时间是指一个进程在就绪队列中等待处理器分配所花费的时间。调度策略应尽量减少进程的等待时间，从而提高系统的整体性能。
<br>
<br>公平性(Fairness)<br>
公平性是指调度策略应确保所有进程得到公平的处理器时间，不应让某些进程长期得不到处理器资源。公平的调度策略可以防止饥饿现象(starvation)，即某些进程长时间等待却无法执行。
<br>
<br>优先级(Priority)<br>
某些系统中，进程可能具有不同的优先级。调度策略需要根据优先级分配处理器时间，确保高优先级进程能及时得到处理。这对于实时系统和需要紧急处理的任务尤为重要。
<br>折中与权衡(Trade-offs)<br>
在实际应用中，不同评估标准之间往往存在冲突。例如，优化周转时间可能会增加等待时间，或者提高吞吐量可能会降低公平性。因此，选择调度策略时需要在这些标准之间进行折中和权衡，以实现系统的最佳性能。
<br><br>在系统中运行的进程根据其行为模式可以被分为不同的类型。根据进程主要消耗CPU资源还是I/O资源，我们将进程分为CPU密集型和I/O密集型。此前，我们先明晰burst的概念。<br><br>CPU burst指的是进程在一段时间段内连续使用CPU。在这段时间里，进程主要执行使用CPU的计算任务，不进行I/O操作。<br>I/O burst就是进程在时间段内连续进程I/O操作，而不进行使用CPU的计算任务。<br><br>CPU密集型和I/O密集型进程的判断实际上就是通过CPU burst和I/O burst的相对时长判断的。CPU密集型进程的CPU burst时间较长，而I/O burst时间较短。而I/O密集型进程则相反。<br>由于I/O密集型进程因为所需要的CPU时间并不多，所以被称为短作业或短进程。类似的，CPU密集型进程也被称为长作业或长进程。<br><br><br><br>先来先服务算法是最简单的CPU调度算法。采用FCFS，OS的调度程序会按照进程进入就绪队列的先后顺序分配CPU时间。<br>若采用非抢占式(non-preemptive) 的先来先服务算法，每个进程会一直占用CPU知道执行结束或主动放弃，然后调度程序会选择就绪队列中的下一个进程继续执行。<br>因为FCFS可以通过FIFO队列的方式轻松实现，因而也称为FIFO算法。<br><br>
<br>FCFS算法是最易于理解和实现的调度算法，这是它最大的优点。
<br>此外，得益于先进先出的性质，FCFS算法是公平的。
<br>FCFS一般适用于批处理系统。
<br><br>
<br>由于算法非常简单，没有优化策略，所以FCFS的效率不高。
<br>等待时间和周转时间受进程顺序影响大，可能导致车队效应(convoy effect)。
<br>此外，由于后来的进程往往需要等待前面所有进程的结束，这会导致靠后进程的饥饿。
<br>（车队效应容易发生在IO密集型进程队列中。这类进程只用很短的时间在CPU上执行。如果有一个 CPU 密集型进程在队列的前面，它会占用 CPU 很长时间，导致后面的 I/O 密集型进程不得不等待。这会导致 CPU 和 I/O 设备的利用率降低，系统效率下降。）<br><br>RR调度算法是增加抢占的FCFS算法。它定义了一个较小的时间片(time slice) 或 时间量(time quantum) ，通常是10~100ns。将就绪队列作为循环队列，CPU调度器循环整个就绪队列，为每个进程分配不超过一个时间片的CPU。RR算法是专门为分时系统设计的，也是最常作为核心使用的调度算法。<br><br>
<br>简单、易于实现且对于就绪队列的每个进程都是公平的。
<br>CPU使用率高：若时间片大小合适，RR算法通常可以保持较高的CPU使用率。
<br><br>
<br>低吞吐量：由于每个进程都要以相同的时间片循环共享CPU资源，因此可能会导致比FCFS更小的吞吐量(下同)。
<br>高等待时间和高响应时间。
<br>在进程调度过程中有上下文切换(Context switching)，当时间片设置的过小时，频繁的上下文切换会占用大量CPU资源。
<br><br>虚拟轮转(Virtual Round Robin, VRR)是传统轮转调度（Round Robin, RR）的改进版本。虚拟轮转调度的主要目的是解决传统轮转调度在处理I/O密集型进程时的效率问题。<br>在传统轮转调度中，每个进程都会被分配一个固定的时间片，当时间片用完时，进程会被切换到队列的末尾。然而，这种方法对I/O密集型进程不太友好，因为这些进程通常会在时间片用完之前就进入等待I/O操作的状态，导致它们频繁地被切换，增加了系统的开销。<br>虚拟轮转调度通过引入一个虚拟队列(auxiliary queue)来解决这个问题。当一个进程进入等待I/O操作的状态时，它会被移到虚拟队列中，而不是直接切换到主队列的末尾。当I/O操作完成后，进程会被重新放回主队列的适当位置，以便尽快获得CPU的使用权。<br><br>最短优先调度算法将每个进程与下次CPU执行长度关联起来。当CPU空闲时，调度器会将CPU资源赋给最短CPU执行时间的进程。短作业优先算法可以是抢占式的，称为最短剩余时间优先(Shortest Remaining Time First)。通常情况下SJF特指非抢占式调度。<br>由于最短时间的进程总是先执行，所以系统的平均等待时间和平均周转时间总是最优的。但是最短优先算法不可避免的会导致长进程的饥饿。而且最短作业优先的策略很难实现，因为CPU无从开始时得知每个进程要使用多长的CPU时间，因此SJF是很难实现的。<br><br>
<br>SJF is an optimal algorithm, which uses the best way of CPU and I/O devices.
<br><br>
<br>算法很难实现。
<br>在执行过程中到达的更短进程得不到及时的响应。
<br>会导致长进程的饥饿。
<br><br>SRTF是最短作业优先的抢占式版。在这种算法下，最短剩余时间的进程总会先于其他进程执行。<br><br>
<br>吞吐量大：短进程会被即时响应。
<br>系统开销小：由于SRTF算法只在进程完成或添加新进程时才做出调度决策，因此系统的管理开销很小。
<br><br>
<br>算法很难实现。 
<br>会导致长进程的饥饿现象。
<br><br>优先级调度算法会根据每个进程的优先级来分配CPU时间。在这种调度算法中，每个进程都会被赋予一个优先级，CPU总是将资源分配给优先级最高的进程。<br>
<br>优先数(Priority)：描述进程重要性和紧急程度的一个属性。
<br>优先数(Priority number)：用于量化优先级的一个数值，一般而言，优先数越低，优先级越高。
<br>优先级调度算法有抢占式(preemptive)和非抢占式(non-preemptive)版本。
<br><br>相比于轮转调度算法，优先级调度算法显然不算是一个公平的算法，因为低优先级的进程往往需要忍受饥饿。但是我们可以通过老化的方法解决这个问题。<br>
<br>通常而言，系统会设置一个时间间隔。每经过一个时间间隔，系统就会随着时间的推移增加等待进程的优先级。
<br>系统需要平衡优先级提升和实际需求，确保高优先级进程的响应时间不会因为频繁提升低优先级进程的优先级而收到太大影响。
<br><br>除了动态提升进程的优先级外，还可以通过降低高优先级进程的优先级来防止饥饿问题。<br>
<br>时间片耗尽：当一个高优先级进程消耗完它被分配的时间片后，其优先级可能自动被降低。
<br>资源占用检测：如果一个进程在一定时间内占用了过多CPU时间，系统可能会动态调整其优先级。
<br><br>在高响应比优先算法中，调度器需要计算就绪队列中所有进程的响应比(Response ratio)，并选择响应比最高的进程分配CPU时间。响应比的计算公式是：W表示进程在就绪队列中的等待时间；<br>
S表示进程的预计服务时间（运行时间）。<br>我们可以看出，这种算法是对SJF算法的改进版，使得长进程经过一段时间的等待（W增加，RR变大）以可以获得CPU。HRRN算法综合等待时间和运行时间，防止长进程饥饿，提供较为公平的调度。通常情况下，该算法默认使用非抢占式。<br><br>多级队列调度算法将就绪队列中的不同进程按照类型划分成多个不同的队列中：<br>
<br>队列分类：按照进程的需求和行为特性，将进程分类到不同的队列中，例如按进程类型（交互式、批处理、系统级等）进行分类。
<br>优先级设置：在MLQ调度中，可以将每个队列设置为不同的优先级，一般的优先级排序为：
<br>调度策略：每个队列可以采用最适合其进程特性的调度策略。如，前台交互式进程通常采用RR策略，后台批处理进程则可能再用FCFS策略。
<br>调度执行顺序：调度器首先会检查高优先级的队列是否有可运行的进程，若没有，调度器才会检查下一优先级的队列。
<br>抢占和非抢占
<br><br><br>多级反馈队列调度算法综合了静态优先级和时间片轮转算法的特点，实现了动态调整进程优先级来优化CPU时间段分配。<br>
<br>进程队列：系统设置多个队列，每个队列具有不同的优先级。通常而言，优先级排序为
<br>调度策略：每个队列均采用时间片轮转算法，队列的优先级越低，队列分配的时间片越长。
<br>调度执行顺序：新就绪的进程首先进入最高优先级队列（）。如果进程在其时间片结束时未完成，则会被降至下一优先级队列（从移到）。如果进程在较低优先级队列中因等待I/O操作而被贬为组测状态，并在I/O完成后再次就绪，它则可能被提升回更高的优先级队列。
<br>抢占和非抢占
<br>Note：尽管多级队列调度算法和多级反馈队列调度算法都划分了很多队列对进程进行划分。但它们在进程调度上有着本质的区别。<br>
<br>在MLF中，进程一旦被分配到某个队列，就会在完成前一直待该队列中。这会必定的导致某些进程的饥饿。
<br>在MLFQ中，进程可以在不同的队列间”移动“。因而，采用MLFQ的OS可以根据进程长短动态地调整进程在长进程队列还是短进程队列。克服了SJF不能预测进程长短的缺点。保持CPU不空闲的前提下实现了较短进程优先，所以MLFQ的效率相较于MLQ要高。
<br><br>Garanteed scheduling的调度算法不同于前面注重于效率的多中调度算法，这种算法旨在确保每个用户公平地获得相等的CPU时间。比如系统内有 n 个用户，那么每个用户将获得 1/n 的CPU时间。或者 m 个进程，每个进程获得 1/m 的CPU时间。这种算法可能不是最高效的，但是相对公平的。<br>为了实现Guaranteed Scheduling，系统需要跟踪每个用户或进程所用的CPU时间。具体来说，系统会计算每个进程的实际CPU时间与预期CPU时间之间的差异。如果某个进程的实际CPU时间少于预期CPU时间，系统会优先分配更多的CPU时间给该进程，以确保公平性。<br><br>Lottery scheduling是一种随机化的调度算法。每个进程根据其优先级或需求在某种资源上获得一定数目的“彩票(lottery tickets)”。资源调度时，调度器会随机抽取一张彩票，持有该彩票的进程将获得资源。<br>在Lottery Scheduling中，假如某资源总彩票数量为 T，某个进程拥有 f 张该类型的彩票，那么这个进程获得该资源的概率大约为 f/T。当进程被创建或终止时，系统会调整彩票的数量，以确保资源分配的公平性。<br><br><br>当没有事情可做时，调度算法会调度什么呢？因为调度器并不能自己生成一个进程去运行，在这种情况下，调度器会加载idel task去运行。Idle Task 没有任何依赖关系（dependencies），即它不依赖于其他任务或资源来执行。由于无事可做的状态任何时候都可能发生，所以idel task不可以被阻塞，以确保随时都可以被加载。<br>在不同的系统里，idel task的实现可能是不同的，可以是重复地唤起调度器，也可能是做一些无意义的加法运算，或是执行一堆的NOP指令。当执行idle task时，CPU会被 halt/switch 到低功耗模式。它的主要目的是确保CPU始终有任务可执行，从而避免系统进入空闲状态。<br>这些无用功好像毫无意义，但是idel task还是有它的存在意义的：<br>
<br>防止了调度器无事可做，避免调度器返回状态异常。
<br>提供了任务运行时间的会计信息，以便我们能够知道系统处于空闲状态的时长。
<br><br>Priority Inversion 是一种调度问题，描述了一种高优先级进程等待低优先级进程释放资源的情形。当一个低优先级的任务持有一个高优先级任务所需的资源时，高优先级任务被迫等待，导致系统性能下降。这种情况通常发生在多任务操作系统中，尤其是在实时系统中。<br>想象这样一种情形，高优先级任务 、中优先级任务  和低优先级任务  。 作为一个高优先级的进程等待低优先级  所占有的信号量。而且  在临界区中，由此  进入阻塞状态。如果在此期间  开始运行，由于  的优先级高于 ，所以  会抢占CPU， 被阻塞，进一步延迟  的执行。这就是Priority Inversion。在实时系统中，这种高优先级进程必须尽快运行。我们需要寻找一种解决办法。<br><br>为了解决Priority Inversion问题，可以使用Priority Inheritance协议。当低优先级任务持有高优先级任务所需的资源时，低优先级任务会暂时继承高优先级任务的优先级，直到释放资源为止。这可以确保高优先级任务尽快获得所需资源，减少等待时间。<br><br>1997年，NASA的Mars Pathfinder探测器在火星上运行时，遇到了一个严重的问题：系统频繁重启，导致数据传输中断。这个问题的根源在于Priority Inversion。具体来说，火星车有这样三个任务：<br>
<br>低优先级任务：获取一个信号量，对information bus进行加锁。
<br>中优先级任务：处理通信（communication）。
<br>高优先级任务：需要使用information bus。
<br>当低优先级任务加锁时，高优先级任务进入阻塞状态。这时，中优先级任务开始运行，并抢占了CPU资源，从而低优先级任务也被阻塞，同时并不释放信号量。由于中优先级任务一直运行，高优先级任务一直得不到锁，系统会认为高优先级任务运行错误（task failure）。<br>高优先级作业一直被阻塞会让系统认为发生了严重的死锁。为了尽快恢复高优先级作业的运行，系统会进行Armageddon的死锁解决方案，即reboot。这就意味着一些数据的丢失。<br><br>和优先级继承的解决思路类似。优先级天花板会将为每个共享资源分配一个优先级天花板来防止低优先级任务阻塞高优先级任务。<br><br><br><br>当我们的视界迈入多处理器的时刻，我们的世界发生了巨变，同时，复杂度会迈上一个新的台阶。当前，我们可以考虑的多处理器系统主要有三类：<br>
<br>Distributed System：服务器多采用的处理器架构，系统中的处理器通过网络连接。
<br>Functionally Speciallized：处理器的功能特化。一个CPU用于图像处理，另一个用于数据处理。
<br>Tightly Coupled：系统中的处理器共享内存和资源，通常通过高速总线连接。
<br>本节课，我们聚焦于紧耦合型的系统，这也是当前个人PC上常见的多 die 多核心CPU的类型。<br><br>
简单起见，一般认为每个处理器只有一个核心。本文部分内容作为补充介绍多 die 多核心 CPU。
<br>在之前的讨论中，我们仅仅将讨论范围放在了单核心处理器(single-core processor) 的调度策略上面。然而，由于主频发展的瓶颈，要提高CPU的性能，我们需要增加CPU的核心数，即多核心处理器(multicore processor)。当下，多核心处理系统在大多数计算机上得到了广泛的应用，在服务器上，处理器的核心数甚至能够达到144核心（Intel Sierra Forest-SP）。<br>在单核系统中，所有的进行共享同一个处理单元，调度器只需要决定下一个要执行的进程是哪个。而在多核系统中，由于调度器需要在多个核心之间分配任务，调度复杂性大大增加。我们下面从三个方面举例其相比单处理器的复杂性：<br>
<br>并行性(Parallelism)：由于在单处理器系统中我们只有一个核心，所有进程必须串行执行，系统能够并发但没有并行。由于核心数的增加，多核系统能够在任务一占据核心0的同时任务一占据核心1同时执行。为系统带来了并行性。
<br>负载均衡(Loading Balancing)：在单核心处理器上，CPU资源十分珍贵，我们想让处理器时时刻刻都保持高负荷的状态。在多核心处理器中，我们需要考虑负载均衡，即如何将任务均匀地分配给各个核心。如何保证每个核心都运行在最佳的状态才是我们要关心的。
<br><img alt="Pasted image 20241003233336.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241003233336.png"><br>
3. 核心亲和性(Core Affinity)：在计算机组成的Memory hierarchy中，我们学习了Cache的分级。其中，每个核心都会对应一个L1 Cache和一个L2 Cache。这些缓冲中存储着最近在当前核心上运行过任务的数据和指令。如果进程频繁切换或混乱调度，就会造成每次核心的调度都会造成缓存失效，性能降低。<br><br>在并行计算中，粒度(granularity)，或“谷粒大小”（grain size)，描述了并行计算中任务的规模大小。系统能够进行并行任务的规模越大，粒度就越大。细粒度(fine-grained) 的任务通常较小，可以均匀地分配到多个处理器或核心上运行。粗粒度(coarse-grained) 的任务较大，不易均匀分配任务，可能导致负载不均衡。所以细粒度的任务一般部署在同一台主机上，而粗粒度的任务可以部署到多处理器系统或分布式系统上。<br>指令间隔(Instruction Interval) 指的是处理器执行指令之间的时间间隔。这种时间间隔包含了指令之间可能存在的等待时间、访存时间和同步通信所用的时间等。<br>粒度大小和指令间隔大小息息相关。一般来说，粒度越大，指令间隔就越大。下面是几种系统，其中粒度和指令间隔逐步增大：<br>
<br>
单处理器系统(Single Processor System)：

<br>粒度：最小
<br>指令间隔：通常较短，因单处理器执行所有任务，无需并行通信和同步。


<br>
功能特化的多处理器系统(Functionally Specialized Multiprocessor System)

<br>粒度：小
<br>指令间隔：适中，因特化处理器执行特定任务，但需要一些同步和通信。


<br>
多处理器系统(Multiprocessor System)

<br>粒度：中等
<br>指令间隔：适中，因多个处理器并行执行任务，需要更多的同步和通信。


<br>
功能特化的分布式系统(Functionally Specialized Distributed System)

<br>粒度：较大
<br>指令间隔：较大，因不同系统节点执行特化功能，需频繁通信和同步。


<br>
分布式系统(Distributed System)

<br>粒度：最大
<br>指令间隔：最大，因任务在不同节点分布，网络延迟和同步开销较大。


<br>细粒度由于任务较小，同步和通信往往只局限在同一台主机上的不同核心上，同步通信的开销较低，因此细粒度系统的指令间隔较小。粗粒度系统由于任务较大，通信和同步的开销较大，所以指令间隔可能非常大。<br><br><br>多核CPU？多CPU？如果你为这两个名词所困扰，我们不妨先了解一下CPU是如何制造的。高纯度硅经过切割之后我们得到一个个晶圆(wafer)，晶圆经过光刻等制造出许多个相同的电路图案，每个图案切割后我们就得到了一个die(裸片)。而die功能越复杂（面积越大），die的良品率就会越低。即die越大，成本越高，但是性能好。<br>如果我们有一个48核心CPU，我们该如何权衡利弊？我们可以将48个核心集成到一个die上封装成单die多核心CPU；还可以将48个核心分别集成在4个die上，每个die对应12个核心封装成多die多核心CPU。在第二种方案上，我们不额外考虑外围电路和总线的封装细节。第二种方案显然成本更小，也有很多实用性，每个die就对应着我们的一个处理器(CPU)。这也是多处理器调度的单位。<br><br>了解了CPU如何制造，现在，你应该能明白多处理器调度的实质其实就是多核心调度。但是这些核心被封装到了不同的die中了。这样带来的问题就是die内数据的传输是会快于die之间的数据传输的，而且多个die还要考虑总线冲突的问题。<br><img alt="Pasted image 20241004014342.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241004014342.png"><br>除此之外，在x86类型的系统上，L1和L2的cache是每个处理器一个的，而L3 cache是所有处理器共享的。当进程  在Core1上运行一段时间后跳到Core2上运行，这就会导致大量的cache层面的page fault（cache miss）。<br><br><br>非对称多处理器(Asymmetric Multi-Processing)：在AMP中，处理器的职责不对称。通常有一个主处理器负责系统调度和IO操作，其他从处理器则仅负责执行分配给它们的任务。主处理器在整个系统中占据主导地位，所有进程调度和系统调用都由它处理，从处理器则不直接参与调度决策。<br>
<br>硬件架构简单，主从职责分明。
<br>
<br>主处理器的性能可能成为瓶颈，扩展性不佳。
<br><br>对称多处理器(Symmetric Multi-Processing)：在SMP中，每个处理器共享同一个主存、IO设备以及操作系统，并且它们具有对等的访问权限。每个处理器都有同样的权利参与任务调度，并执行相同的操作。由于所有处理器可以访问共享的资源，调度器的任务是将进程公平地分配给各个处理器，使得系统负载均衡。<br>
<br>处理器平等共享资源，简化了设计。
<br>处理器之间需要共享内存和I/O总线，这可能会导致瓶颈。
<br>现代操作系统大多采用SMP结构，因为SMP系统能够更好地利用多核处理器的并行计算能力，并且更适合现代通用计算环境中的高并发和负载均衡需求，比如Linux、Windows、macOS等。而AMP更多用于嵌入式、实时系统等特定场景中。<br><br>非一致性内存访问(Non-Uniform Memory Access) 是一种多处理器内存架构，用于优化多处理器系统中的内存访问效率。在NUMA架构中，系统中的多个CPU被划分成不同的节点（Node），根据CPU和主存的链接方式，每个node访问不同内存区域的速度并不一致。根据这种差异，我们说每个节点有自己的本地内存和资源。<br>内存、IO和CPU的链接方式使得每个节点的CPU可以快速访问本地内存和IO，但访问其他节点的内存或IO会有较高的延迟。<br><img alt="Pasted image 20241004021149.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241004021149.png"><br>在NUMA系统上编写程序时，尽量将线程绑定到对应的CPU，并从该CPU的本地内存分配内存，以最大化性能。使用工具如numactl可以查看和设置NUMA相关的信息，优化程序的内存访问模式。<br><br><br>如果我们有4个处理器，如果其中一个处理器的利用率是 100% 而其他三个处理器无事可做，这种情况在我们看来是不理想的，因为所有任务都在一个处理器上运行可能会导致L1和L2 cache频繁的发生cache miss。<br>负载均衡就意味着系统的工作负载相对均衡，每个处理器的利用率都相当。相比于所有的处理器共享一个全局的任务队列，负载均衡在每个处理器有自己的私有任务队列的情况下更加重要。因为任务最初是分配到各个处理器的私有队列中的。如果某个处理器的队列过载，而另一个处理器的队列较空闲，那么负载就不再均衡。这时就需要通过负载均衡策略（如PUSH和PULL迁移）来重新分配任务，以确保所有处理器的负载均衡。<br>
<br>PUSH migration：当某个处理器的任务队列过载，调度器会主动将任务从这个处理器上PUSH到其他负载较轻的处理器上。PUSH migration是一种主动的负载均衡策略，适用于避免个别处理器过载的问题。
<br>PULL migration：当某个处理器负载过轻，处理器就会从其他负载中的处理器那里PULL任务。PULL migration是一种被动的负载均衡策略，用于确保处理器不闲置，始终保持忙碌状态。
<br><img alt="Pasted image 20250112055229.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250112055229.png"><br><br>多处理器系统引入处理器亲和性策略使得进程尽可能在同一个处理器上连续运行。这可以最大限度地保持进程数据在处理器的本地缓存(Cache)中，减少由于进程迁移而导致的缓存未命中开销，以及减少处理器间的缓存一致性开销，从而提升系统性能。<br>
<br>硬亲和性(Hard Affinity)：规定进程只能在某个特定处理器上运行，调度器不会将其迁移到其他处理器。
<br>软亲和性(Soft Affinity)：尽量使进程在某个处理器上运行，但在特定情况下，调度器可以将其迁移到其他处理器。
<br>Linux中有关于两种处理器的亲和性的选项，我们可以使一个进程只在一个处理器上运行的同时让另一个进程尽量在另一个处理器上运行。<br><br>负载均衡调度是从全局视角进行的优化，期望进程能够自由地在不同的处理器之间迁移。然而这种频繁的迁移可能会导致缓存命中率直线下降的问题，降低缓存的利用效率。<br>另一方面，处理器亲和性注重局部的优化，期望进程尽可能地留在一个处理器上，以便提高缓存的命中率的同时也减少缓存的一致性问题。但这样可能带来处理器负载不均匀的问题。<br><br>超线程技术(Hyper-Threading Technology，HTT)是英特尔公司开发的一种技术，旨在提高处理器的并行处理能力。通过在每个物理处理器核心上创建多个逻辑处理器，超线程技术使得单个处理器能够同时处理多个线程，从而提高了处理器的利用率和整体性能。<br>超线程技术通过在一个物理核心上运行多个线程，可以在一个线程发生memory stall时，切换到另一个线程继续执行，从而减少处理器的空闲时间，提高处理器的利用率。这种技术在多任务处理和高并发应用中尤为有效，因为它能够更好地利用处理器资源，减少内存访问延迟对性能的影响。<br>我们知道CPU可以处理算数和逻辑运算，有了超线程技术，我们不再把CPU看作单一不可划分的资源。举个例子，我们有线程 1 和线程 2，我们通过超线程可以实现在线程 1 使用CPU算数运算的同时线程 2 使用逻辑运算。<br><img alt="hyper.webp" src="https://congzhi.wiki/congzhi's-os-series/pics/hyper.webp"><br><br><br><br>本课作为简单补充实时调度并对实时操作系统进行简短的介绍。简单来说，实时调度就是实时操作系统上进行的调度。那什么是实时系统？我们从它的实时性来源——墙上时钟时间来进行介绍。<br><br>对于实时操作系统而言，任务必须在严格的时间限制内完成。这个时间基准就是由墙上时钟时间来提供的。时间限制就意味着deadline，在实时系统中，对missing deadline没有任何容忍。如果任务没有在严格的截止时间前完成的话，就意味着系统故障。<br>常见的操作系统（如Windows、Linux、MacOS）并不能怎么匹配实时操作系统，主要原因在于它们对超时截止时间没有保证(guarantee)。这些操作系统设计的初衷是为了通用性和多任务处理，而不是为了满足严格的实时性要求。<br>在Windows上，即使你在任务管理器中将任务设置为实时优先级(Real-time priority)，系统也不保证任务能够在严格的时间限制内完成。这是因为Windows的调度机制并不是为硬实时任务设计的，系统中的其他因素（如内存管理、中断处理等）仍可能导致任务无法按时完成。<br>相比之下，实时操作系统(Real-Time Operating Systems, RTOS)专门设计用于满足实时性要求。它们具有更严格的调度机制和时间管理，以确保任务在规定的时间内完成。例如，RTOS会使用优先级调度、最早截止时间优先(EDF)等算法来确保任务按时完成，并处理硬实时任务的严格时间限制。<br><br>不同于前面调度中对“快”的追求，实时系统中安全和时效是最主要的（可预测性(Predictablity)）。最重要的任务必须尽早地完成，并且在相应的时间内完成。在实时系统中，对于不同的任务，我们有不同的解决方案。我们可以把实时任务分成这三种：<br>
<br>硬实时任务(Hard Real-Time Tasks)：硬实时对错过截止时间没有任何容忍度，任何延迟都会被视为系统失败。所以硬实时任务必须在严格的时间限制内完成，否则就会导致系统的故障。例如，对即将到来的导弹轨道进行计算拦截，即便延迟仅仅0.1秒钟，都可能相差数百米，这显然不是我们能够接受的。
<br>固实时任务(Firm Real-Time Tasks)：固实时任务也需要在截止时间内完成，但如果偶尔错过截止时间，系统不会立即出现故障。然而，错过截止时间会影响系统的性能和可靠性。例如，在线交易系统中，偶尔的延迟可能不会导致系统崩溃，但会影响用户体验。
<br>软实时任务(Soft Real-Time Tasks)：软实时任务在”时间限制“上的管理相对更松弛，允许在一定范围内的时间延迟。比如天气预报15分钟后下雨，但是现在已经下雨了，虽然带来的错误的信息，但是代价也许是我们能接受的。
<br>实时任务要求实时的可预测性，这就是为什么Java这类解释型语言不适宜作为实时系统中的工作语言。更何况Java虚拟机在进行垃圾回收(Garbage Collection, GC)时，JVM会暂停JVM中的所有应用程序线程执行，直到垃圾回收完成。这肯定不符合RTOS对可预测性的要求。<br><br>在实时系统中，实时失败(Real-Time Failure) 是指系统未能在规定的时间内完成任务，导致系统无法满足其时间约束。这种失败可能会导致严重的后果，特别是在硬实时系统中。<br><br>如果任务是硬实时的，我们有两种任务无法在截止时间内完成的场景：<br>
<br>调度太晚：例如，需要两小时完成的任务在距离截止时间还有一小时的时候才开始调度运行。
<br>任务执行中断：调度开始运行时，任务是可以完成的。但是由于某种原因（如高优先级的任务被调入），完成任务变得不太可能。
<br>第一种情况下，系统可能会拒绝任务的开始请求，或是永远也不会调度任务运行。因为没有必要在浪费CPU时间在一个不能按时完成的任务上面。第二种情况呢？有没有什么办法使得任务能按时完成的同时尽量减少对系统造成的影响？当然有，这就是我们为什么引入实时调度的原因。<br><br><br>实时系统在以下五个关键方面具有其独特性：<br>
<br>确定性(Determinism)
<br>响应性(Responsiveness)
<br>用户控制(User Control)
<br>可靠性(Reliability)
<br>故障软处理(Fail-Soft Operation)
<br><br>确定性预示着操作可预测，以确保任务能够在规定的时间内完成。完美的确定性(determinism)并不存在。在实时操作系统上，有时候我们只是希望得到一个保证，即不管怎么样，实时系统都能尽量确保任务按时完成。这种保证不一定是绝对的，但必须能够在大多数情况下满足系统的时间约束。<br>
尽管我们在实时系统上追求这种determinism，但是这并不意味这non-determinism是不好的。一个典型的例子就是caching，通过一些优秀的置换算法，cache可以大大提升系统的性能。在一些嵌入式系统上，可能并不会配备cache，通过单一的主存结构，实际上你可以得到更好的determinism。因为任何任务访问主存的时间都是相近的。<br><br>Responsiveness并不简单地指系统响应所需要的时间。determinism相当于告诉我们当任务发出请求之后，系统多久才能接收到请求，这需要规定在一个确切的值。而responsiveness指系统收到请求后开始响应事件(handle the event)的时间间隔。由于中断还可能被高优先级的任务中断嵌套，responsiveness并不仅仅涵盖从接收到请求到执行中断服务例程(interrupt handler)的时间，还要把执行到高优先级中断服务例程的时间加到一起。<br><br>Administrator control对于系统的影响是巨大的，毕竟每个系统最终的服务对象都是人嘛。在实时系统中，我们可以适当的增加系统的实时性（完全的硬实时系统），也可以减弱实时性的存在，如果操控得当，你得到的实时系统可能会类似于typical OS。<br>尽管我们的可操作空间很大，但由于操作对象是实时系统，有些地方是不能修改的。在实时系统中，我们可以使用以下两种策略：<br>
<br>不做修改，系统以原汁原味地方式提供操作。
<br>根据需要做适当的修改：作为administrator，我们确实能在系统上做很多修改。比如调整任务的优先级、实时类型（硬实时、固实时、软实时），甚至选择系统的调度策略。因为操作系统并不能判断哪个任务是实时的，哪个是general purposes的。
<br><br><br>不同于典型系统中调度影响性能不同，实时系统中，调度直接影响着系统的成与败。当你选择了一个很差的调度策略，系统可能会忽略掉高优先级任务的运行请求，系统出现故障，“保证”无法得到保证。通过这些方面的介绍，你可能也能感受到了任务调度在实时系统中的地位。<br>对于实时系统，我们要确保在规定时间内完成所有的硬实时任务，同时尽量完成尽可能多的软实时任务。不同调度算法的选择可能会导致系统中截然不同的任务完成情况。请记住，在实时系统中，保证重要任务的按时完成是首要任务，“效率”并不是我们首要考虑的。<br>在调度策略的选择上，所有的非抢占式调度算法都不能有效地发挥作用。一旦有硬实时任务未能按时完成，系统就会出现故障。同样，分时系统也不适用于实时系统，因为你不能让高优先级的任务等待低优先级任务的时间片完成。调度越是干脆利落，对实时于系统的稳定性和可靠性就越好。<br><br>在实时系统中，我们可以将执行的任务分成下面这四种：<br><br>望名生意，fixed-instance的任务只在固定的、预先定义的一段时间内运行。它们有固定的执行实例，通常用在那些特定时间执行操作的场景中，一般只会运行一次。比方说系统启动时初始化系统的任务、数据库的初始化等。<br><br>这些任务在固定的周期内重复执行。它们在每个周期结束时被重新调度，广泛用于定期检查、传感器数据采集等场景。例如，每隔一秒钟读取一次传感器数据。周期性任务有两个相关的属性： 和 。前者指的是周期，后者指最坏的运行时间<br>由此，我们可以得到下面的公式：这个公式用来计算系统的利用率，即所有任务的最坏运行时间与其周期的比值之和。我们需要保证 。因为一旦  ，那就以为着系统过载了，即系统内的周期性作业太多了。实时系统不能够保证每个任务都能在规定时间内完成。<br>那我们只要选择合适的调度策略，保证系统内  就好了么？事情还远远没有这么简单。因为系统内不仅仅只存在周期性任务，除了fixed-instance任务，我们仍然还要两类任务要去了解。<br><br>Aperiodic任务没有固定的执行周期，这些任务的触发是随机的，且没有最小的时间间隔限制。这就使得当一个aperiodic硬实时任务运行时，如果有更高优先级的硬实时任务到达，我们可能无法保证前一个aperiodic任务按时完成。<br>Aperiodic任务一般用于处理非周期性的事件，例如用户的请求或外部事件。任务的执行时间可能是不确定的，但仍需要在合理的时间范围内完成。如何调度这些任务，以及用什么顺序完成任务的调度，是值得我们思考的问题。<br><br>这些任务类似于aperiodic任务，但它们有最小的时间间隔限制(Minimum Inter-Arrival Time)。这意味着在任务之间必须保持一定的间隔时间（比如说  ）。这类任务通常用于处理偶发事件，如紧急报警或异常处理。<br><br>在正式的进入实时调度策略的学习之前，我们还需要思考一个问题。之前的学习中，我们能发现在实时系统中"deadline"到处都是，那怎么才能知道任务大概需要多长时间执行呢？不管在之前的学习中还是生活中，预知未来一直都是一个很困难的事情。但是在实时系统中，我们需要这样的事件来保证系统的determinism。<br><br>在周期性的任务小节中，我们见过这个公式：上面的  指的是最坏运行时间，怎么得到的？我们的日常生活中很多测试的软件，其中不乏有指令执行时间的测试工具和整个软件执行时长的测试工具，在日常中，我们有两种方法来预测这个worst case scenario time。<br>
<br>源代码分析(Source code analysis)
<br>实证测试(Empirical testing)
<br><br>软件要运行，就要按照其源码一行一行地执行。为了得到  ，源代码分析预测的方式就需要大致地知道所有的这些机器指令全部执行完毕需要多久。我们要得到最坏情况下的执行时间，当然也就要忽视一些机器和系统层面的优化，例如，忽略流水线(pipelining)和编译器优化等。<br>为了使得预测具有可行性，市面上有很多我们可以遵循的标准。例如，在 NASA/JPL 指南下：<br>
<br>代码中不可以出现递归和goto语句；
<br>如果代码出现循环语句，循环边界必须是固定的；
<br>在软件初始化之后，不允许再出现动态内存分配。
<br>虽然这些指南提供了一个框架来确保代码的可预测性和稳定性，但它们并不是绝对的规约。如果你能够有效地控制动态内存分配所带来的时间开销，那么在代码的任何位置使用malloc进行内存动态分配是可行的。（使用binary buddy system）<br>源代码分析通常适用于代码量较小的任务，因为在这种情况下，分析和预测每条指令的执行时间相对简单。然而，当代码量变得庞大和复杂时，源代码分析的难度会显著增加。这就像估算从宿舍走到食堂的时间和从宿舍走到家里的时间，显然这两者的复杂度不在一个层面。在这种情况下，我们可能需要另一种预测方法——实证测试。<br><br>除了源代码分析，我们还有另一种更简单和直接的办法：把程序从头到尾地跑一遍，看看执行时间有多长。这种办法就是实证测试。实证测试的预测方法除了能够应对绝大多数情况下的任务外，还能够简单地预估一下系统的性能，同样的任务，通过比较在不同机器上运行的时长，我们就可以大致得出两个系统的性能。<br>通过一遍遍地模拟任务在系统内的运行时长，我们可以统计这些信息来设置一个最坏运行时间。利用统计到的模拟运行时间，我们可以使用置信区间(confidence interval)的概念来设置最坏运行时间。例如，当我们模拟的运行时间 99% 的置信区间是 [10s, 20s]，我们就可以设置最坏运行时间为 21s 或 22s。<br><br><br>前面我们简单地提到了实时系统和典型操作系统的不同，了解了一下为什么非抢占和分时的调度算法都不在适用于实时系统，我们补充了timeline的相关概念和调度思想。在本节课中，我们将开始介绍单处理器和多处理器上的实时调度算法。<br><br>
没有最优的调度策略，有的是相对于某个环境下最优的调度策略。
<br><br>EDF，最早截止时间优先算法，算得上是学生们最熟悉也是世界上最常被人类应用的算法。毕竟有句话是这样说的：“DDL是第一生产力”。学生们的想法是不管三七二十一，哪个作业最先截止，就先完成哪个作业。即无论任务的优先级如何，哪个任务离它的时间期限最近，就率先完成哪个任务。<br>如果一个任务正在执行的同时另一个更急迫的任务到了，那正在运行的任务就会被挂起以运行新的那个任务。这就意味着有的periodic任务可能被aperiodic/sporadic的任务给抢占掉（urgent call is coming），对于一些硬实时任务可能不是非常友好，因为有的软实时任务离deadline可能更近。<br>在未过载的系统中，我们有能力完成所有的任务，软实时硬实时并无大碍，只要完成所有的任务就好。但是在过载的系统中，有的硬实时任务就可能被更紧急的软实时任务抢占CPU，从而造成系统故障。尽管系统设计者在设计系统时需要避免过载(overloading)的发生，但是在现实中我们不能指望系统永远步发生故障。为了避免过载带来的系统故障，我们需要做一些改变。<br><br>和我们之前提到过的优先级继承(priority inheritance)很像，为了避免软实时任务抢占硬实时任务的CPU，Deadline Interchange 允许在任务的截止时间之间进行交换，以优化任务的调度顺序，让优先级更高的硬实时任务优先进行调度。这种方法可以最大化系统资源的利用率的同时确保高优先级任务能够按时完成，减少系统过载带来的故障风险。<br><br>
Slack time is how long a task can wait before it must scheduled to meet a deadline.
<br>执行剩余时间算法(LSTF)，是我们要学习的另一种实时调度算法。和EDF有些类似，只是将离比较的时间从  换成了 。就相当于在ddl之前你还可以玩多长时间。Slack time的计算公式如下：举个例子，如果任务只需要执行10ms就能结束而距离deadline还有50ms，那么slack time就等于40ms。我们只要在40ms前执行这个任务就可以保证任务的完成。Slack time可以给我们指示哪个任务是目前应当尽早来做的（我们的目标是not missing any deadlines）。<br>LSTF其实就是在EDF的基础上进行了微调，他们的缺点是相同的，都没有对优先级进行考虑。我们提到过，这可能会使一些硬实时任务miss its deadline，从而导致系统的故障。<br><br>Rate-monotonic调度引入了优先级的概念，主要用于周期性任务的调度。任务优先级的确定基于任务的周期长度：周期越短的任务，优先级越高。RMS的优先级一旦确立，在运行时不会再改变。RMS适用于周期性任务的调度，但是并不算是最优的算法(not optimal)，在  时，系统依然可能出现故障。<br>例如我们有一下三个任务，他们的分别是：、、。在这种情况下，利用率应当为：而经分析发现，第三个任务将会运行异常（fail to meets its deadline）。而使用EDF算法调度就不会出现这种情况。那我们怎么保证所有被RMS调度的周期性任务能够按时完成呢？我们有以下公式：当所有任务的利用率乘积加1的结果小于等于2时，RMS可以保证所有任务按时完成。<br><br>这个算法是rate monotonic的变体，将deadline作为优先级的确定的依据。在DMS下，距离deadline最近的任务的优先级最高。<br><br>由于EDF（Earliest Deadline First）算法是最优算法，因此在实现调度器时，我们希望尽量向EDF靠拢。我们之前提到过，EDF调度器无法区分任务是软实时还是硬实时，这可能导致系统内任务不能预期完成。一种解决方法是让非周期性任务和软实时任务的优先级永远低于硬实时或固实时任务的优先级，但这种做法可能会导致任务间抢占可能会变得更加频繁、平均任务完成时间的增加，这对于调度算法来说，可能并不是我们想要的结果。<br><br>为了简化调度并优化这种解决方案，在实时系统的理论中，我们使用Polling Server来解决这一问题。polling server相当于一个携有多个非周期性任务的“容器”，polling server本身是一个有着固定执行时间的周期性硬实时任务。它会在固定的时间间隔内检查是否有非周期性任务需要处理，并在需要时分配资源进行处理。这种方法可以有效地将非周期性任务与周期性任务结合起来，保证系统的响应性和稳定性。<br>Polling server是aperiodic server的一种实现方式（periodic version）。我们举个现实中的例子，polling server就相当于午休时间，一般可能是12：00-2：00。你可以在这两小时中做你想做的任何事。相当于polling server在运行时调度一些非周期性的任务。在polling server运行的时间内，非周期性的软实时任务会被调度运行。通过定期分配处理时间给非周期性任务和软实时任务，确保它们能够及时得到处理，同时不会影响硬实时任务的调度。这种方法在保证系统整体性能的同时，减少了非周期性任务和软实时任务的等待时间。<br><br>我们还可以拥有多个server来服务各种各样类别的任务。相较于aperiodic server，multiple server可以进行更细粒度的任务管理。比如一个服务器处理硬实时任务，一个服务器处理非周期性任务等。<br><br>DDS是对polling server的进阶。在polling server中，当期限内全部的非周期性任务执行完毕了，server就会将剩余的时间片丢掉，立即结束本次周期。这就相当于午休时间没事儿做了就去工作了一样。而DDS会保留剩余的时间，以便同周期未来的某一时刻使用。还是午休的类比，如果在午休时间内没有事情做，可以把这段时间留到以后使用，比如下午或晚上。DDS这种安排方式既不浪费时间，又能在需要时迅速处理紧急任务。但是这种保留的时间仅限于今天（同周期内）。<br><br>比起polling server那样在一个周期内分配一大块空间的方式，DSS将这一大片时间分成多个小块(time chunks)，这样做有什么好处呢？我们以等公交为例，polling server相当于30分钟的前2分钟过去3趟公交车，DSS相当于每10分钟一趟公交。虽然平均下来两种方式都是10min/bus，但是第二种方式的公交车载人数往往更多。通过分散时间片，DSS能更均匀地分配资源和负载，避免资源的浪费和系统负载的突发。这类似于每10分钟来一趟公交，可以更均匀地分配乘客流量。<br>在实现上，DSS并不像公交车的例子那样严格地按照每趟10分钟的方式执行，而是相当于一种chunk credit的方式工作。具体来说，DSS 会为非周期性任务分配一个时间片信用额度。这些信用可以在任务需要时使用，而不是在固定时间间隔内强制执行。当一个非周期性任务到达时，如果DSS有足够的chunk credit，该任务会立即得到处理。如果credit unavailable，任务则会被延迟到下个周期。<br><br>又是时候从单处理器的世界中跳出来了，在本节中，我们介绍有关多核实时调度的相关知识。在本节里，我们先区分这两个概念：<br>
<br>Task（任务）：这是需要完成的工作或操作的描述。一个任务可以是任何需要处理的事情，比如计算一个复杂的数学公式、处理一段音频数据，或者控制一个设备。任务本身定义了应该做什么以及如何做，但不包括具体的执行实例。
<br>Job（作业）：这是一个任务的具体实例。每次任务被执行时，都会生成一个作业。比如，如果一个任务是每分钟读取一次传感器数据，那么每次读取操作就是一个作业。作业可以有不同的执行时间、资源需求和截止日期（deadline）。
<br>当我们的视角不再局限于单处理器，多处理器给我们带来了更好的性能。我们思考这几个问题：<br>
<br>抢占是否允许？
<br>作业的迁移(migration)是否允许？
<br>作业的并行化(parallelism)是否允许？
<br><br>多处理器能够一次性允许多个作业，那么抢占的算法还需要么？或者说是抢占是否允许呢？从典型操作系统的单处理器调度中，我们就学习到optimal algorithm是需要抢占式的调度的，而且在实时系统中，没有抢占有可能会造成某些作业影响硬实时任务的执行，我们当然需要抢占。<br><br>作业迁移是指将一个正在执行的作业从一个CPU迁移到另一个CPU。我们提到过，作业的迁移就意味着cache misses，对于CPU dedication scheduling approach（作业1在固定CPU1上完成，作业2固定在CPU2上完成。如：partitioned scheduling），作业的迁移就是不允许的。然而，完全不允许作业的迁移就可能会使得某个作业因为CPUx缺少CPU时间而不能在deadline前完成（其他的CPUy和CPUz可能空闲）。<br>对任务队列的管理上，我们有global scheduling和partitioned scheduling。前者是指所有CPU共享同一个任务队列，后者指每个CPU都有自己的任务队列。对于migration，global scheduling为了能够实现更好的负载均衡，调度器可以动态地将任务分配给最空闲的处理器。Partitioned scheduling则不允许job migration，一旦任务分配到某个处理器，即使其他处理器空闲，也不能进行迁移。<br>这两种approach都会以不同的方式浪费相同的处理器。我们需要更好的方案。在调度的实现上，我们将两者进行结合诞生出另一种调度方式：semi-partitioned scheduling。在任务的初始分配上，任务首先被分配到特定的处理器的任务队列上，类似于 partitioned scheduling。然而，在某些处理器过载时，semi-partitioned scheduling又允许允许少量的任务迁移到其他处理器上执行，以实现负载均衡。这种approach即实现了负载均衡，又使得调度开销相应地降低。<br><br>作业并行化是指将一个作业分解成多个可以独立执行的部分，并让多个CPU同时处理这些部分，从而加速作业的完成过程。<br><br><br><br><br><br>早期传统的UNIX调度有如System V R3和BSD 4.3，这时的系统对实时性并不支持。在后面版本SVR4才对一些实时性进行了支持。本小节中，我们所讨论的都是多级反馈调度系统，其中每个队列都使用轮转算法(Round Robin)进行时间片的切换。<br>传统的UNIX系统虽然有时间片的概念，但是时间片间隔非常长，原始版本的默认值长达1s（一般为100ms-300ms左右）。这就意味着如果进程没有在这1s内被阻塞或完成任务，那么这个进程就会被抢占。这这个时期，系统的优先级经由进程类型和之前的执行情况得出。<br>CPU的利用率可以经由下面的公式得出：其中  用来标记进程，而  是interval。<br>此外，进程的优先级可以由下面的公式得出：其中  是进程  的基础优先级， 表示进程  的一个 "nice" 值。（"nice" 值是UNIX 运行用户自愿降低进程优先级的方式，用来be "nice" to other users。）<br>那个时期的计算机很稀缺，没有人想要将宝贵的时间片让给其他人。所以 "nice" 实际上是给系统管理员使用的。nice值通常从-20到19之间调整，默认的初始值为0，但admin可以在初始时用 nice 命令在启动进程时设置 nice 值，也可以在运行时用  renice 命令让一个进程变得 nicer 一点。<br>在传统的UNIX系统里，一旦一个进程被分到某个进程优先级队列中去，系统会避免进程迁移到其他的队列中。系统会先满足自己所需要的，之后才会尽可能利用好剩下的资源。根据任务类型的不同，优先级由高到低的顺序分为：<br>
<br>Swapper (move process to and from disk)
<br>Block I/O device control
<br>File manipulation
<br>Character I/O device control
<br>User processes
<br>正如你所见，用户进程的优先级最低。然而，这种方式并不算最优，因为所有的I/O密集型进程运行速度较慢，但它们的优先级却高于用户进程。在I/O密集型进程进行I/O操作时，应将CPU让给CPU密集型进程。<br><br>在System V Release 4版本的Unix操作系统中，由于引进了实时性，系统将最高的优先级给了实时进程，然后是内核，最后是用户进程。相比之前的系统，SVR4最大的不同是增加了更多的优先级，现在我们有160种不同的优先级，我们还把他们划分为三大类别。而且引入了抢占点。<br><br>Berkeley Software Distribution是UNIX的一个变种，而FreeBSD是BSD的一个变种。它和SVR4很类似，相比SRV4，它的优先级更多，有256种。并且将进程划分为五个大类。SVR4并不支持多处理器，而在FreeBSD则实现了对多处理器的拓展支持。<br>FreeBSD通过一种交互性评分机制来判别一个线程是否是交互式线程。原理也很简单，如果一个线程经常地被阻塞(blocked)，那就说明这是一个交互式的线程。我们定义有一个最大的交互分  ，将线程的运行时间记录为  ，睡眠时间记录为  。<br>如果睡眠时间大于运行时间，那么交互分数就是：如果运行时间大于睡眠时间，那么交互分数就是：<br><br>在前面我们已经学过了亲和性和负载均衡了。FreeBSD也有Push和Pull机制。<br>Pull: bit mask to indicate it's idle.<br>
Push: twice per second tasks to equalize highest and lowest CPUs<br><br>Windows使用基于优先级的抢占调度算法来调度线程。确保拥有最高级优先级的线程运行，而优先级又随着程序的执行发生变化，确保每个线程都能够获取CPU而避免饥饿问题。这个选择线程运行的程序在Windows中被称作 dispatcher。<br>如果高优先级的线程被unblocked，他就会抢占低优先级的线程。Windows有32种不同的优先级，其中包括regular(1-15)和实时类别(16-31)。优先级为0运行的是一个内存管理任务。Dispatcher在每种优先级中都维护一个队列。当没有任务运行的时候，系统的 Idle 进程就会运行。<br><img alt="Pasted image 20250112042240.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250112042240.jpg"><br>Windows将这些任务划分为六类优先级，分别是：<br>
<br>Realtime
<br>High
<br>Above Normal
<br>Normal (A process usually in this class)
<br>Below Normal
<br>Low
<br>当进程的时间片到达，线程执行就会被中断。如果任务是实时的，那么它的优先级就会被降低。<br>当一个进程被阻塞时，其优先级确实会暂时提升，以便在阻塞事件完成后能够更快地恢复执行。这个优先级提升的幅度取决于阻塞事件的类型。例如，等待键盘输入的进程会获得比等待磁盘操作的进程更大的优先级提升。<br>此外，为了提供更好的体验，系统还会为运行在前台(foreground)的进程提供额外的优先级。<br><br>Linux有两种调度模式：实时的和非实时的。即使你使用实时调度器，系统中仍然可以存有非实时线程被调度。Linux调度器根据不同的调度类(scheduling classes)来进行调度。Linux 2.6.23版本之前，根据优先级，Linux将系统分为以下三大类：<br>
<br>SCHED_FIFO: 先进先出的实时线程。
<br>SCHED_RR: 轮转的实时线程。
<br>SCHED_OTHER: 其他（非实时）的线程。（也叫普通调度类SCHED_NORMAL）
<br>其中，每个类别中又有许多不同的优先级。与Windows中优先级越高，数字越高不同。在Linux中，数字越低，优先级越高。实时优先级的范围从[0-99]，其他优先级的范围从[100-139]。也就是说，仅当RR或FIFO队列中没有可调度线程时，SCHED_OTHER才会被调度。<br>在2.6.23版本后，根据优先级，主要的调度类有下面五种：<br>
<br>SCHED_FIFO
<br>SCHED_RR
<br>CFS: 取代了SCHED_OTHER，成为当前Linux的默认调度类。
<br>SCHED_BATCH: 用于批处理任务，尽量减少对交互任务的
<br>SCHED_IDLE: 最低优先级的任务，只有系统空闲时才会执行。
<br><br><br>在FIFO类中，线程的调度有一些需要注意的规则。当以下其中一个条件被满足，系统就会中断当前正在运行的FIFO的线程：<br>
<br>更高优先级的FIFO线程准备好了。
<br>当前FIFO线程被阻塞。
<br>当前FIFO线程让出CPU。
<br>如果相同优先级队列中两个线程同时准备好了，那么等待事件更长的线程就会被选中。<br><br>RR类中的线程调度策略和FIFO一样，只不过多了时间片轮转调度。当时间片用完且这时线程还没有执行完毕。那么调度器就会中断当前线程并选择一个更高优先级或相同优先级的线程调度执行。如果此时的线程就是优先级最高的，那么就会有选择这个线程进行调度。<br><br><br>在 Linux 2.4 和更早期的版本中，Linux内核使用传统的算法进行非实时调度，造成的结果是时间复杂度很高(O(n))。之后，在2.6.23版本之前，引入了被称为O(1)调度器的调度算法，因为执行时间是一个常数时间(O(1))。O(1)调度器更好的适配了SMP系统，加入了CPU亲和性和负载均衡。<br>但之后在2.6.23版本，CFS(Completely Fair Scheduler)代替了O(1)调度器。CFS旨在提供一种更加公平和高效的调度机制，通过红黑树数据结构来管理任务，确保每个任务都能公平地获得CPU时间片，从而提高系统的整体性能和响应速度。<br>在6.6版本之后，默认的调度器被换为EEVDF。<br><br>Because the traditional scheduling algorithm is a linear time algorithm, the more processes the current system has, the worse the performance becomes. Therefore, with the traditional scheduler, you cannot effectively handle a very large number of processes.<br>Besides, it does not match with the SMP system and would incur a penalty due to its design:<br>
<br>A single run queue.<br>
我们之前提到过，每个CPU都有自己的run queue，这是为了CPU亲和性，提高cache的命中率。系统只维护一个run queue对于负载均衡当然是好事，但是cache就很容易被架空。
<br>A single run queue lock.<br>
系统只有一个run queue lock，这就意味着只用一个mutex lock来保护对run queue的操作。当一个进程想要enqueueing或dequeueing run queue时，其他进程就只能等待。
<br>An inability to preempt running process.<br>
当低优先级进程运行时，高优先级进程必须等待。并不支持优先级抢占。
<br><br>了解如上的传统调度器的问题，我们来学习一下O(1)调度器是如何解决这些问题的。采用O(1)调度器后，内核会为每个处理器（核心）分别维护一个prio_struct数据结构，记录着array中任务的数量、优先级位图和优先级队列。MAX_PRIO指的是最大的优先数，也就是139。<br>struct prio_array {
    unsigned int nr_active;                // numbers of tasks in the array
    DECLARE_BITMAP(bitmap, MAX_PRIO+1);    // priority bitmap, 0 for empty
    struct list_head queue[MAX_PRIO];      // priority queues
};
<br>我们知道，Linux中有140种不同的优先级，每个优先级都会维护一个任务队列。优先级位图中的一位bit表示该优先级队列中有任务存在。这样，我们在调度任务的时候就不用遍历整个queue了。<br><img alt="Pasted image 20250114004941.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250114004941.jpg"><br>在O(1)调度器中，任务会被分配到两个不同的优先级数组中，active queues和expired queues。调度器会从active数组中选择下一个要运行的任务。当一个任务用完了其时间片，就被移动到expired数组中。当active数组中的所有任务都用完了时间片，调度器会交换active和expired数组（交换指针），这样expired数组中的任务就会重新变为active，并且可以再次被调度执行。<br><br>CFS调度器是自2.6.23版本之后的内核默认调度器，由 Ingo Molnár 编写，旨在提供公平高效的调度，但可惜它的时间复杂度是O(log n)。CFS通过虚拟运行时间(vruntime)来衡量每个任务的运行时间，确保每个任务都能公平地获得CPU时间。CFS使用红黑树来建模管理就绪队列，最左边的组代表着有最小虚拟运行时间的任务，即优先级最高。获取执行最左边的组的时间复杂度为O(Iog n)。<br>当任务的时间片用完或被抢占，这个任务就会重新插入就绪队列，并更新其vruntime。这时红黑树就需要进行re-balancing，以确保自平衡的特性。<br><img alt="Pasted image 20250114013155.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20250114013155.png"><br>
CFS并不使用固定的时间片长度，而是采用了一个称为“目标延迟”（target latency）的概念。目标延迟就是一个time window，并期待所有的线程都能够在这个窗口内至少运行一次。也就是说，目标延迟越长，任务切换的频率就会越低（CPU的性能越好，目标延迟也会越短）。<br><br>vruntime，也就是virtual run time，是CFS跟踪任务执行时间的方式。从名字就能看来，vruntime并不等同于实际的运行时间，CFS会根据任务的优先级（nice）和最近的调度情况来调整虚拟运行时间。优先级较高的任务会有较小的虚拟运行时间，而优先级较低的任务会有较大的虚拟运行时间。<br>比方说，CFS有一个decay factor，最近调度过的任务其实际运行时间的权重会偏大，从而影响其虚拟运行时间。此外，nice值越大，vruntime也会偏离实际运行时间越大。一个nice值为0的线程的vruntime就等于实际的运行时间。<br><br><br>在2016年，有研究人员发表了 <a data-tooltip-position="top" aria-label="https://blog.acolyer.org/2016/04/26/the-linux-scheduler-a-decade-of-wasted-cores/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054" rel="noopener nofollow" class="external-link" title="1" href="https://blog.acolyer.org/2016/04/26/the-linux-scheduler-a-decade-of-wasted-cores/?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054" target="_blank">The Linux Scheduler: a Decade of Wasted Cores</a>。这篇文章由Jean-Pierre Lozi、Baptiste Lepers、Justin Funston、Fabien Gaud、Vivien Quéma和Alexandra Fedorova共同撰写，发表在EuroSys 2016的会议上（<a data-tooltip-position="top" aria-label="https://people.ece.ubc.ca/sasha/papers/eurosys16-final29.pdf?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054" rel="noopener nofollow" class="external-link" href="https://people.ece.ubc.ca/sasha/papers/eurosys16-final29.pdf?citationMarker=43dcd9a7-70db-4a1f-b0ae-981daa162054" target="_blank">click here</a>）。<br>在138次的测试中，研究人员发现这些bug导致了系统性能下降了13%到24%。那多核调度是如何造成如此幅度的系统性能下降的？文章中提到的四种问题，造成了同一种现象：即使系统中仍然存在有等待执行的线程，系统也并不调度这些现象，而是让核心长时间处在空闲状态。核心短时间没有相关线程调度是正常的，毕竟把线程从一个核心上push/pull到另一个核心需要时间。但是，要是线程无缘无故等待很长时间（几百毫秒），那将成为一个问题。<br><br>在有的系统里，每个核心会被分配一个run queue。如果一个核心上有一个低优先级的任务，而另一个核心上有三个高优先级队列运行，不难发现，高优先级的任务实际上运行的时间反而更短。为了使得调度更加的公平，Linux会周期性地检查这些队列，保证所有核心的负载均衡。这就是我们之前介绍过的负载均衡。<br>负载均衡意味着额外的性能支出，和一些对于核心而言重新学习的时间支出，负载均衡往往意味着low cache locality和non-uniform memory access，这些都需要代价。我举一个例子，当工厂中有5个人做着不一样的工作，为了使得工人们负载均衡，老板可以每隔一段时间查看一下这些工人负载的情况如何（老板也是一个核心）。老板巡查需要一定的开销，工人A一直做着工作A，而工作A完成了，工人A被调去帮助工人B，然而任务B是工人A之前没有接触过的，所以这就需要工人A先进行学习，再开始工作。<br>因此，内核将多个硬件有相互关系的核心进行统一管理，组成一个更大的单元/组，我们称之为scheduling domain。比方说如果四个核心共享一个L2 cache，那就将其组织成一个调度域。将任务在同一个scheduling domain中进行push/pull就减少了核心的“学习”成本。<br><br><br>我们将核心分组管理，然而，某个核心可能窃取其他核心的劳动成果。相当于三人组中总有摆烂的那个人。通过组内核心的平均负载来看，数据可能很漂亮，但是有可能某几个核心满负荷而剩下的核心处于空闲状态。“平均”具有误导性。<br>如何解决呢？我们评估组内最小负载作为我们的衡量指标，即干活最轻的那个核心负载如何决定整个小组的负荷情况。这种方法解决了运行时约13%的性能损失。<br><br>Linux中我们用taskset命令让某个任务持久的在某个核心上运行，如果调度组并没有按照硬件的相近性进行组织，那么调度组的创建将没有意义。组织这种错误的调度组是因为所有调度组的构建都是从核心0的视角出发进行的。然而这种构建方法跟硬件的拓扑和相似性并无关系。<br><br>我们之前了解过处理器亲和性，但有时候过度亲和并不是一件好事。当一个现象在组1的某个核心上被阻塞，当它被其他线程唤醒时，由于处理器的亲和性，它可能还会在之前运行过的核心上运行，如果核心正忙呢？那就等待。这和我们经常出入一个理发店很类似，可能宁愿等2小时也不愿意去其他理发店里理发。<br>经过处理器亲和性可能减少cache misses，但是要是其他核心空闲，有等待的时间可能在其他核心上早已结束运行。<br><br>导致一个应用上的所有线程都在一个核心上运行。]]></description><link>https://congzhi.wiki/congzhi's-os-series/9.-cpu-scheduling.html</link><guid isPermaLink="false">Congzhi's OS Series/9. CPU Scheduling.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 03 Jun 2025 09:15:42 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240528012310.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240528012310.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[10. Synchronization and Mutex]]></title><description><![CDATA[ 
 <br>第一遍重写中<br><br><br><br>在之前的学习中，我们了解到许多并发给我们带来的诸多好处，如更好的资源利用率、更低的响应时间、更好的用户体验。但在享受这些便利的同时，并发还会为系统带来许多灾难。在设计系统的并发时，我们要考虑资源共享的问题，不当的程序可能会为系统带来条件竞争(Race condition)、饥饿和死锁。这些问题一定要在开始就规避掉，不然会酿成大祸。<br><img alt="this_is_fine.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/this_is_fine.jpg"><br><br>同步指通过协调多个并发线程（或进程）的执行，是他们能够安全的共享资源或进行通信。同步的目的是防止竞争、数据不一致和死锁等问题，从而保证系统的正确性和稳定性。我们下面先举两个例子展示不同的同步模式。<br>在多核系统上，两个线程是有并行执行的能力的。但有时候，两个线程虽然可以同时执行，但是不可以同时访问某些资源，我们称这种情况为互相排斥的同步问题(mutual exclusion)。在之前的学习中，共享内存的访问就需要进行互斥的访问（虽然也需要serialization的同步）。<br>还有另一种情况，两个线程的执行必须要有一定的顺序，比如B线程需要A线程执行完毕后才能执行或是两个线程交替着执行亦或是更复杂的情况。我们一律将这种情况称作serialization的同步问题。同样的，我们之前学习管道的进程间通讯方式就需要进行serialization的同步。<br><br>我们很多写的简单测试程序都是确定性的程序，即我们在程序运行前我们就能知道输出是什么（比如简单的打印"Helloworld\n"）。但当程序涉及到并发时，我们的程序有时就不再是确定性的程序了，我们称之为不确定性程序(Non-deterministic program)。<br>下面的C++程序就是不确定性的程序：<br>#include &lt;iostream&gt;
#include &lt;thread&gt;

int counter = 0;

int main(){
	std::thread newThread([](){
        for (int i = 0; i &lt; 10000000; i++){
            counter++; 
        }   
    });
	for(int i = 0; i &lt; 10000000; i++){
		counter++;
	}
    newThread.join();
	printf("I counted: %d\n", counting);
	return 0;
}
<br>这个程序很奇怪，当你打印结果，你会发现它的输出千奇百怪，总之就是不为20000000。为什么？虽然看上去并不需要同步，但高级语言中的++、--不仅仅只是看上去那么简单。这些指令被称为read-modify-write指令，你需要先读值、修改最后写回结果。造成这种情况完全是因为read-modify-write指令并不是一气呵成的。（两线程都先读，之后两线程先后写，会造成什么情况？）<br>由这种不确定性引起的程序bug，我们将其戏称为Heisenbug（得名于 "Heisenberg Uncertainty Principle"）。这时，我们就需要对资源的操作进行 mutual exclusion 的同步操作。<br><br>我们现在明白，造成 Heisenbug 的主要原因在于资源的分配问题和执行顺序的控制问题。上边的程序中并发引起的 Heisenbug 就是由于单一资源分配不当导致的。当然，资源个数也可以有很多，这时要考虑的问题就会随着资源个数的增加而变得不同。<br>
<br>单一资源：如果资源只有一个，多个并发进程都想访问它，则需要排队一个一个地访问资源，确保在同一时刻只能有一个进程访问该资源。
<br>多个资源：如果资源有多个，需要合理地分配资源给并发进程，防止出现资源多分配或资源未分配的情况。
<br>对执行顺序的控制能够确保多个进程或线程按照预期的顺序执行，以满足特定的任务依赖关系和执行逻辑。对于这些非确定性程序，我们可以用一些同步工具来管理和控制资源或执行顺序。通过这些同步工具，我们能够有效防止竞态条件和资源冲突，确保系统的正确性和稳定性。但在此之前，我们有必要先了解一下临界区的管理。<br><br><br>在之前的示例中，我们发现程序的不确定性是由一条 read-modify-write 高级语言指令（如 i++;、i--;）引起的。为了避免数据竞争，我们只需要对这条指令进行管理即可。这也正是我们即将要学习的临界资源和临界区中所要关注的内容。<br><br><br>临界资源是指那些在同一时刻只能被一个进程访问的共享资源，如共享内存、文件、打印机及其他系统互斥资源。对临界资源的访问必须是不可中断的。前面引起Heisenbug的C++程序中，那个全局变量就是需要互斥访问的临界资源。<br><br>临界区就是并发线程（或进程）中访问了临界资源的一段代码。要使得系统运行稳定、规避并发带来的一系列问题，我们需要保证时刻只有一个进程在临界区内代码来避免出现竞争条件或是数据不一致现象。之前的例子中，counter++;就是对临界区的访问，这种情况下，我们的临界区可以划分的很小。<br>for(int i = 0; i &lt; 10000000; i++){
// Critical Section starts form here
	counter++;
// To here
}
<br><br>现实生活中，订票系统是我们身边较为常见的可能引起数据不一致的场景。我们通过 pthread 库给这个进程中生成了两个线程。我们共有两张票，两个线程都会 booking 一张票，因此我们期望进程执行后余票为0。但由于并发执行，两个线程在进入临界区时可能会导致数据的不一致性。<br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;

int ticket_amount = 2;

void* ticketAgent(void* arg){
	int t = ticketAmount;
	if (t &gt; 0){
		printf("One ticket sold\n");
		t--;
	}else{
		printf("Ticket sold out\n");
	}
	ticketAmount = t;
	pthread_exit(0);
}

int main(int argc, char const* agrv[]){
	pthread_t ticketAgent_tid[2];
	for(int i = 0; i &lt; 2; i++){
		pthread_create(ticketAgent_tid+i, NULL, ticketAgent, NULL);
	}
	for (int i = 0; i &lt; 2; i++){
		pthread_join(ticketAgent_tid[i], NULL);
	}
	sleep(1);
	printf("The left ticket is %d\n", ticketAmount);
	return 0;
}
<br>运行结果如下：<br><img alt="Pasted image 20240701005315.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240701005315.png"><br>代码执行后我们看到一些结果是正确的，还有一些执行中售出两张票但仍留了一张。这是因为并发线程交替执行”临界区“中的代码，在上面我们也提到了 -- 这种 read-modify-write 指令。如此，并发执行会引起 Heisenbug 。对于这种情况，我们提出临界区管理的原则。<br><br>管理临界区的本质就是要对访问临界区的并发进程进行同步，要实现的目标有互斥、前进和有限等待。通过这些目标，我们能够在不同的同步模式下管理临界区并避免竟态条件和资源冲突所带来的一系列问题。<br>
<br>互斥(Mutual Exclusion,Mutex)：同一时间只能有一个进程/线程进入临界区。
<br>前进(Progress)：进程/线程在持有锁的情况下才能执行临界区内的代码。
<br>有限等待(Bounded Waiting)：完成操作后释放锁，让其他线程有机会进入临界区。
<br><br>互斥锁是由操作系统提供的临界区同步工具，可以保证临界区的管理目标要求。互斥锁的上锁、解锁操作是原子化的，因而保证了每次只会有一个进程/线程进入临界区。互斥锁的同步模型如下：<br>acquireLock();
//+--------------------+
//|                    |
//|  critical section  |
//|                    |
//+--------------------+
releaseLock();
<br>在这段同步模型的伪代码中，我们满足了临界区管理的三大原则，即：<br>
<br>进入临界区前请求锁（互斥），如果成功则上锁并进入临界区（前进），否则等待
<br>离开临界区后释放锁，让其他进程有机会进入临界区（有限等待）
<br><br>下面是一些POSIX系统提供的一些互斥锁的系统调用。相关操作有：<br>#include &lt;pthread.h&gt;
pthread_mutex_t lock;
pthread_mutex_init(&amp;lock,NULL);
pthread_mutex_lock(&amp;lock);
pthread_mutex_unlock(&amp;lock);
pthread_mutex_destroy(&amp;lock);
<br>借助pthread库，我们就可以实现线程间的互斥。我们在 ticket booking 中加入互斥锁，如下：<br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;

int ticketAmout = 2; // 票的数量: 全局变量
pthread_mutex_t lock; // 定义互斥锁
void* ticketAgent(void* arg){
    pthread_mutex_lock(&amp;lock); // 上锁
	int t = ticketAmout;
	if (t &gt; 0){
		printf("One ticket sold\n");
		t--;
	}else{
		printf("Ticket sold out\n");
	}
	ticketAmout = t;
    pthread_mutex_unlock(&amp;lock); // 解锁
	pthread_exit(0);
}

int main(int argc, char const* agrv[]){
	pthread_t ticketAgent_tid[2];
    pthread_mutex_init(&amp;lock, NULL); // 锁的初始化
	for(int i = 0; i &lt; 2; i++){
		pthread_create(ticketAgent_tid+i, NULL, ticketAgent, NULL);
	}
	for (int i = 0; i &lt; 2; i++){
		pthread_join(ticketAgent_tid[i], NULL);
	}
	sleep(1);
	printf("The left ticket is %d\n", ticketAmout);
	return 0;
}
<br>观察输出，你会发现此时不会有资源共享问题导致的数据不一致性了。pthread库除了提供互斥锁，还有读写锁、自旋锁(Spin-lock) 等，这些都可以为我们临界区的管理提供同步的帮助。<br><br><br>从上节课的例子中，我们看到互斥锁的使用十分简单。借助 pthread 库，我们只需要定义并初始化一把锁，接着进行上锁、解锁和锁的摧毁操作即可。要实现互斥锁对临界区的管理，就需要满足互斥、前进、有限等待三个条件。本节课，带着对临界区的管理原则，我们来尝试实现一把锁。<br><br><br>在第一次尝试中，互斥锁并不满足互斥和有限等待的条件，因为测试和上锁并不是一气呵成的。因而，线程1和线程2可能同时通过测试并上锁，导致两个线程都认为自己获得了锁。此外，没有其他的条件分支，这个锁实际上并不能发挥其应有的作用。<br>bool mutex_lock = false;

lock(mutex_lock){
	if(mutex_lock == false){  // test
		mutex_lock = true;    // lock
	}
}
//+--------------------+
//|                    |
//|  critical section  |
//|                    |
//+--------------------+
unlock(mutex_lock){
	mutex = false;
}
<br><br>为了满足有限等待的条件，我们将判断嵌入到 while 循环中。如果线程2在进入循环时不满足  mutex_lock = false 的条件就会进入空循环，等待线程1解锁释放临界区资源。但是在第二次尝试中，测试和上锁仍然是分开的，不符合互斥的条件。<br>bool mutex_lock = false;

lock(mutex_lock){
	while(mutex_lock != true){  // test
		;
	}
	mutex_lock = true;          // lock
}
//+--------------------+
//|                    |
//|  critical section  |
//|                    |
//+--------------------+
unlock(mutex_lock){
	mutex_lock = false;
}
<br><br>为了使得测试和上锁操作连贯且不可打断，我们需要引入原子操作的概念。原子操作就是在执行过程中需要一气呵成、不会分割、不能也不会被中断的操作。原子操作也称为原语，一般由硬件实现或系统提供。无论是什么原子操作，需要实现其原子性就离不了硬件平台的支持。<br><br>硬件支持的原子操作是由处理器提供的一些指令，确保这些操作在执行时不会被其他操作中断。下面列举了几个常见的硬件原子操作指令的实现：<br>
<br>Compare-and-Swap（比较并交换）：这个指令比较内存位置的值与给定的值，如果匹配，则将其更新为新值。常用于无锁编程中的互斥操作。CAS 实现大致是这样的：
<br>// The atomic hardware instruction CAS would not be interrupted.
bool compare_and_swap(int* ptr, int old_val, int new_val){
	if(*ptr == old_val){
		*ptr = new_val;
		return true;
	} else {
		return false;
	}
}	
<br>
<br>Test-and-Set（测试并设置）：用于检查某个内存位置的值，并在检查的同时将其设置为新值。TS 指令的实现大致如下：
<br>// Same, wouldn't be interrupted
int test_and_set(int* ptr, int new_val){
	int old_val = *ptr;
	*ptr = new_val;
	return old_val;
}
<br>
<br>Fetch-and-Add（取回并增加）：这个指令从内存位置取回一个值，并将其增加一个给定的值。多用于计数器递增操作，比如实现线程安全的计数器。
<br>// Same...
int fetch_and_add(int* ptr, int value){
    int old_val = *ptr;
    *ptr += value;
    return old_val;
}
<br><br>系统提供的原子操作主要通过操作系统和编程语言的库函数实现，来确保在多线程环境下操作的安全性。以下是一些常见的方法：<br>
<br>Mutex（互斥锁）：互斥锁是一种用于保护临界区的机制，确保同一时间只有一个线程能够访问临界区代码。在POSIX线程库中，pthread_mutex_lock&nbsp;和&nbsp;pthread_mutex_unlock&nbsp;用于加锁和解锁。
<br>Spinlock（自旋锁）：自旋锁是一种简单的锁机制，线程在获取锁之前会一直循环检查锁的状态。常用于短时间持锁的情况，以减少线程上下文切换的开销。
<br>Atomic&nbsp;Variables（原子变量）：编程语言也提供原子变量和操作，保证变量的读写操作是原子的。例如，在C++中，可以使用 std::atomic 来定义原子变量，提供原子性的读写操作。
<br>需要注意的是，系统所提供的这些原子性操作通常是通过对硬件封装所得的。如果硬件层面不支持原子操作，系统很难在不借助这些底层支持的情况下实现真正的原子性。<br><br>用原子操作指令，我们能够完成一个如下的简易版 spinlock。<br>//未上锁:0
//已上锁:1
bool mutex = 0;

lock(mutex){
	while(compare_and_swap(mutex, 0, 1) == false){  //测试+上锁
	//while(test_and_set(mutex, 1) == 1){
		;
	}
}
//+--------------------+
//|                    |
//|  critical section  |
//|                    |
//+--------------------+
unlock(mutex){
	mutex = 0;
}
<br><br>忙式等待是指一个进程或线程在等待某个条件满足时，不断地循环检查该条件是否满足的技术，这个循环等待的过程仍然在消耗CPU资源。我们本节课前面的互斥锁正是应用了这种技术，这类锁也称作自旋锁(SpinLocks)。<br><br>若进程请求锁时发现锁已不可用，那就让该进程或线程睡觉(阻塞/等待状态)，这样就不会消耗CPU来执行等待。但是，临界区中的进程离开临界区时在解锁操作中要唤醒之前等待的进程(从等待状态迁移到就绪状态)。上节课我们用到的 pthread_mutex 就是这类锁，即阻塞锁。<br><br><br>上节课我们简单地实现了自己的自旋锁，在课程末尾，我们提到了阻塞锁。我们实现自旋锁时的忙等待白白浪费CPU资源，这不是我们想要的。这节课，我们就来学习互斥锁。并比较两者的差别。<br><br>POSIX 系统提供了pthread_mutex互斥锁来实现不同操作之间的互斥。我们之前也在课程中使用过pthread_mutex。当检查到互斥锁已经被其他线程所侵占时，线程就会阻塞自己，由于这种实现方式，互斥锁也被称作阻塞锁。<br>下面是POSIX thread库中所提供的互斥锁的系统调用。我们最熟悉的是lock和unlock的系统调用，简单明了。当我们使用pthread_mutex_lock()时，这个函数会先检查锁是否可用，若是不可用就阻塞自己，锁可用后会向阻塞线程发送信号。<br>int pthread_mutex_lock(pthread_mutex_t *mutex);
/* 
Parameters:
	1. mutex: Pointer to the mutex to lock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>如果你想实现一些条件分支，例如当锁不可用时怎么怎么办，pthread还提供trylock的逻辑。不同于mutex_lock，trylock并不会阻塞线程。当如果互斥锁已经被其他线程占有时trylock会返回错误码 EBUSY，表示锁当前不可用。<br>int pthread_mutex_trylock(pthread_mutex_t *mutex);
/* 
Parameters:
	1. mutex: Pointer to the mutex to try to lock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>下面是对锁的解锁操作。<br>int pthread_mutex_unlock(pthread_mutex_t *mutex);
/*
Parameters:
	1. mutex: Pointer to the mutex to unlock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>说完了加锁和解锁，要使锁可用，锁的初始化和销毁是绕不开的点。使用pthread_mutex_init()我们可用将锁初始化为几种不同的状态。如果状态参数设置为NULL，那么就会使用默认的mutex类型，即PTHREAD_MUTEX_DEFAULT，行为未被定义。此外，我们还有其他的几种行为，如下：<br>#include &lt;pthread.h&gt;

int pthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t *attr);
/* 
Parameters:
	1. mutex: Pointer to the mutex to initialize.
	2. attr: Pointer to a mutex attributes object, or NULL for default attributes.
	   - PTHREAD_MUTEX_NORMAL: Mutex relocking is not allowed,  will cause deadlock.
	   - PTHREAD_MUTEX_ERRORCHECK: Mutex relocking will return an error.
	   - PTHREAD_MUTEX_RECURSIVE: Allowing the same mutex relock multiple times.
	   - PTHREAD_MUTEX_DEFAULT: Default mutex, behavior undefined.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_mutex_destroy(pthread_mutex_t *mutex);
/* 
Parameters:
	1. mutex: Pointer to the mutex to destroy.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>一旦调用 pthread_mutex_destroy() 销毁了一个互斥锁，该互斥锁对象就不能再被使用了。销毁互斥锁会释放与该互斥锁相关的所有资源。如果在销毁后尝试使用该互斥锁，可能会导致未定义行为，甚至程序崩溃。<br><br>int pthread_mutex_lock(pthread_mutex_t *mutex){
	//尝试使用CAS操作来获取锁
	if(atomic_compare_and_swap(&amp;mutex-&gt;lock,0,1)){  //这是一个快速路径
		//成功获取锁
		return 0;
	}
	//锁已经被其他执行流占有，进入慢速路径(阻塞自己，省略实现)
	return -1;
}
<br><br>POSIX 系统也提供了 pthread_spinlock 自旋锁来实现不同操作之间的互斥。我们前面实现过自己的一个自旋锁。自旋锁在检查到锁已经被其他线程占用时，不会阻塞线程，而是会在一个循环中不断检查锁的状态，直到锁可用。下面是 POSIX thread 库中所提供的自旋锁的系统调用：<br>#include &lt;pthread.h&gt;

int pthread_spin_init(pthread_spinlock_t *lock, int pshared);
/* 
Parameters:
	1. lock: Pointer to the spinlock to initialize.
	2. pshared: If non-zero, the spinlock is shared between processes; if zero, it is shared between threads of the same process.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_spin_destroy(pthread_spinlock_t *lock);
/* 
Parameters:
	1. lock: Pointer to the spinlock to destroy.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_spin_lock(pthread_spinlock_t *lock);
/* 
Parameters:
	1. lock: Pointer to the spinlock to lock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_spin_trylock(pthread_spinlock_t *lock);
/* 
Parameters:
	1. lock: Pointer to the spinlock to try to lock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_spin_unlock(pthread_spinlock_t *lock);
/* 
Parameters:
	1. lock: Pointer to the spinlock to unlock.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>自旋锁的系统调用和阻塞锁的系统调用很类似，唯一的差别就在于阻塞还是忙等待。你可能注意到初始化函数也有所不同，但是你实际上可以通过下面这种方式设置互斥锁的状态：<br>pthread_mutex_t mutex;
pthread_mutexattr_t attr;

pthread_mutexattr_init(&amp;attr);
pthread_mutexattr_setpshared(&amp;attr, PTHREAD_PROCESS_SHARED);
pthread_mutex_init(&amp;mutex, &amp;attr);
<br>下面是其函数原型：<br>int pthread_mutexattr_setpshared(pthread_mutexattr_t *attr, int pshared);
/* 
Parameters:
	1. attr: Pointer to the mutex attributes object.
	2. pshared: The process-shared attribute. It can be set to:
	   - PTHREAD_PROCESS_SHARED: The mutex can be shared between processes.
	   - PTHREAD_PROCESS_PRIVATE: The mutex is shared only between threads of the same process.

Return value: Returns 0 on success, otherwise an error number.
*/
<br><br><br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;
#include &lt;time.h&gt;

#define NUM_ITERATIONS 100000

pthread_mutex_t mutex;
int counter = 0;

void* mutex_thread_func(void* arg){
	for (int i = 0; i &lt; NUM_ITERATIONS; ++i)
	{
		pthread_mutex_lock(&amp;mutex);			//Get mutex lock
		counter++;
		for (int j = 0; j &lt; 10000; ++j);	//Increase system load
		pthread_mutex_unlock(&amp;mutex);		//Release the lock
	}
	return NULL;
}

int main(int argc, char const *argv[])
{
	pthread_t t1,t2;
	struct timespec start, end;

	pthread_mutex_init(&amp;mutex, NULL);		//Initialize the lock

	clock_gettime(CLOCK_MONOTONIC, &amp;start);	//Start time counting

	pthread_create(&amp;t1, NULL, mutex_thread_func, NULL);
	pthread_create(&amp;t2, NULL, mutex_thread_func, NULL);

	pthread_join(t2, NULL);
	pthread_join(t1, NULL);

	clock_gettime(CLOCK_MONOTONIC, &amp;end);	//Finish time counting

	pthread_mutex_destroy(&amp;mutex);			//Destroy the lock

	double elapsed = (end.tv_sec - start.tv_sec) 
				   + (end.tv_nsec - start.tv_nsec) / 1e9;

	printf("Final counter value with pthread_mutex_t: %d\n",counter);
	printf("Elapsed time with pthread_mutex_t: %f\n",elapsed);

	return 0;
}
<br><br>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;
#include &lt;time.h&gt;

#define NUM_ITERATIONS 100000

pthread_spinlock_t spinlock;
int counter = 0;

void* mutex_thread_func(void* arg){
	for (int i = 0; i &lt; NUM_ITERATIONS; ++i)
	{
		pthread_spin_lock(&amp;spinlock);		//Get spin lock
		counter++;
		for (int j = 0; j &lt; 10000; ++j);	//Increase system load
		pthread_spin_unlock(&amp;spinlock);		//Release the lock
	}
	return NULL;
}

int main(int argc, char const *argv[])
{
	pthread_t t1,t2;
	struct timespec start, end;

	pthread_spin_init(&amp;spinlock, PTHREAD_PROCESS_PRIVATE);		//Initialize the lock

	clock_gettime(CLOCK_MONOTONIC, &amp;start);	//Start time counting

	pthread_create(&amp;t1, NULL, mutex_thread_func, NULL);
	pthread_create(&amp;t2, NULL, mutex_thread_func, NULL);

	pthread_join(t2, NULL);
	pthread_join(t1, NULL);

	clock_gettime(CLOCK_MONOTONIC, &amp;end);	//Finish time counting

	pthread_spin_destroy(&amp;spinlock);		//Destroy the lock

	double elapsed = (end.tv_sec - start.tv_sec) 
				   + (end.tv_nsec - start.tv_nsec) / 1e9;

	printf("Final counter value with pthread_spinlock_t: %d\n",counter);
	printf("Elapsed time with pthread_spinlock_t: %f\n",elapsed);

	return 0;
}
<br><br>运行的结果因机器而异，但这两种锁的实现方式决定了它们的特点。对于互斥锁而言，线程/进程切换的开销是其主要的开销；而对于自旋锁，忙等待让CPU空转是其主要的开销。我们知道，进程/线程的切换需要保存上下文信息，这需要一定的代价，从而，两种锁的比较就变成了空转的时间更长还是上下文切换的时间更长。<br>不难想象，线程占有锁的时间越长，相比忙等待，线程切换的系统开销占比就会越小，这时mutex显然更优。但由于空循环忙等待的开销占比基本一成不变，当短时间持有锁时则 spinlock 更优。<br><br><br><br>我们学过了临界区的管理原则有：互斥、前进、有限等待。在原子操作和互斥锁工具还没有被应用的时代，Dekker算法和Peterson算法就是当是两种为了解决临界区管理问题所应用的算法。<br><br>Peterson算法是软件解决管理临界区问题经典算法。它利用线程间访问共享资源的方式实现进程/线程间互斥访问临界区的算法。<br>Peterson算法的伪代码如下：<br>//Peterson算法的变量
bool flag[2] = {false,false};  //flag[i]表示第i个线程/进程有进入临界区的意愿
int turn;                      //turn的值j表示现在第j个值有进入临界区的令牌

/*
当flag[i] &amp;&amp; turn == i,这时只有第i个进程才能进入临界区。实现了对临界区的管理
*/
<br>P0进程：<br>flag[0] = true;  //P0a
turn = 1;        //P0b
while(flag[1] &amp;&amp; turn == 1){
	//busy wait
}
//start of critical section
...
//end of critical section

flag[0] = false;               //unlock
<br>P1进程：<br>flag[1] = true;  //P1a
turn = 0;        //P1b
while(flag[0] &amp;&amp; turn == 0){
	//busy wait
}
//start of critical section
...
//end of critical section

flag[1] = false;               //unlock
<br><br>在算法执行时，由于进程的并发性，语句执行顺序及结构可能出现如下几种可能：<br><br><br>Peterson 算法是一种不依赖硬件实现的原子操作机制。如今大多数CPU以指令乱序执行来提高执行效率，此时实现 Peterson 算法就得使用相关内存屏障指令。现在一般使用硬件支持的原子操作机制（比如 test-and-set 或 compare-and-swap），这些机制往往只需要很少的硬件支持。<br><br>和 Peterson 算法一样，Dekker 算法也是用进程间共享资源的方式实现进程间的互斥操作。同年 Dijkstra 提出信号量的概念也是受 Dekker 算法的影响。<br>Dekker 算法的伪代码如下：<br>// Dekker算法的变量
bool wants_to_enter[2] = {false, false}; // 表示进程是否想要进入临界区
int turn = 0; // 表示当前的优先权
<br>进程P0：<br>// 进程P0
wants_to_enter[0] = true;    //P0a
while (wants_to_enter[1]) {
    if (turn != 0) {
        wants_to_enter[0] = false;
        while (turn != 0) {
            // 忙等待
        }
        wants_to_enter[0] = true;
    }
}
// 临界区
...
turn = 1;
wants_to_enter[0] = false;
// 剩余部分
<br>进程P1：<br>// 进程P1
wants_to_enter[1] = true;    //P1a
while (wants_to_enter[0]) {
    if (turn != 1) {
        wants_to_enter[1] = false;
        while (turn != 1) {
            // 忙等待
        }
        wants_to_enter[1] = true;
    }
}
// 临界区
...
turn = 0;
wants_to_enter[1] = false;
// 剩余部分
<br><br><br><br><br><br>信号量概念最早由荷兰计算机科学家Edsger W. Dijkstra在1965年提出，是用与多线程环境下同步和互斥的一种机制。这个机制包含一个值和两个原子操作：<br>
<br>信号量值：一个整数值
<br>P操作：荷兰语 Proberen，意为“测试”，也称为 wait 操作。
<br>V操作：荷兰语 Verhogen，意为”增加“，也称为 post/signal 操作。
<br>信号量是一种比互斥锁更强大的同步工具，它的引入极大地简化了复杂的并发控制问题，使得操作系统和并发编程中的资源管理变得更有效和可靠。<br><br><br>如果信号量只包含数值，且在数值不可用时实施忙式等待，那么就成这对PV操作为自旋PV操作。（也叫”整数信号量“）<br>semaphore sem;
wait(sem){
	while(s &lt;= 0){
		//busy waiting
	}
	s--;
}
post(sem){
	s++;
}
<br><br>要消除自旋等待，我们可以把信号量打造成一个包含值和队列指针的结构体。具体思路如下：我们在执行P操作时，先判断数值是否可用，若不可用，利用进程指针将进程给阻塞掉。等到资源可用后，V操作在阻塞队列中再一个个地唤醒阻塞进程。这就是阻塞PV操作，也叫记录型信号量。<br>typedef struct{
	int value;
	struct process* L;
}semaphore;
semaphore sem;
wait(sem){
	s.value
	if(s.value &lt; 0){
		block(s.L);
	}
}

post(sem){
	s.value++;
	if(s.value &lt;= 0){
		wakeup(P);
	}
}
<br><br>我们假设有一个信号量s，它的初始值为1，我们还有3个并发进程 P1、P2、P3 都要对s先进行P操作，再进行V操作，下面是其中一种可能的顺序：<br><br><br><br>从之前互斥锁实现中，我们已经学会如何使用“锁”来管理临界区，我们使用信号量和PV操作同样可以完成临界区管理的任务。以下我们给出一段伪代码：<br>semaphore mutex = 1;

Thread n:
	//其他代码
	wait(mutex);    //相当于lock
	//+--------------------+
	//|                    |
	//|  critical section  |
	//|                    |
	//+--------------------+
	post(mutex);    //相当于unlock
	//其他代码
<br>结合上面的伪代码，很容易想明白当有多个进程并发地要访问临界区时，只有第一个执行P操作的进程能够进入临界区，并抢占这里的唯一的信号量资源。其他进程由于没有资源可以使用，因此只能自旋或阻塞等待释放临界区资源。<br>
<br>初始状态：mutex = 1，临界区空闲，任何进程可以进入临界区。
<br>进程执行：某个进程执行了P(mutex)操作。信号量mutex减1变成0，此时临界区被占用
<br>线程离开临界区：返还临界区资源，mutex加1。其他进程可以进入临界区。
<br>这种实现互斥的信号量初值总是1，它的值总是在0和1之间变化，因此被称作二值信号量。<br><br>我们知道，在计算机世界中由于并发进程执行是异步的。如果我们想要多个进程按照某个顺序执行要怎么办呢？我们可以利用PV操作实现这多个进程之间的同步。<br>假设我们现在有两个进程P1和P2，P2需要在P1完成某些操作后才能继续执行任务，我们可以先定义一个名为sync的信号量，如下：<br>semaphore sync = 0;    //初始化为0，表示P2需要等待

P1:
	//其他代码
	//执行P2需要等待的操作
	post(sync);
	//其他代码
P2:
	//其他代码
	wait(sync);    //等待P1相关操作执行完毕
	//继续执行
	//其他代码
<br>通过上面的伪代码，我们不难看出，即使机器中的代码是异步执行的，但是P2相关操作还是可以同步等待P1相关操作的完成。不论大厨炒菜多么快速，他都需要等待菜全部切好才能开始炒菜，不然就可能导致不好的事情出现。<br>这个例子中，不论是P1先执行还是P2先执行，没有P1进行V操作，P2就无法执行P操作从而继续执行其他代码。从这个例子中也能明白，我们同步的目标是保证P2在P1完成相关操作后才能继续执行。<br><br>我们已经学习过如何用信号量实现同步和互斥，但是我们仍然有一种情况没有考虑到。如果在多进程系统中，并发的进程要同时访问共享资源呢？我们考虑以下几种情况：<br>
<br>某类管理资源在同一时刻只允许一个进程使用（二值信号量实现的临界区互斥管理）
<br>对文件或数据库资源，可以某一时刻有多个进程同时读取甚至写入。
<br>资源较多，系统可以按进程的需求进行分配。
<br>我们已经学过第一种情况的管理机制，那么底下的2、3情况呢？我们需要一种机制来防止资源竞争来确保系统的稳定性。<br><br>计数信号量的初始值是一个非负整数，用于表示多个相同资源的可用数量。从这里，我们能看出二值信号量实际上是计数信号量的特殊情况，其中计数信号量的值只能是0或1。引入计数信号量扩展了对临界区资源数量大于1的情况的处理能力，使得多个线程可以同时访问多个相同的资源。<br>例：假设系统中有3台打印机，多个进程都需要使用这些打印机，我们可以使用一个计数信号量来管理这3台打印机的申请和释放。<br>semaphore printer = 3;

Process n:
	//其他代码
	wait(printer);
	//+-----------------+
	//|                 |
	//|  using printer  |
	//|                 |
	//+-----------------+
	post(printer);
	//其他代码
<br>
<br>初始化信号量：信号量的初始值为3，表示系统中有3台可用的打印机。
<br>当进程需要使用打印机时，执行P操作。P操作会检查信号量的当前值，如果值大于0，则表示有可用的打印机，信号量值减1，进程获得使用打印机的权限。如果信号量值为0，表示没有可用的打印机，进程将等待，直到有打印机可用。
<br>当进程使用完打印机后，执行V操作。V操作会增加信号量的值，表示释放了一台打印机。如果有等待的进程，V操作将唤醒其中一个进程，使其可以继续执行。
<br>进程数量：当进程数量 ≤ 3 时，所有进程都可以同时使用打印机；当进程数量 &gt; 3 时，只有3个进程可以同时使用打印机，其他进程需要等待。
<br><br><br><br>POSIX标准提供了几个用于信号量（semaphore）的系统调用，这些调用可以用于进程或线程之间的同步。以下是几个主要的POSIX信号量系统调用：<br><br>#include &lt;semaphore.h&gt;

int sem_init(sem_t *sem, int pshared, unsigned int value);
/* 
Parameters:
	1. sem: Pointer to the semaphore to initialize.
	2. pshared: If non-zero, the semaphore is shared between processes; if zero, it is shared between threads of the same process.
	3. value: Initial value of the semaphore.

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br>int sem_destroy(sem_t *sem);
/* 
Parameters:
	1. sem: Pointer to the semaphore to destroy.

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br>int sem_wait(sem_t *sem);
/* 
Parameters:
	1. sem: Pointer to the semaphore to decrement (wait).

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br>int sem_trywait(sem_t *sem);
/* 
Parameters:
	1. sem: Pointer to the semaphore to decrement (try to wait).

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br>int sem_post(sem_t *sem);
/* 
Parameters:
	1. sem: Pointer to the semaphore to increment (signal).

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br>int sem_getvalue(sem_t *sem, int *sval);
/* 
Parameters:
	1. sem: Pointer to the semaphore.
	2. sval: Pointer to an integer to store the current value of the semaphore.

Return value: Returns 0 on success, -1 on failure and sets errno appropriately.
*/
<br><br><br><br><br>之前我们学习二值信号量实现的同步中，我们实际上就已经接触了signaling的同步模式，相当于一个进程/线程对另一个进程/线程的等待。我们另外举一个例子，假如我们有信号量sem，初始化为0。通过信号量的PV操作，我们就能实现两个进程/线程之间的同步。<br>Thread A<br>Statement A1;
post(sem);
<br>Thread B<br>wait(sem);
Statement B1;
<br>上面的例子中，如果B线程开始执行，由于Statement A1并没有完成（信号量没有被post），所以线程B会被阻塞，等待信号量的释放。所以只要线程A调用post(sem)之后，线程B才会开始执行。<br><br><br>Rendezvous的同步模式是一种双向的同步模式，它相当于是对signaling的拓展。与signaling不同，rendezvous不仅仅是一个进程/线程在等待另一个进程/线程的信号，而是两个进程/线程彼此都在等待对方。因而，在这种同步模式中，我们需要两个信号量sem1和sem2并初始化为0。<br>Thread A<br>Statement A1;
post(sem1);
wait(sem2);
Statement A2;
<br>Thread B<br>Statement B1;
post(sem2);
wait(sem1);
Statement B2;
<br>上述的例子中，只有当线程A和线程B都走完Statement A1和Statement B1时，下一阶段的任务才会开始。这种双向同步确保了两个线程在特定的同步点上达成一致，然后才能继续各自的任务。<br><br>问题来了，如果线程的数量越来越多，会发生什么？如果线程数量不断增加，会导致同步和管理的复杂性显著增加。为了避免死锁，必须确保PV操作平衡，即每个post操作应有相应的wait操作。<br>在有三个线程（A、B、C）的情况下，可以如下进行rendezvous的同步：<br>Thread A<br>Statement A1;
post(sem1);
post(sem1);
wait(sem2);
wait(sem2);
Statement A2;
<br>Thread B<br>Statement B1;
post(sem2);
post(sem2);
wait(sem1);
wait(sem3);
Statement B2;
<br>Thread C<br>Statement C1;
post(sem3);
post(sem3);
wait(sem1);
wait(sem2);
Statement C2;
<br>这种方式中，每个线程在执行完自己的第一部分工作后（Statement&nbsp;A1,&nbsp;B1,&nbsp;C1），分别post一个信号量并wait两个信号量。每个线程都在等待另外两个线程的信号，从而确保所有线程都在同步点相遇后才能继续执行剩余工作。<br>虽然这种设计可以防止死锁，但随线程数量增加，同步复杂度和信号量的管理也会增加。为了简化管理和实现，我们可以使用一些更高级的同步原语或模式，我们将介绍的下一个同步模式屏障(barrier)就是为此而生的。<br><br>屏障是多线程编程中用于同步多个线程执行进度的设计模式。它的核心思想是：强制所有的线程在某个预定义的点等待，直到所有线程都到达该点后，才能继续执行后续操作。比如在下面的伪代码中，我们有 N 个参与屏障同步的线程。<br>每个线程到达时，count 都会 + 1。但最开始，这些线程会被阻塞掉，直到所有的 n 个线程都已就绪。随后我们有信号量 barrier 来控制线程的阻塞和唤醒。<br>Thread N<br>wait(mutex)
    count++
post(mutex)
// At this point, every thread will block here until count == n
if count == n
    for i from 1 to n
        post(barrier) // Unblock all threads waiting on the barrier
    end for
end if
wait(barrier) // Each thread waits on the barrier until it is released
<br>你可以在多人参与的游戏中看到这种设计模式。当你买了一个新手机后，你可能会抱怨某个玩家真的应该换一部手机了，怎么那么慢。比如下面的蓝方吕布玩家，他真的应该换一台设备了。<br><img alt="moba_game_begin.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/moba_game_begin.jpg"><br><br>Turnstile&nbsp;pattern&nbsp;是一种设计模式，主要用于在并发编程中控制对共享资源的访问。它像个旋转门一样，只允许一个线程在特定时间内通过，从而确保多个线程之间能够有序地进行协调。<br>条件变量和&nbsp;turnstile&nbsp;pattern&nbsp;是相关的。在&nbsp;turnstile&nbsp;pattern&nbsp;中，条件变量可以用来管理线程的等待和唤醒。当资源不可用时，线程会被阻塞并放入条件变量中。当资源可用时，通过条件变量发出信号，唤醒等待的线程，使其继续执行。<br><br><br><br>生产者-消费者问题描述了两个进程，一个是生成数据的生产者进程，一个是负责消费数据的消费者，两者通过共享缓冲区进行通信。由于问题描述的固定大小的缓冲区，也被称有限缓冲问题。核心挑战在于如何有效地管理缓冲区，确保生产者不会在缓冲区满时继续生产，也确保消费者不会在缓冲区空时继续消费。这个问题有很多变体，例如我们可以设置 x 个生产者， y 个消费者之类的。<br><br>在生产者-消费者问题中，有许多规则需要双方遵守的。当我们设置缓冲区大小为BUFFER_SIZE。我们规定：<br>
<br>当缓冲区为空时，消费者不可以从缓冲区中读取信息。
<br>当缓冲区满了后，生产者也不可以在缓冲区写入任何东西。
<br>对于缓冲区的操作，我们需要确保其是互斥访问的，以免导致数据竞争问题。
<br><br>通过这三条规则，我们可以用以下的伪代码对生产-消费的过程进行模拟。下面是一个忙等待加互斥的例子，我们用互斥锁对临界区进行了保护（缓冲区相关操作）。如果缓冲区满，生产者会一直查看缓冲区是否有空隙；若是缓冲区空，消费者也会一直查看缓冲区是否有可读信息。<br>Producer<br>added = false;
while(added = false){
	wait(mutex);
	if(count &lt; BUFFER_SIZE){
		// Add item.
		count++;
		added = true;
	}
	post(mutex);
}
<br>Consumer<br>removed = false;
while(removed = false){
	wait(mutex);
	if(count &gt; 0){
		// Remove item.
		count--;
		removed = true;
	}
	post(mutex);
}
<br><br>我们之前比较过互斥锁和自旋锁的优劣，如果我们要长时间持有锁，忙等待显然会一直白白浪费资源。这种情况下，检查一次缓冲区，如果不满足相应的条件就直接阻塞进程/线程明显是更好的办法。使用锁好像效益来的不再可观，我们需要使用其他的工具。<br>我们可以使用两个信号量，他们的最大值都是BUFFER_SIZE。两个信号量的描述如下：<br>
<br>items信号量：从0开始，表示缓冲区内现有多少可读的数据。
<br>spaces信号量：从BUFFER_SIZE开始，表示缓冲区还有多大的可用空间。
<br>下面的伪代码中，mutex用于确保对缓冲区的访问是互斥的，从而避免了数据竞争问题。spaces和items信号量则用于控制缓冲区的容量和可读数据量。这样可以确保生产者和消费者在操作缓冲区时遵守规则，从而实现同步互斥。<br>Producer<br>wait(mutex);
wait(spaces);
// Add item.
post(items);
post(mutex);
<br>Consumer<br>wait(mutex);
wait(items);
// Remove item.
post(spaces);
post(mutex);
<br>这个示例会有什么问题？我们的确保证了对缓冲区的互斥访问。但是如果缓冲区满时，生产者率先进入临界区，生产者会阻塞在wait(spaces);。因为缓冲区满了，需要消费者进入缓冲区消耗资源但是由于生产者持有互斥锁，消费者无法进入缓冲区消耗资源。这就会导致死锁(Deadlock) 的发生。如果缓冲区为空，消费者先进入临界区也会导致类似的死锁问题。<br><br>我们对伪代码进行一些调整，生产者和消费者在等待spaces和items信号量时不再持有互斥锁，从而避免了死锁的发生。从而生产者和消费者可以在缓冲区满或空的情况下正确地等待和释放资源。<br>Producer<br>wait(spaces);
wait(mutex);
// Add item.
post(mutex);
post(items);
<br>Consumer<br>wait(items);
wait(mutex);
// Remove item.
post(mutex);
post(spaces);
<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;pthread.h&gt;
#include &lt;semaphore.h&gt;

#define BUFFER_SIZE 20
#define PRODUCER_NUMBER 10
#define CONSUMER_NUMBER 10

int* buffer;
int pindex = 0;
int cindex = 0;
sem_t spaces;
sem_t items;
pthread_mutex_t mutex;
unsigned int seed = 252;

int produce(int id){
    int r = rand_r(&amp;seed); // rand_r for thread safe.
    printf("Producer: %d\tproduced: %d.\n", id, r);
    return r;
}

void consume(int id, int value){
    printf("Consumer: %d\tconsumed %d.\n", id, value);
}

void* producer(void* arg){
    int* id = (int*) arg;
    for(int counter = 0; counter &lt; 100; ++counter){
        int num = produce(*id);
        sem_wait(&amp;spaces);
        pthread_mutex_lock(&amp;mutex);
        buffer[pindex] = num;
        pindex = (pindex + 1) % BUFFER_SIZE;
        pthread_mutex_unlock(&amp;mutex);
        sem_post(&amp;items);
    }
    free(arg);
    pthread_exit(NULL);
}

void* consumer(void* arg){
    int* id = (int*) arg;
    for(int counter = 0; counter &lt; 100; ++counter){
        sem_wait(&amp;items);
        pthread_mutex_lock(&amp;mutex);
        int num = buffer[cindex];
        buffer[cindex] = -1;
        cindex = (cindex + 1) % BUFFER_SIZE;
        pthread_mutex_unlock(&amp;mutex);
        sem_post(&amp;spaces);
        consume(*id, num);
    }
    free(id);
    pthread_exit(NULL);
}

int main(int argc, char** argv){
    buffer = malloc(BUFFER_SIZE * sizeof(int));
    for(int i = 0; i &lt; BUFFER_SIZE; i++){
        buffer[i] = -1;
    }
    sem_init(&amp;spaces, 0, BUFFER_SIZE);
    sem_init(&amp;items, 0, 0);
    pthread_mutex_init(&amp;mutex, NULL);

    pthread_t producer_thread[PRODUCER_NUMBER];
    pthread_t consumer_thread[CONSUMER_NUMBER];

    for(int i = 0; i &lt; PRODUCER_NUMBER; i++){
        int* id = malloc(sizeof(int));
        *id = i;
        pthread_create(&amp;producer_thread[i], NULL, producer, id);
    }
    for(int j = 0; j &lt; CONSUMER_NUMBER; j++){
        int* id = malloc(sizeof(int));
        *id = j;
        pthread_create(&amp;consumer_thread[j], NULL, consumer, id);
    }
    for(int k = 0; k &lt; PRODUCER_NUMBER; k++){
        pthread_join(producer_thread[k], NULL);
    }
    for(int k = 0; k &lt; CONSUMER_NUMBER; k++){
        pthread_join(consumer_thread[k], NULL);
    }

    free(buffer);
    sem_destroy(&amp;spaces);
    sem_destroy(&amp;items);
    pthread_mutex_destroy(&amp;mutex);
    pthread_exit(0);
}
<br><br><br>读者写者问题描述了多个读者和写者对共享数据的访问。读者-写者问题与生产者-消费者问题相似而又不同。相似的是两个问题中都有数据的输入（生产者/写者）和输出方（消费者/读者），而且生产者/写者修改共享数据时不能有其他线程访问共享数据。<br>不同的是，读者-写者问题中的输出方（读者）并不涉及到对数据的修改操作（do not modify），这就意味着多个读者在读取数据的同时不会引起冲突。而且很多现实问题中写入很稀有但是读操作非常常见，允许缓冲区中多个读者读数据实际上可以提升很多性能。<br><br>现在我们考虑第一种情况，即一个写者对应着多个读者。假设读和写的操作都在一个房间中进行，即进行读写操作的房间实际上是我们的临界区。因而我们需要一个二元信号量roomEmpty对临界区进行管理。在读者进入临界区时，我们不想计算读者的数量时出现数据竞争的问题，所以我们使用mutex让读者一个一个地进入临界区。<br>当写者要进入临界区中时，我们需要保证临界区中没有读者存在，在solution-1中，我们用以下的伪代码表示写者的行为：<br>Writer<br>wait(roomEmpty);
// Write something
post(roomEmpty);
<br>我们提到过，在读者-写者问题中，临界区中可以存在多个读者。因此我们在solution-1的伪代码中给出如下的读者行为：<br>Reader<br>wait(mutex);
readers++;
if(readers == 1){
	wait(roomEmpty);
}
post(mutex);
// Read data.
wait(mutex);
readers--;
if(readers == 0){
	post(roomEmpty);
}
post(mutex);
<br><br>在solution-1中写者的行为逻辑简单又清晰，等待房间里没有读者了，写者进入房间中进行写操作。而由于可能会有很多读者，读者在进出房间时会挨个登记（wait(mutex) 和 post(mutex)），如果是第一个进入房间的，就负责标识房间已被占用（wait(roomEmpty)），最后一个出房间就负责标识房间空闲（post(roomEmpty)。<br>读者的这种行为模式叫做light switch pattern（先进入房间的人开灯，最后一个离开房间的人关灯）。在solution-1中，读者的行为会为写者带来很多困扰。比如读者占用房间后，后来的读者可以随意进出，只要保证最后一个离开房间的写者离开时释放房间资源就可以了。但是，你没有办法知道后面究竟有多少读者要读。在solution-1中，写者只能干等，这会导致写者长时间得不到资源的问题，也就是饥饿(Starvation)：which means a thread may never gets a chance to run.<br>写者的饥饿可能对博客这种读写问题的影响不会很大，但是对那些对实时性有要求的系统的影响尤其大（比如数据库的读写问题）。<br><br>为了避免写者的饥饿问题，我们需要另辟蹊径，重新找一个方法。我们试想一下solution-2的描述：当写者到达，已经在房间里的读者们继续阅读，但是后来者就不可以进入临界区阅读数据了。在solution-2的假设下，写者只需要等待临界区中读者读完数据走出临界区，不需要担心无休止到来的读者。<br>Solution-2我们引入了另一个二元信号量turnstile。相当于对“门”进行控制，无论是读者还是写者，要进入房间（临界区）必须通过“门”。通过占有“门”资源，读者和写者就能顺序地进入临界区。Solution-2中的写者行为如下：<br>Writer<br>wait(turnstile);
wait(roomEmpty);
// Write data.
post(roomEmpty);
post(trunstile);
<br>Reader<br>wait(turnstile);
post(turnstile);
wait(mutex);
readers++;
if(readers == 1){
	wait(roomEmpty);
}
post(mutex);
// Read data.
wait(mutex);
readers--;
if(readers == 0){
	post(roomEmpty);
}
post(mutex);
<br><br>虽然我们成功地使写者免受了饥饿的困扰，但是写者仍然不对临界区拥有任何特权。想想看，如果前面仍然有许多的读者在写者前面排队等待进入临界区，写者就仍然需要队列前面的所有读者进入临界区-读取-出临界区后才能对临界区资源进行操作。<br>Solution-2仍然不够实时，怎么办？我们需要划分优先级，使得写者相对读者总是享有有限进入临界区的特权。可以通过将信号量roomEmpty划分为两个信号量：noReaders 和 noWriters 来实现。通过让写者持有noReaders信号量来控制读者进入临界区。<br><br>在Solution-3中，我们引入优先级机制，写者可以相对读者享有优先权。第一个写者会等待并占有noReaders信号量，阻止新的读者进入临界区。这样，写者可以在当前读者完成操作后立即进入临界区，而不需要等待新的读者完成操作。<br>Writer<br>wait(writeMutex);
writers++;
if(writers == 1){
	wait(noReaders)
}
post(writeMutex);
wait(noWriters);
// Write data.
post(noWriters);
wait(writeMutex);
writers--;
if(writers == 0){
	post(noReaders)
}
<br>Reader<br>wait(noReaders);
wait(readMutex);
readers++;
if(readers == 1){
	wait(noWriters);
}
post(readMutex);
post(noReaders);
// Read data.
wait(readMutex);
readers--;
if(readers == 0){
	post(noWriters);
}
post(readMutex);
<br><br><br>#include &lt;pthread.h&gt;
pthread_rwlock_t rwlock;
<br>#include &lt;pthread.h&gt;
int pthread_rwlock_init(pthread_rwlock_t *rwlock, const pthread_rwlockattr_t *attr);
/* 
Parameters:
	1. rwlock: Pointer to the read-write lock to initialize.
	2. attr: Pointer to a read-write lock attributes object, or NULL for default attributes.
	   - PTHREAD_PROCESS_SHARED: The lock can be shared between processes.
	   - PTHREAD_PROCESS_PRIVATE: The lock is private to the process (default).

Return value: Returns 0 on success, otherwise an error number.
*/
<br>int pthread_rwlock_destroy(pthread_rwlock_t *rwlock);
/* 
Parameters:
	1. rwlock: Pointer to the read-write lock to destroy. The lock must be uninitialized before calling this function.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>// Blocks if the lock is currently held by a writer.
int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock);
/* 
Parameters:
	1. rwlock: Pointer to the read-write lock to acquire for read access.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>// Does not block. Returns immediately if the lock cannot be acquired.
int pthread_rwlock_tryrdlock(pthread_rwlock_t *rwlock);
/* 
Parameters:
	1. rwlock: Pointer to the read-write lock to attempt acquiring for read access.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>// Blocks if the lock is currently held by a reader or writer.
int pthread_rwlock_wrlock(pthread_rwlock_t *rwlock);
/*
Parameters:
	1. rwlock: Pointer to the read-write lock to acquire for write access.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>// Does not block. Returns immediately if the lock cannot be acquired.
int pthread_rwlock_trywrlock(pthread_rwlock_t *rwlock);
/*
Parameters:
	1. rwlock: Pointer to the read-write lock to attempt acquiring for write access.

	Return value: Returns 0 on success, otherwise an error number.
*/
<br>// Releases the lock held by either a reader or a writer.
int pthread_rwlock_unlock(pthread_rwlock_t *rwlock);
/* 
Parameters:
	1. rwlock: Pointer to the read-write lock to release.

Return value: Returns 0 on success, otherwise an error number.
*/
<br><br><br><br>条件变量和我们学过的条件语句非常类似，我们用不同的条件语句可以使得在条件满足时跳进特定的分支。条件变量也一样，条件变量的作用就是允许一个线程等待特定的条件被满足，在条件满足时通知其他线程继续执行。不同的是，条件变量是我们达成线程同步的一种方法，协调了线程之间的执行顺序。确保了某些线程在条件满足之前阻塞等待。<br>那这些阻塞的线程怎么才能知道条件已经满足了呢？在之前，我们可能会使用信号量、互斥锁来实现线程同步和协调。那这些和我们本节学习的条件变量相比有何不足呢？当我们使用信号量时，我们只能给特定的某一个线程发送信号。而使用条件变量，我们就可以选择当条件得到满足后，是给某个特定的线程发送信号还是给所有等待事件发生的线程发送信号。<br><br>在 POSIX 线程库底下，我们有许多相关的系统调用。下面我们来看看这些不同的系统调用。<br><br>我们用 pthread_cond_init 来初始化一个条件变量。详细的系统调用原型如下：<br>#include &lt;pthread.h&gt;
int pthread_cond_init(pthread_cond_t *cond, const pthread_condattr_t *attr);
/* 
Parameters:
	1. cond: Pointer to the condition variable to initialize.
	2. attr: Pointer to a condition variable attributes object.
	   - NULL: default attributes (PTHREAD_PROCESS_PRIVATE|CLOCK_REALTIME).
	   - PTHREAD_PROCESS_SHARED: condition variable can be shared between processes.
	   - PTHREAD_PROCESS_PRIVATE: condition variable is only used within a single process.
	   - PTHREAD_CONDATTR_CLOCKID: clock type used by the condition variable (e.g., CLOCK_REALTIME, CLOCK_MONOTONIC).

Return value: Returns 0 on success, otherwise an error number.
*/
<br>下面是简单的例子，举例如何创建并销毁一个环境变量。<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

int main() {
    pthread_cond_t cond; // Declare a condition variable
    int res; // Variable to store the result of initialization
    res = pthread_cond_init(&amp;cond, NULL);
    if (res == 0) {
        printf("Condition variable initialized successfully!\n");
    } else {
        printf("Failed to initialize condition variable. Error code: %d\n", res);
    }
   pthread_cond_destroy(&amp;cond);
    return 0;
}
<br><br>pthread_cond_wait 允许线程等待某个条件满足，同时释放互斥锁以避免资源竞争。<br>int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
/* 
Parameters:
	1. cond: Pointer to the condition variable to wait on.
	2. mutex: Pointer to the mutex that should be locked by the calling thread.

Return value: Returns 0 on success, otherwise an error number.
*/
<br><br>pthread_cond_signal 的作用是唤醒一个在条件变量上等待的线程。如果有多个线程在等待条件变量，函数只会唤醒一个线程（由系统决定的）。<br>int pthread_cond_signal(pthread_cond_t *cond);
/* 
Parameters:
	1. cond: Pointer to the condition variable to signal.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>我们用一个例子来学习：<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;

pthread_mutex_t mutex;
pthread_cond_t cond;
int condition = 0; // Explicitly initialize the condition variable to 0
void mutex_init(){
	int res = pthread_mutex_init(&amp;mutex, NULL);
	if (res == 0) {
		printf("Mutex lock initialized successfully!\n");
	} else {
		printf("Failed to initialize mutex lock. Error code: %d\n", res);
	}
	return;
}
void cv_init(){
    int res;
    res = pthread_cond_init(&amp;cond, NULL);
    if (res == 0) {
        printf("Condition variable initialized successfully!\n");
    } else {
        printf("Failed to initialize condition variable. Error code: %d\n", res);
    }
    return;
}

void *thread_func(void *arg) {
    pthread_mutex_lock(&amp;mutex);
    while (!condition) {
        pthread_cond_wait(&amp;cond, &amp;mutex);
        // The thread will block here, and the mutex lock will be released until the condition variable is signaled.
    }
    printf("Thread %ld continues\n", (long)pthread_self());
    pthread_mutex_unlock(&amp;mutex);
    return NULL;
}

int main() {
	mutex_init();
	cv_init();
    pthread_t threads[8];
    for (int i = 0; i &lt; 8; i++) {
        pthread_create(&amp;threads[i], NULL, thread_func, NULL);
    }
    
	usleep(1000000); // Sleep for a second to ensure the thread is waiting on the condition variable
    printf("Main thread is about to signal the condition variable.\n");
    pthread_mutex_lock(&amp;mutex);
    condition = 1;
    printf("Condition fulfilled by main thread.\n");
    for(int i = 0; i &lt; 8; i++){
	    pthread_cond_signal(&amp;cond);
    }
    pthread_mutex_unlock(&amp;mutex);

    for (int i = 0; i &lt; 8; i++) {
        pthread_join(threads[i], NULL);
    }
    
    // Destroy the condition variable and mutex to release resources
    pthread_cond_destroy(&amp;cond);
    pthread_mutex_destroy(&amp;mutex);

    return 0;
}
<br>首先，我们定义了三个全局的资源变量并对其进行初始化。<br>之后，我们启动了 8 个线程来执行 thread_func() 函数。在函数里面，线程会进入循环并调用 pthread_cond_wait(&amp;cond, &amp;mutex); ，这时，每个运行到 pthread_cond_wait 的线程都会被阻塞并释放其拥有的互斥锁。（当线程被唤醒后会重新获取 mutex ）<br>因为我们让主线程睡了 1 秒钟，所以主线程会在这 8 个子线程都阻塞后才进入临界区完成一系列的操作，之后挨个调用pthread_cond_signal ，因为主线程这时仍然持有锁，所以即使唤醒线程，它仍然阻塞。但因为等待条件被满足，所以当主线程释放锁，8 个线程会重新获取锁并开始执行。<br>之前我们提到了 barrier 的同步模式，虽然我们用条件变量实现这样一个 Barrier 同步模式很简单，但实际上，上面的实现仍然不能够保证可靠和 barrier 的思想。比如，我们使用 usleep 来确保所有的子线程都执行到 pthread_cond_wait(&amp;cond, &amp;mutex);。而且我们用循环来唤醒所有的线程，效率也不高。（触发多次内核态的系统调用）<br><br>pthread_cond_broadcast() 用来唤醒所有在条件变量上等待的线程。上面，我们每次只唤醒一个子线程，效率极为低下。但通过 pthread_cond_broadcast() ，我们可以保证一次系统调用就可以唤醒所有的子线程。它的函数调用原型如下：<br>int pthread_cond_broadcast(pthread_cond_t *cond);
/* 
Parameters:
	1. cond: Pointer to the condition variable to broadcast.

Return value: Returns 0 on success, otherwise an error number.
*/
<br>现在，你可以用更少的系统调用来实现屏障：<br>int count;
pthread_mutex_t lock;
pthread_cond_t cv;

void barrier(){
	pthread_mutex_lock(&amp;lock);
	count++;
	if(count &lt; NUM_THREADS){
		pthread_cond_wait(&amp;cv, &amp;lock);
	} else{
		pthread_cond_broadcast(&amp;cv);
	}
	pthread_mutex_unlock(&amp;lock);
}
<br>我们将之前一个一个唤醒线程的方式用 pthread_cond_broadcast() 来替换：<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;

pthread_mutex_t mutex;
pthread_cond_t cond;
pthread_barrier_t barrier;
int condition = 0;

void mutex_init() {
    if (pthread_mutex_init(&amp;mutex, NULL) == 0)
        printf("Mutex lock initialized successfully!\n");
    else
        printf("Failed to initialize mutex lock.\n");
}

void cv_init() {
    if (pthread_cond_init(&amp;cond, NULL) == 0)
        printf("Condition variable initialized successfully!\n");
    else
        printf("Failed to initialize condition variable.\n");
}

void barrier_init() {
    if (pthread_barrier_init(&amp;barrier, NULL, 8 + 1) == 0)
        printf("Barrier initialized successfully!\n");
    else
        printf("Failed to initialize barrier.\n");
}

void *thread_func(void *arg) {
    printf("Thread %ld waiting at barrier.\n", (long)pthread_self());
    pthread_barrier_wait(&amp;barrier);

    pthread_mutex_lock(&amp;mutex);
    printf("Thread %ld has acquired mutex lock.\n", (long)pthread_self());

    while (!condition) {
        pthread_cond_wait(&amp;cond, &amp;mutex);
    }
    printf("Thread %ld continues after the condition is fulfilled.\n", (long)pthread_self());
    pthread_mutex_unlock(&amp;mutex);
    return NULL;
}

int main() {
    mutex_init();
    cv_init();
    barrier_init();

    pthread_t threads[8];
    for (int i = 0; i &lt; 8; i++) {
        pthread_create(&amp;threads[i], NULL, thread_func, NULL);
    }

    pthread_barrier_wait(&amp;barrier);
    pthread_mutex_lock(&amp;mutex);
    condition = 1;
    printf("Condition fulfilled by main thread.\n");
    pthread_cond_broadcast(&amp;cond);
    pthread_mutex_unlock(&amp;mutex);

    for (int i = 0; i &lt; 8; i++) {
        pthread_join(threads[i], NULL);
    }

    pthread_cond_destroy(&amp;cond);
    pthread_mutex_destroy(&amp;mutex);
    pthread_barrier_destroy(&amp;barrier);

    return 0;
}
<br><br>int pthread_cond_destroy(pthread_cond_t *cond);
/* 
Parameters:
	1. cond: Pointer to the condition variable to destroy.

Return value: Returns 0 on success, otherwise an error number.
*/
<br><br>好了，现在我们还有一个问题：为什么条件变量总是和互斥锁在一起配合使用？比如说你会看到：<br>pthread_mutex_lock(&amp;mutex);
    if (condition_is_false) {
	    pthread_cond_wait(&amp;cond);
	}
pthread_mutex_unlock(&amp;mutex);
<br>我们前面提到过，条件变量是一种类似 if...else... 的逻辑。在多线程的环境中，我们需要保证检查条件是否满足和进入等待条件必须是原子操作，一气呵成的。不然就可能因为线程的并发执行而发生条件竞争。假如我们有下面的情况：<br>void *thread_func(void *arg) {
    if (!condition) { // Check
	    pthread_cond_wait(&amp;cond); // And wait
	}
    return NULL;
}

int main() {
    pthread_t threads_A;
    pthread_create(&amp;threads_A, NULL, thread_func, NULL);
    condition = 1;
    pthread_cond_signal(&amp;cond);
    pthread_join(threads_A, NULL);
	return 0;
}
<br>这里没有互斥锁，就可能导致在子线程检查完毕后，主线程抢占 CPU 并修改条件为满足并调用 pthread_cond_signal。这就会使得子线程调用&nbsp;pthread_cond_wait 时条件已满足，导致无限等待。所以我们需要加锁。<br><br>为什么不在条件变量中集成互斥锁？<br><br><br>条件变量可以用于管程的创建（更高层级的抽象），所以管程是一个更高级的同步工具，有点类似于OOP中的类。在C++中，我们用类来打包数据和相关的操作，管程的目标就是将那些共享的数据和对这些共享数据的操作进行打包封装起来。有了管程，我们就不需要手动地操作那些共享数据了，减少了出错的可能性。<br><br>在传统的方法中，我们需要人为地对锁进行管理。下面这里例子中，我们有一个加锁和一个解锁，乍一看是正确的，但是程序中的if条件判断使得程序实际上存在着两个分支。我们需要给这两个分支的互斥锁都做好善后工作，因此实际上我们需要两个解锁函数。<br>void foo(){
	pthread_mutex_lock(&amp;lock);
	/* Read some data. */
	if(condition_is_true){
		printf("Cannot continue due to reasons.\n");
		// Missing something here~
		return;
	}
	/* Do something more. */
	pthread_mutex_unlock(&amp;lock);
}
<br>这种人为管理资源的模式太麻烦了，我们需要一种更加智能的管理方式。在C语言中，我们可以使用pthread库中的条件变量和互斥锁来实现管程。而 C++ 的 RAII 机制使得管程的实现尤为容易。<br>#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

typedef struct {
    pthread_mutex_t mutex;
    pthread_cond_t cond;
    int count;
} Monitor;

void init(Monitor *m) {
    pthread_mutex_init(&amp;m-&gt;mutex, NULL);
    pthread_cond_init(&amp;m-&gt;cond, NULL);
    m-&gt;count = 0;
}

void increment(Monitor *m) {
    pthread_mutex_lock(&amp;m-&gt;mutex);
    m-&gt;count++;
    pthread_cond_signal(&amp;m-&gt;cond);
    pthread_mutex_unlock(&amp;m-&gt;mutex);
}

void wait_for_count(Monitor *m, int target) {
    pthread_mutex_lock(&amp;m-&gt;mutex);
    while (m-&gt;count &lt; target) {
        pthread_cond_wait(&amp;m-&gt;cond, &amp;m-&gt;mutex);
    }
    pthread_mutex_unlock(&amp;m-&gt;mutex);
}

void destroy(Monitor *m) {
    pthread_mutex_destroy(&amp;m-&gt;mutex);
    pthread_cond_destroy(&amp;m-&gt;cond);
}

int main() {
    Monitor m;
    init(&amp;m);

    pthread_t t1, t2;
    pthread_create(&amp;t1, NULL, (void *(*)(void *))increment, &amp;m);
    pthread_create(&amp;t2, NULL, (void *(*)(void *))wait_for_count, &amp;m);

    pthread_join(t1, NULL);
    pthread_join(t2, NULL);

    printf("Final count: %d\n", m.count);

    destroy(&amp;m);
    return 0;
}
<br>#include &lt;iostream&gt;
#include &lt;mutex&gt;
#include &lt;condition_variable&gt;
#include &lt;thread&gt;

class Monitor {
private:
    std::mutex mtx;
    std::condition_variable cond;
    int count = 0;
public:
    void increment() {
        std::unique_lock&lt;std::mutex&gt; lock(mtx);
        count++;
        cond.notify_one();
    }
    void wait_for_count(int target) {
        std::unique_lock&lt;std::mutex&gt; lock(mtx);
        cond.wait(lock, [this, target] { return count &gt;= target; });
    }
    int get_count() const {
        return count;
    }
};

int main() {
    Monitor monitor;

    std::thread t1(&amp;Monitor::increment, &amp;monitor);
    std::thread t2(&amp;Monitor::wait_for_count, &amp;monitor, 1);

    t1.join();
    t2.join();

    std::cout &lt;&lt; "Final count: " &lt;&lt; monitor.get_count() &lt;&lt; std::endl;

    return 0;
}
<br><br><br>在学习原子类型变量之前，要规避并发访问共享数据带来的竞争条件问题，我们可能想当然地使用锁来解决。将共享数据的访问放到一个临界区中，然后对临界区进行加锁和解锁来实现这一过程。但是会带来额外的性能开销（两次系统调用）。<br>#include &lt;pthread.h&gt;

pthread_mutex_lock();   // system call * 1
shared_var++;
pthread_mutex_unlock(); // system call * 2
<br>上面利用互斥锁实现的对共享资源访问的方法中，我们看到，系统调用的开销要远远大于对共享资源操作所带来的开销。这时只会使用锁的单一解决方法就成为了一种枷锁。那有没有办法降低这种资源损耗？当然有：原子类型。<br><br>我们前面已经学习过像&nbsp;test-and-set、compare-and-swap&nbsp;等硬件提供的原子操作指令。没有硬件提供的原子指令，软件再怎么模拟也不可能实现相似的原子性操作。利用机器提供的这些原子操作指令的接口，我们可以在上层封装这些原子操作为己所用。<br>实际上，互斥锁和原子类型都是对这些机器指令的封装和抽象，但不同的是，原子类型直接利用指令提供的原子性操作，避开了系统调用的开销，而锁更加高级，常用于处理复杂的竞争条件问题。对于单一共享资源的处理，我们简单地利用原子类型就可以了。<br>早期 C 标准库没有引入原子操作时，GNU 标准下的 C 库（glibc）就通过对下层机器指令的封装提供了原子类型。这些原子类型操作保证了操作的原子性，规避简单的条件竞争。在 C++11 之后，我们可以包含 &lt;atomic&gt; 头文件来使用C++中的原子操作。当我们在高级语言中使用这些原子类型时，编译器会将这些数据类型转换成硬件能够提供的原子操作指令。<br>下面我们展示了 C++11 中的原子类型。对 std::atomic 类型的操作是原子性的，不可被打断的。<br>#include &lt;atomic&gt;

std::atomic&lt;int&gt; shared_var(0);

shared_var++; // Operations to shared_var is unbreakable.
<br><br><br>下面是GCC（GNU&nbsp;Compiler&nbsp;Collection）提供的一组内建函数，主要用于进行原子操作和实现无锁编程。它们利用硬件提供的原子指令，确保操作的原子性，避免了竞态条件。<br>type __sync_lock_test_and_set(type *ptr, type value)
bool __sync_bool_compare_and_swap(type *ptr, type oldval, type newval)
type __sync_val_compare_and_swap(type *ptr, type oldval, type newval)
<br>type&nbsp;__sync_lock_test_and_set(type&nbsp;*ptr,&nbsp;type&nbsp;value):&nbsp;该函数将value写入*ptr，并返回*ptr的旧值。这是一个原子的“测试并设置”操作，常用于实现简单的锁。<br>bool&nbsp;__sync_bool_compare_and_swap(type&nbsp;*ptr,&nbsp;type&nbsp;oldval,&nbsp;type&nbsp;newval):&nbsp;该函数如果*ptr等于oldval，则将newval写入*ptr，返回true；否则，不修改*ptr，返回false。<br>type&nbsp;__sync_val_compare_and_swap(type&nbsp;*ptr,&nbsp;type&nbsp;oldval,&nbsp;type&nbsp;newval):&nbsp;该函数如果*ptr等于oldval，则将newval写入*ptr，并返回*ptr的旧值。<br>除此之外，gcc还提供其他的一些内建函数。如下：<br>// Return the old value:
type __sync_fetch_and_add(type *ptr, type value);
type __sync_fetch_and_sub(type *ptr, type value);
type __sync_fetch_and_or(type *ptr, type value);
type __sync_fetch_and_and(type *ptr, type value);
type __sync_fetch_and_xor(type *ptr, type value);
type __sync_fetch_and_nand(type *ptr, type value);

// Return the new value:
type __sync_add_and_fetch(type *ptr, type value);
type __sync_sub_and_fetch(type *ptr, type value);
type __sync_or_and_fetch(type *ptr, type value);
type __sync_and_and_fetch(type *ptr, type value);
type __sync_xor_and_fetch(type *ptr, type value);
type __sync_nand_and_fetch(type *ptr, type value);
<br><br>自始至终，我们学到的原子操作都是关于 modifying 的原子操作，但从来没有提到读指令的原子操作。那该如何保证每次读到的数据都是最新的呢？<br>x86机器并没有 build-in 读原子操作指令。由于机器上的读指令在大多数情况下是原子性的，单独的读操作通常是不可分割的。在x86机器上，读取一个32位整数或一个64位整数（在64位架构上）通常默认就是原子操作。但如果这样，我们仍然可能得到过时的数据，因为如果读的时候刚好写入了新的数据，那么读到的数据可能就不再是最新的了。<br>我们其实可以用 modifying 的原子操作来做一个读的事情。比如对原先的变量原子加或减0，就能够保证我们每次读到的数值都是最新的。如果不在乎性能开销，使用锁也是可以的。]]></description><link>https://congzhi.wiki/congzhi's-os-series/10.-synchronization-and-mutex.html</link><guid isPermaLink="false">Congzhi's OS Series/10. Synchronization and Mutex.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 14 Jun 2025 13:26:05 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/this_is_fine.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/this_is_fine.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[12. Memory Management]]></title><description><![CDATA[ 
 <br>第一遍重写中<br>page size is a trade-off, and a larger page size can improve TLB hit rate. Bigger page size means fewer entries(same TLB now can cache more memory addresses)<br><br><br>操作系统的诞生源于人们为了更有效的操作裸机（硬件）和对操作高层次抽象的追求。为了更好的理解本阶段的内容，内存管理阶段开始前，我们需要先了解一点组成原理和体系结构的内容。<br><br>我们知道，冯诺依曼计算机是以存储器为中心的。不管是我们的操作系统还是应用都要跑在主存上。那内存和外存的关系是怎么样的？在我们执行某些程序时，程序和数据会从外存批量传送到内存上，在由 CPU 从内存中取指执行。CPU 执行完毕后结果写回到内存中，必要时再由内存成批传送到外存长久保存。<br><img alt="Pasted image 20240728054403.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240728054403.png"><br>下图展示了主存的结构，我们假设系统按字节连续编址，那么每个存储单元就是一个字节。不难理解，地址线的条数和主存的最大字节数关系就等于：虽然地址线条数能够产生主存的访存空间有这么大，但由于各种限制，内存可能做不到这么大（首个32位机 Intel 80386 早在1985年就出现了，但是4GB的内存条知道2000年代初期才开始普及）。为了能够利用到这么大的访存空间，计算机系统引入了内存层次结构，这是理解虚拟存储器的关键技术之一。<br><br>早期人工操作计算机的过程繁琐而复杂。为了简化操作并提高效率，操作系统诞生了，操作系统为人们提供一个”虚拟的机器“，人们再也不用和底层电路打交道。但对于程序员，使用这样的计算机仍不知足，因为内存的发展和 CPU 的相比太慢了。程序员想要的是一种和 CPU 速度相匹配，容量   ”无限大“的存储器。存储器层次结构的思想就是处于这种情况下诞生的。<br><img alt="Pasted image 20240729010510.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729010510.png"><br>在存储器的层次结构中，只有寄存器和cache能够与 CPU 的速度相匹配，但败在容量小。主存相比 CPU 要慢上10多倍，容量相比cache可以大很多。辅助存储器更是能够提供几近无限的存储大小，但对于 CPU 而言太慢（个时钟周期）。而通过组合这些不同层次的存储器并采取合理的映射方式和读写策略，存储结构就可以为程序员提供非常好的使用体验。这种多层次存储器结合起的、存储容量又大访问速度又快的“存储器”就是我们说的虚拟存储器。<br>这样层次化的存储器结构中，相邻两个层次之间是一定要进行数据传送的。传送的最小单位是一个定长块 (block)，互为副本。在主存和磁盘之间，这种定长块被称为页 (page)。在cache和主存之间，就叫定长块（32/64/128字节），cache中最小传送单位叫槽 (slot)，定长块大小和槽大小相同。<br><br><br>早期的操作系统并不提供内存管理机制。主存空间是需要程序员自己管理的，如果主存空间很小，每次只能在内存中运行一个程序，要运行另一个程序时就需要将上一个进程从内存中取出来，换到外存中先放着，再将外存中需要运行的进程加载到内存中。<br>在1961年，曼彻斯特的研究人员提出一种自动执行(overlay)的方式，其思想是将地址空间和主存容量的概念区分开。程序员在虚拟地址空间中编写程序，而程序在真正的内存中运行。由一个专门的机制实现地址空间和实际主存之间的映射。<br>在当时的一种典型计算机中，指令中主存地址是16位，但是主存容量只有4KB。地址空间有  这么大，我们要如何自动执行程序？可以将地址空间划分成4K大小的一个个区间，让内存空间的地址以4096（4K）取模，将区间内每位地址都能对应上4K的主存范围间。程序员可以在 16KB（0-65535）的范围内写程序，不用关心主存空间的大小。<br><img alt="Pasted image 20240729033616.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729033616.png"><br>这种可寻址的地址就是一种虚拟内存，区间后来也叫做页(page)，把主存中存放页的区域叫做页框(frame)。最早的主存只有一个页框。<br><br>现代的分页方式和早期分页方式十分类似。其基本思想都是把内存分成固定长度的存储块（也叫页框、实页、物理页），每个进程也划分成固定长的程序块（又称为页，虚页、逻辑页）。<br>在执行程序时，我们就可以把程序块装到可用的存储块中，不需要用连续页框存放一个进程。为此，操作系统会为每个进程分配一个页表(page table)。通过页表就可以实现逻辑地址向物理地址的转换(Address Mapping)。我们下面举例说明一下地址转换：<br>
<img alt="Pasted image 20240729035023.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729035023.png"><br>
假如某个进程有四个页的大小。因为虚拟内存中的页和主存中的页框是一样大的，因此我们只需要在页表中描述虚拟页和物理页框之间的映射关系就行了。而且由于程序的局部性，只将活跃的页面留在主存并不会太多影响运行速度。这种”按需调页(Demand Paging)“方式分配主存就是虚拟存储管理的概念。<br><br>人们引入虚拟存储技术是为了解决一对矛盾：<br>
<br>由于技术成本等原因限制的主存容量；
<br>程序要求的主存容量越来越大。
<br>有了虚拟内存，上面的矛盾迎刃而解。程序员可以在比主存大得多的空间内编写程序。当程序执行起来时，只把当前需要的程序段和相应数据块调入主存，不用的地方先放在磁盘上。只当发生缺页(page fault) 时，操作系统才需要介入并进行主存和磁盘之间的信息交换。<br>虚拟存储器的机制由硬件和操作系统共同协作实现，涉及到操作系统中许多概念，如进程、上下文切换、存储器分配、虚拟地址空间和缺页处理等等。<br><img alt="Pasted image 20240729180635.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729180635.png"><br>在之前的学习中，我们学习了虚拟地址空间的概念。在32位机器中，每个进程都可以分配高达4GB的虚拟地址空间，不同的操作系统对用户空间和内核空间的大小有不同的规定。我们也可能会有疑问：如果每个进程都在磁盘中占4GB的虚拟空间，磁盘岂不是只能存放很少数量的用户程序？实际上当然不是这样！虽然每个进程都会分配4GB的虚拟内存空间，实际上在磁盘上每个进程可能只会用到很小的磁盘存储（空洞页面不占用磁盘）。<br><img alt="Pasted image 20240729183709.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729183709.png"><br><br>我们想要实现虚拟存储器的管理，我们还要考虑：<br>
<br>页大小应该为多大？
<br>主存与辅存的空间如何分区管理？
<br>程序块/存储块之间如和映像？
<br>逻辑地址和物理地址如何转换，转换速度怎么提高？
<br>主存和辅存之间如何进行替换？
<br>页表如何实现，页表项要记录那些信息？
<br>如何加快访问页表的速度？
<br>要找的内容不在主存怎么办？
<br>如何保护进程各自的存储区不被其他进程访问？
<br>虚拟存储器的管理分为了三种方式：分页式、分段式、段页式。<br>假定一个页有4KB，在32位机器上，页表的项数就会有：页表有这么多项，假如一个页表项占4KB，那么页表会有4MB的大小，比页还大。因此页表也要分页管理。页表存放在虚拟地址空间中的内核区。每个进程有一个页表，其中有装入位、修改（Dirt）位、替换控制位、访问权限位、禁止缓存位、实页号。各个进程理论上有相同的虚拟空间，但是实际大小看具体的实现方式，如”空洞“页面如何处理等。<br>我们将页分成了三类：<br>
<br>未分配页：进程的虚拟地址空间中“空洞”对应的页（如VP0、VP4） 
<br>已分配的缓存页：有内容对应的已装入主存的页（如VP1、VP2、VP5等） 
<br>已分配的未缓存页：有内容对应但未装入主存的页（如VP3、VP6）
<br><img alt="Pasted image 20240729201529.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729201529.png"><br>
可执行文件的存储器映像如下图所示。<br><img alt="Pasted image 20240729201908.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729201908.png"><br><br>我们已经了解了分页式虚拟存储器是怎么工作的。将虚拟存储空间划分成大小相等的一个个页，然后将主存空间划分成和页大小相同的一个个页框。然后用页表中的页表项对应每个页到页框的关系。但分页的方式会有一些问题，比如一个数据跨在两个不同的页中。<br>将虚拟地址空间分段很好的解决了这种问题。通过程序数据的需求分配不同的段，按照程序逻辑结构将虚拟地址空间划分成多个相对独立的部分（代码段、只读数据段、课读写数据段等等）。而且相比分页，分段方式能更好地进行存储保护（数据和代码在同一个页中，地址越界、保护违例）。<br><img alt="Pasted image 20240730015834.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730015834.png"><br>
编译器优化和操作系统调度管理。分段系统将主存空间按实际程序中的段来划分，每个段在主存中的位置记录在段表中，并附以“段长”项。段表由段表项组成，段表本身也是主存中的一个可再定位段。<br><br>分段式的虚拟内存管理确确实实解决了分页式虚拟内存管理的痛点，但是它也走向了另一个极端。占空间多，段内存换进换出很难管理。因此我们折中，段页式存储器完美的迎合了我们的需求。<br>程序的虚拟地址空间按模块先分段、段内再分页，但进入主存仍以页为基本单位。这一，逻辑地址就由段地址+段内页地址+页内偏移量三个字段构成。用段表和页表（每段一个）进行两级定位管理。根据段地址到段表中查阅与该段相应的页表首地址，转向页表，然后根据页地址从页表中查到该页在主存中的页框地址，由此再访问到页内某数据。<br><br><br>在第零课的学习后，我们应该对虚拟内存有了大概的理解。在操作系统中，内存管理涉及的是“内存-外存”这个层次的管理。外存（如磁盘）作为一个大仓库，为用户进程提供逻辑上无限大的编程空间。由于磁盘是一种外部设备，我们将这种磁盘提供的后备资源称为虚拟内存。因为这种逻辑上的关系，也被称为逻辑内存。在本阶段，我们需要额外关注以下的话题：<br>
<br>内存的物理地址和进程的逻辑地址
<br>地址映射和内存保护
<br>内存分配和回收
<br>分页和分段
<br>虚拟内存
<br><br>当我们使用内存时，感受到的是一段连续的内存，这多亏了操作系统的抽象。而我们实际上使用的内存在物理上的地址可能并不连续。我们下面先来了解什么是物理内存地址，它是怎么得到的？<br><br>我们直接访问的数据无一例外都存放在主存中。由于主存采用随机存取方式，因此也称为随机存取存储器（Random Access Memory, RAM）。它是计算机系统中的关键硬件组件，用于临时存储和快速访问数据和指令。<br>下面是RAM的一个组织图，每个bit数据存放在内存中一个个cell中。计算机通过字线和位线将这些cell编址成一个位平面(Bit plane)。由于大多机器按字节编址，所以一般的DRAM bank需要8个这样的位平面摞在一起。下图的例子并不确切，但对于理解地址译码来说是一个很好的例子。<br><img alt="Pasted image 20240730033638.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730033638.png"><br>物理地址和地址译码的关系很紧密。假设上图我们有8个位平面，我们可以通过物理地址来得到我们想要的byte。其中，我们就需要一个译码器将物理地址翻译成某个确切的byte信息。<br>地址译码器有n个地址输入线和  个输出线。对于每一种输入组合，只有一条输出线会被选中（低电平），其他线保持高电平。因为32位机器上地址位数有32位（现代计算机通常有行缓存，行选中和列选中是分批次进行的），所以能够产生4GB的物理地址。<br><br>物理地址是计算机内存中的实际地址。每个内存单元都有唯一的物理地址，通过这个地址，CPU可以直接访问内存中的数据。物理地址是由硬件直接管理的，因为它在硬件层面上是唯一且固定的，因此也被称为绝对地址(absolute address)。<br>物理地址一般从0开始，而且一般以字节作为最小单位。至于物理地址的长度多大要取决于CPU的架构。32位架构就能访问4GB大小的物理内存。64位架构能访问的大小则是一个天文数字，有生之年也不知道能不能见到这么大的存储器。<br><br>在第零课的先导课中，我们提到了段页式虚拟内存管理。逻辑地址就等于段基址+偏移量（相对地址）实现的。这里的相对地址就是相对段基址而言的。将地址从一种形式（如逻辑地址）转换成另一种形式（如物理地址）的过程。在现代计算机中，一般使用MMU来完成地址转换的工作。<br>在用户眼中连续的地址就是逻辑地址。逻辑地址是程序运行时CPU生成的，操作系统分配给进程使用的地址。每个进程都有各自的逻辑地址，通常从0开始。<br><br>物理地址 = 绝对地址，但逻辑地址 != 相对地址。相对地址是相对于某个基准地址来说的。由编译器在编译时生成，通常用于指令中的跳转、调用等。也就是偏移量(offset)。<br><br><br><br>在我们用c程序编写了一个hello.c文件后，计算机是不能直接执行这个文件的。原因是计算机只能识别二进制的机器级代码，hello.c是源程序的文本文件。要让计算机识别并执行我们所编写的源程序，还需要做以下步骤：<br>
<br>预处理(cpp)<br>
预处理阶段，程序会处理以 '#' 开头的预编译指令并删除所有的代码注释。经过预编译的处理，我们得到预处理文件（hello.i），这时的文件仍然是一个可读的文本文件，不包含任何宏定义。
<br>编译(cc1)<br>
编译过程就是将预处理后得到的预处理文件（如hello.i）进行 词法分析、语法分析、语义分析、优化后，生成汇编代码文件。经过编译后，得到的汇编代码文件（如hello.s）还是可读的文本文件，CPU无法理解和执行它。
<br>汇编(as)<br>
程序经过编译后生成汇编语言源程序，在汇编阶段，汇编程序（汇编器）会用来将汇编语言源程序转换成机器指令序列（机器语言程序）。汇编指令和机器指令一一对应，前者是后者的符号表示，它们都属于机器级指令，所构成的程序称为机器级代码。汇编结果是一个可重定位目标文件（如，hello.o）。
<br>链接(ld)<br>
链接过程将多个可重定位目标文件合并以生成可执行目标文件。<br>
<img alt="Pasted image 20240730092536.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730092536.png">
<br>装入内存<br>
<img alt="Pasted image 20240729201908.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240729201908.png">
<br><br>因为汇编代码和二进制机器代码都是机器级代码，因此我们能很明白地在汇编代码中，窥见一些程序加载前的要做的事。代码中CALL和PRINT两个标号代表了那一行指令的逻辑地址，这些标号会在链接过程中经过符号解析和重定位转换成相应的逻辑地址。而这些逻辑地址会在不同时机 中将逻辑地址转换成不同的物理地址。<br>Logical address          Assembly code
0000               START: MOV AX, 1234H
0003                      ADD AX, 5678H
0006                      JMP NEXT
0009                HERE: SUB AX, 1234H
000C                NEXT: MOV BX, AX
000F                      CALL PRINT
0012                      HLT
0013               PRINT: PUSH AX
0014                      POP AX
0015                      RET
<br><br><br>源代码被编译成目标文件，生成的目标文件包含逻辑地址，逻辑地址从0x000开始。在链接阶段，决定程序将被加载到物理内存中的确切位置。之后链接器会将目标文件中的所有逻辑地址转换位相对于加载地址的物理地址。例如，假设决定程序将加载到物理位置0x300，随后，0x300就变成一个基地址，虚拟地址0x003就会被转换为物理地址0x303。<br>这种链接时地址转换的好处就是简单，而且系统开销低。但是缺点也是显而易见的，那就是灵活性差，不适合多任务的环境。只用场景只有哪些简单的单任务系统。<br><br>链接时转换的方案是在可执行文件加载进内存前就已经指定好在物理内存的哪个位置运行了。装载时转换指在可执行文件被加载到内存时，将程序中的逻辑地址转换成物理地址的过程。这种方法在程序加载时进程一次性转换，并将地址固定下来，也称为静态重定位。适用于内核模块和动态链接库(DLL)等场景。<br>相比链接时转换，加载时转换的方式更为灵活，因为程序的逻辑地址在编译和链接时不需要是固定的，可以在加载时根据内存的实际情况进行调整。但是这种方式不可避免的导致加载时间增加，而且加载后不可在内存中移动。<br><br>加载进内存程序中的指令地址仍然保存逻辑地址，在执行过程中，将逻辑地址动态转换成物理地址。这个过程就是运行时地址转换，也叫动态重定位。动态重定位通常通过硬件和操作系统的支持，比如使用 MMU 实现逻辑地址到物理地址的映射。运行时转换是现代系统中广泛采用的方法。<br>
<br>优点：高灵活性、内存保护、高效内存利用。
<br>缺点：性能开销大、复杂性高、对硬件要求。
<br><br>
<br>编译阶段：源代码转成目标代码，每个目标文件中的地址称为逻辑地址。
<br>链接阶段：将多个目标文件链接合成可执行文件，该阶段会分配每个段的虚拟地址，在根据目标文件的逻辑地址和加载段生成全局虚拟地址。此时虚拟地址空间就构建完毕，构建信息就是ELF的头部信息。
<br>加载阶段：OS 会先读取ELF头部信息，根据虚拟地址空间的信息加载必要的段到物理内存中，其他部分留在虚拟内存。
<br>运行阶段：当某个要访问的页不再内存中，则请求操作系统从虚拟内存将其调入物理内存。如果物理内存不足就进行页面置换。
<br><br><br><br>内存管理单元(MMU)，也叫页内存管理单元(PMMU)，是一种支持虚拟内存和分页的硬件设备或电路，负责将虚拟地址转换成物理地址。<br>
<br>在现代操作系统中，进程的逻辑地址空间大小理论可达计算机架构的最大值，而物理内存通常比理论最大值小。
<br>一个进程通常不会用完全部的逻辑地址空间，也很少会在在某一时刻使用大量内存。因此，可以同时在物理内存中容纳多个进程。
<br>MMU的任务就是把每个进程的逻辑地址映射到物理内存的不同区域，同时也要保证进程之间的隔离性（保护地址不会越界或溢出）。
<br><img alt="Pasted image 20240730215541.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730215541.jpg"><br><br>基址和限长是虚拟内存的一种简单形式，通过一组或少量处理器寄存器（基址寄存器和限长寄存器）来控制对计算机内存的访问。工作原理如下：<br>
<br>分配内存区域<br>
每个用户进程被分配一个连续的主存区域。操作系统将该区域的物理首地址加载到基址寄存器（重定位寄存器）中，将其大小加载到限长寄存器中。
<br><img alt="Pasted image 20240730220832.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730220832.jpg"><br>
<br>
物理地址转换<br>
我们已经了解了基址寄存器和编址寄存器的作用，我们通过下图直观学习以下物理地址是这么转换的。当进程重定位到 14000 地址的基址时，这个基址会存放在基址寄存器中，和CPU送来的所及地址相加就可以得出物理地址了。不过要注意，在这里还要判断地址是否越界的问题，我们马上介绍。<br>
<img alt="Pasted image 20240730223332.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730223332.jpg">

<br>
内存保护<br>
通过进程P中基址和限长寄存器中的值，我们就可以轻松地判断是否存在非法的址错误。在下图中，硬件（MMU）会检查每个CPU送来的地址，检查是否越界等等。一旦发现送来的地址值小于基地址，或者大于限长寄存器（base+limit）中的值，MMU就会通过一条trap指令叫出操作系统处理这种 addressing error。

<br><img alt="Pasted image 20240730220817.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730220817.jpg"><br>结合物理地址转换和内存保护，我们就可以简单的描述出MMU地址转换和内存保护方法的步骤了。因为CPU送来的是逻辑地址，因此我们在这里省略和基地址的比较过程。<br><img alt="Pasted image 20240730221149.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730221149.jpg"><br><br><br>在计算机的内存管理中，连续分配管理方式是一种基础的分配方法。它的主要特点是将进程装入内存时，要求进程占据一块连续的内存空间。这种方法可以有效的管理和分配内存资源，但是也会有一些限制。<br>连续分配管理方式可以进一步细分为固定分区和可变分区两种模式。这两种模式各有优缺点，适用于不同的应用场景。后面的分段、分页就是对这两种模式的延伸。<br><br>固定分区管理方式是一种将内存划分为若干固定大小分区的方法（分区大小不一定相同）。由于内存分区大小固定，所以每个分区在系统启动时就已经确定，不能动态调整。此外，在固定分区中，每个分区只能容纳一个进程，如果一个进程之战1MB，而分区大小为5MB，这就会造成4MB的浪费，即内部碎片(Internal fragmentation)。当我们分配分区时，尽量使得内部碎片尽可能的小。<br>当系统有多个进程需要运行时，会将每个进程分配到一个合适大小的分区中。如果所有分区都被占用，新到的进程将无法进入内存，必须等待。<br><img alt="Pasted image 20240801010828.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240801010828.png"><br><br>在固定分区中，为了实现内存的分配和回收，系统需要维护一张表用于跟踪分区当前的状态。这张表记录着各个分区的编号、起始地址、大小和占用情况。<br><br>在固定分区下，当进程需要申请内存空间时，分配器会根据进程的大小，在表中寻找一个合适的分区进行分配。之后，将该分区的起始地址填入 Base register 中，将分区大小或进程大小填入 Limit register 中，并标注占用该分区的进程 ID 号。<br><br>当进程结束后，清除该分区的占用情况使分区重新可用。<br><br>在固定分区中的地址转换和保护中有两种不同的声音：<br>
<br>限长寄存器中应当存放的是分区大小，也就是物理地址在内部碎片中也是允许的；
<br>限长寄存器存放的是进程大小，限制进程在分区内部碎片的访问权限。
<br><br>权衡利弊，固定分区的好处就是简单易实现。但是其不灵活的缺点难免导致产生大量的内存内部碎片，不可避免的造成资源的浪费。<br><br>与固定分区的管理方式不同，可变分区的管理方式并不与先划分固定大小的分区，而根据进程的实际需要动态地分配内存。<br><br>由于可变分区的内存管理方式不再固定地划分分区。为了确保内存分配高效性，系统需要维护两张表：空闲分区表和已用分区表。空闲的分区也叫做孔(holes)，初始情况下，整个内存区域就是一个空闲分区。<br><img alt="Pasted image 20240730220934.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240730220934.jpg"><br><br>根据进程大小在空闲分区表中寻找一个合适的分区进行分配，更新空闲分区表和已用分区表，并初始化进程的 Base register 和 Limit register。<br><br>在释放内存时，更新已用分区表和空闲分区表（如果可以，合并多个连着的空闲分区，这被称为 coalescence）。<br><br>没什么好说的，参见上节课内存保护的部分。<br><br>相比固定分区，可变分区为我们带来了一定的灵活性。由于分区大小就是进程的大小，所以可变分区避免了内部碎片的产生。然而，这种分区为我们带来了外部碎片，也就是我们说的holes。如果孔的分配不合理，就可能出现许多无法利用的孔。<br>为了减少外部碎片的占比，在可变分区中，我们有许多分配算法。<br><br>由于可变分区的分区方式是动态可调整的，因此如何分配“孔”就决定了内存的利用率。常见的分区策略有——First fit、Next fit、Best fit、Worst fit、Quick fit。<br><br>首次适应从头开始搜索空闲分区表，检查每个块。一旦找到大小合适的空闲分区块，将其分配给进程。除了这种分配方式可能产生过多的内存碎片外，首次适应有许多优点：<br>
<br>速度快 O(n)，找到第一个合适的就能分配了。
<br>易于实现。
<br><br>下次适应是从上次分配结束的位置开始，继续向前查找，找到第一个足够大的空闲块进行分配。如果到达内存末尾，则从头开始继续查找。下次适应具有与首次适应（First Fit）相同的优点：速度快O(n)和易于实现。<br>在某些情况下，下次适应可能更均匀地利用内存，因为它不会总是从内存的起始位置开始查找。然而，它也可能会导致更多的内存碎片，因为它不会总是选择最靠前的空闲块。此外，下次适应可能在某些情况下更快，因为它避免了每次分配都从头开始查找。<br><br>相比于前两种傻瓜式的比较适应算法，最佳适应会搜索整个空闲分区表，找到最接近进程所需大小的空闲分区块，然后将其分配给进程。相比较于前两者，最佳适应的外部碎片产生最小。但是有些实现方式可能使得搜索时间过长。<br>然而，如果使用 AVL 树或红黑树，最佳适应算法的速度可能会优于首次适应算法：Θ(In(n))。<br><br>最差适应算法反其道而行之，它通过搜索整个空闲分区表，找到最大的空闲分区表，将其分配给进程。为什么是最大的内存块？因为大块内存分配后，留下的空闲区域依然很大，便于后续的分配。这就是最差适应的中心思想，但是也可能造成更大的内存浪费。<br>可以使用 max heap、 binomial heap 或 Fibonacci heap 来实现最差适应算法。<br><br>快速适应是一种优化内存分配速度的算法。它通过维护多个空闲块列表，每个列表对应不同大小的内存块。当需要分配内存时，Quick Fit 会直接从对应大小的空闲块列表中查找和分配内存。这种方法可以显著减少内存分配和释放的时间开销，但可能会导致内存碎片问题。<br><br>
<br>可变分区方式的内存利用率较高，可以更灵活地适应不同大小进程的需求。
<br>存在外部碎片（孔）和分配开销。
<br>合并技术 (coalescence)：内存管理器会将相邻的空闲块合并成一个更大的空闲块。
<br>紧凑技术 (compaction)：将进程在内存中进行移动，消除不可用的孔。但是这种移动内存块的方式会增加系统开销。
<br><br><br><br>在上节课的连分配管理方式中，我们看到无论是固定分区还是可变分区，都要求进程在内存中是完整的。但无论是哪种连续分配方式，都不可避免的会有碎片产生。因此，在本节课及往后，我们开始讨论非连续性存储管理方式，为找到一种更好的方式管理我们的内存。<br>实际上，我们在阶段6第三课中已经对“段”这个概念有所耳闻了。我们知道虚拟内存中不同的段承担的职责是不一样的，我们有代码段、全局变量段、stack、heap、C标准库等各种段。尽管段内地址连续，但不同的段在内存中可以离散排布，我们依据不同的段对内存进行管理。<br><br>段式存储管理方式中，逻辑空间被分为了若干个段，每个段定义了一组完整逻辑意义的信息（代码段、数据段、堆栈段）。段式存储管理的优点是不同的段在加载进内存时不要求是连续的，但是某一个段内的内存需要是连续的。<br>
<br>分段：每个逻辑段都有一个段号。
<br>段内连续：段内的逻辑地址总是从 0 开始。
<br>段的大小：不同段的大小不完全相同。
<br>段表：段表用来记录每个逻辑段的编号。物理起始大小、段大小及访问权限等。<br>
下面举例用段表只展示出内存保护所需的段基址和段长部分。<br>
<img alt="Pasted image 20240801075550.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240801075550.png">
<br>分配方式：读取ELF头部，识别出需要加载的各个段，为每个段分配内存（查找、更新空闲分区表），最后更新段表。
<br>回收过程：更新段表-&gt;释放内存（更新空闲表）。
<br><br><br>逻辑地址由两部分构成：（1）段号，（2）段内偏移。段号决定虚拟内存中可划分多少个段，段内位移决定最小的段大小是多大。<br><br>
<br>防止越界
<br>访问权限（CPL 和 DPL，还有段属性是只读、只写还是怎样）<br>
<img alt="Pasted image 20240801075704.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240801075704.png">
<br><br><br><br>我们在 ICS 先导课中已经学过页式存储的基本原理，我们将进程逻辑地址空间划分成大小相同的块，我们称为页(page)；再把物理内存划分成若干相同大小的块，称为页框(page frame)。一个页框放一个页。当我们分配内存时，我们可以将进程的页面离散地存放到也框里面，通过页表保存页和页框的映射关系（虚拟地址到物理地址之间的映射）。<br><br>在32位机器上，逻辑地址占32bits，其中有页内位移12bits和页面号20bits。我们知道，页和页框中的页内位移是一一对应的。其中，逻辑地址通过页表映射出相对应的物理地址。因此要实现页式存储管理，页表中的页表项不需要记录页内位移的任何信息。而且虚拟地址空间是连续的，我们只需要在页表项中记录页框号和一些标志位即可。<br><br>32位机器上的页表项占四个字节，这里提供下面的图简单了解相关位的含义。<br><img alt="Pasted image 20240805013618.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240805013618.png"><br><br><br>32位机器上，一个进程的虚拟地址空间理论上是 4GB 空间，每个页 4KB 情况下可以划分出来 1M 个页出来。我们知道每个页表项需要 4B 空间，那么维护这一个进程的页表就需要占用 4MB 的内存空间（单级页表静态分配需要1000个连续页框的大小）。为了解决单个进程的页表占用大量内存的问题，通常采用多级页表(Multi-Level Page Table) 方案，我们会在之后的课程中学习。<br><br>当 CPU 想要访问一个逻辑地址时，会触发地址转换，这时需要访问物理内存两次。<br>
<br>访问页表：不难理解，你想从虚拟地址得到物理地址，你就需要从PTBR(Page Table Base Register)读取页表起始地址，然后从这个页表中查询页面号对应的页框号。随后得到物理地址。
<br>访问转换后的物理地址
<br>当 CPU 频繁访问内存时，页表查找的开销会显著增加。这时，我们采用一种更高效的解决方案——快表。我们将在下节课介绍。<br><br>
<br>分配过程

<br>读取ELF头部，识别出需要加载的各个段
<br>为每个段的虚拟地址划分页面
<br>为每个页面分配一个页框（查找并更新页框表）
<br>更新页表


<br>回收过程

<br>更新页表
<br>释放内存（更新空闲页框表）


<br><br><br>在虚拟地址空间中，我们把虚拟的地址空间划分成一个一个页，因而产生的逻辑地址很容易得出来：虚拟地址 = 页面号 + 页内位移。<br>
<img alt="Pasted image 20240801081814.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240801081814.jpg"><br><br>页表项处理页框外，还有很多标志位来提供更为精细的访问控制。<br>
<br>P(Present)位：表示页是否在物理内存中（1表示页在内存中）
<br>R/W(Read/Write)位：控制页的读写权限（0表示可读/1表示可读写）
<br>U/S(User/Supervisor)位：控制页的访问权限（0表示只有内核能访问/1表示用户也可以访问）
<br>A(Accessed)位：指示该页是否被访问过（页的置换算法）
<br>D(Dirty)位：指示该页是否被写过（write-back）
<br>NX(No Execute)位：表示该页是否可执行（64位机器）
<br><br><br>我们已经学习了段式内存管理和页式内存管理。在 ICS 先导课中，我们结合32位机器（IA-32）进行了学习。在32位机器上，段页式内存管理应用非常普遍。在64位机器上，页式存储管理方案更为常见。常见的分页表结构有三种，分别是：<br>
<br>分层分页(Hierarchical Paging)
<br>哈希分页表(Hashed Page Tables)
<br>倒排分页表(Inverted Page Tables)
<br>在后续的课程中，我们将探讨分层分页的页式存储管理方式，包括快表和多级页表等。<br><br><br><br>学习页式存储管理方案中，每当CPU 访存时，访问页表会为机器带来额外的开销（两次访存）。为了减少这种开销，设计人员使用 Cache 来减少时间上的开销。现代计算机使用地址转换旁路缓冲存储器，由于合理使用TLB能够将查询页表的时间开销降低到原先的5%甚至以下，由此得名快表。<br><br>TLB&nbsp;是用 SRAM 做成的一种全相联存储器（命中率高）。TLB 中存储最近使用的页表项，加快了地址的转换速度（减少访存次数）。因为是全相联存储器，所以它会将 CPU 送来的页号和 TLB 中缓存的所有页号同时比较（硬件实现复杂，成本高）。如果找到匹配条目即为命中（TLB hit）。<br>若命中(hit)，就会直接返回匹配条目的物理地址，这种情况当然最好。根据匹配条目的位置，我们有两种不同的结局：<br>
<br>物理地址中的数据在缓存中，不需要访问内存。
<br>如果物理地址中的数据块不在缓存中，则需要额外访问一次内存。
<br>若不命中(miss)，就会再查看页表，转换物理地址的同时将页表项写进 TLB。物理地址中的数据在缓存中，需要访问一次内存。<br>
<br>物理地址中的数据不在缓存中，页表命中。产生两次访存。
<br>页表不命中。最差的情况，需要访问磁盘。<br>
<img alt="Pasted image 20240805022104.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240805022104.png">
<br><br>TLB 的命中率对系统性能至关重要，越高的命中率就以为这更多的地址转换可以在 TLB 中完成，减少了访存的次数（访问内存页表）。<br>通常情况下，TLB的命中率非常高。一般在 90% 到 99% 之间。影响的因素有多个：<br>
<br>程序的访问模式（程序的局部性原理）
<br>TLB 的大小和结构
<br>要保持TLB命中率，我们还需要注意：当发生进程切换时，会使所有的 TLB 条目失效。<br><br>
<br>TLB 的造价高：SRAM的全相联存储器
<br>TLB 的功耗高：能占到微处理器总功耗的 10% 以上
<br><br><br> 在上节课快表的学习中，我们用快表（一种cache）解决了页表所带来频繁的访存开销。在本节课中，我们学习多级页表，了解一下多级页表是如何优化页表所带来的存储开销的。<br><br>现如今，16GB、32GB甚至64GB的主机内存屡见不鲜。对于本节课，我们看到页表的分级可能不太理解，我们继续用 32 位 4GB 的虚存举例子。没有页表的分级结构前，我们需要 4MB 的连续内存来存储每个进程的页表。<br>现在的内存价格大约是 15rmb/GB，换算成 rmb/MB 更加便宜。而在1985年 Intel 386 推出时，内存的价格是骇人的 500$/MB。当时Windows 1.0 (Nov 1985) 只需要 192KB RAM 就可以运行。我们可能感觉不到那是一个多么黑暗的年代，但是我们能够感同身受的是多级页表的存在真的非常必要。当时的 i386 使用的就是二级页表。<br><br>在 x86-32 架构下，一个进程的虚拟地址空间的理论大小为: 假设一个页面大小为 4KB，可划分出来的页面数量就有 1M 这么多，每个页表项 4B ，就会产生 1M*4B = 4MB 的连续空间大小。如果系统中有100个进程，那么即使什么都不做，都会用掉400M的连续内存。而进程一般不会用掉4GB的内存，因而有很多空洞页面。也就是说，每个进程是用不掉这1M个这么多的页表项的。<br>但是在单级页表下，即使进程不访问主存，也要为进程分配足够的页表项来覆盖整个虚拟地址空间。（连续存储）<br>而我们可以将页表的内存空间再进行分页，就可以把这些页表页离散的存到内存里了。这样不仅仅有助于提高内存的利用率（将大象分块放入冰箱），同时操作系统也可以实现按需分配页表空间了。刚刚单级页表中，操作系统需要为那些空洞页面也分配页表项，这太浪费空间了！当操作系统可以按需分配页表后，就可以完全不为空洞页面分配页表项，节省内存。<br>为了实现页表的分页，我们当然还需要额外地准备一张页目录表(Page Directory Table)，用来记录页表页存放的页框号。（二级页表）<br><br><br>先将页表分页，本来 4MB 大小的页表可以分成 1K 页，每个页目录表(page directory/first level page table) 就只占 4KB 的空间，刚好是一个页的大小。其中每个页目录表项有 4B，记录页表页的存放的页框号，页目录表项就是 PDE(Page Directory Entry)。<br>
<img alt="Pasted image 20240924011424.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240924011424.jpg"><br>
每个页目录表项都对应着一个页表(page table)。我们知道，一个页表可以映射4MB的内存，那么使用二级页表就可以通过第1级页表映射第2级页表，第二级页表进一步映射有：那这4GB虚拟内存需要全部分配吗？当然不！在下图中，红色的部分就是页目录表项所记录的空洞页框号，在主存中并不用分配所对应的页框号，在页表中不需要给这部分空间分配页表项。所以操作系统就不用给这部分空间分配页表页(second level page table)，直到进程申请到了这部分的内存（按需分配）。<br><img alt="Pasted image 20240814201027.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240814201027.png"><br>
从上面的图中，我们看到页目录表的出现好像为内存增加了额外的负担，但别忘了这是操作系统不需要为空洞页面分配相应页表页的前提下的。如果进程只是用了 1GB 的虚存空间，那么用这 4KB 去换取 3MB 的页表页不载入内存好像是一个不错的交换。<br><img alt="Pasted image 20240814203923.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240814203923.png"><br><br>进入 64 位架构的时代后，进程的虚拟地址空间剧增（用户区和内核区各占256TB）。如果我们仍然使用二级页表的连续页框来存储，那么将产生：的二级页表空间，我们的内存显然是不够用的，因而出现了多级页表。我们在这里介绍四级页表。解决方法简单粗暴——对页目录表继续分页。在四级页表中，页面大小仍然是 4KB，从顶至底的页表分别是：<br>
<br>PGD（9bits）：Page Global Directory
<br>PUD（9bits）：Page Upper Directory
<br>PMD（9bits）：Page Middle Directory
<br>PTE（9bits）：Page Table Entry
<br>Page（12bits）
<br><br>优点：<br>
<br>分散的页表管理
<br>支持非常大的地址空间<br>
缺点：
<br>地址转换时间增加（通过增加TLB优化）
<br>开销增加
<br>复杂性增加
<br><br><br>Caching是一种用于提高数据访问速度和系统性能的技术。在中学时，我们常常把正在用的书放在面前，把常用到的书摞在一起放到课桌上，不常用的书放到书包里。这就是一种cache。在我们学习快表的时候，我们知道当 TLB 命中率越高，机器的性能就越好。（因为cache常用更快的存储介质）<br><br>局部性原理，也叫引用局部性。分为时间局部性和空间局部性。局部性原理在计算机科学中的应用是非常广泛的，不仅仅应用在替换算法中。局部性原理还体现在存储器层次结构和编译器优化上。<br><br>时间局部性指的是，如果某个数据在被访问过一次，那么在不久的将来很可能还会被访问。当我们将这些数据放在 cache 中不进行替换，就会大大增加系统的运行性能。我们用一个例子来说明：<br>#include &lt;stdio.h&gt;

int main() {
    int i, j, sum = 0;
    int matrix[1000][1000];
	// matirx的初始化
	for(){
	...
	}
	
    for (i = 0; i &lt; 1000; i++) {
        for (j = 0; j &lt; 1000; j++) {
            sum += matirx[i][j];
        }
    }

    printf("Value of sum is: %d\n", sum);
    return 0;
}

<br>在这个例子中，变量&nbsp;sum&nbsp;和&nbsp;j&nbsp;是经常被访问的数据。将这些数据保存在缓存中可以显著提升性能，因为它们在短时间内被多次访问。。<br><br>空间局部性是指，当某个数据被访问到后，它周围空间之后一段时间也很有可能被访问到。继续上面的例子，如果将循环中变量 i 和 j 进行如下改动，那么空间局部性就会变得非常差。因为每次访问的地址都不是连续的，而且每一行都有1000个元素（4000字节），所以完全利用不到 cache 这个层次，总是访问主存甚至磁盘。<br>    for (j = 0; j &lt; 1000; j++) {
        for (i = 0; i &lt; 1000; i++) {
            sum += matrix[i][j];
        }
    }
<br>我们看到，尽管先按列访问的方法的作用和按行访问相同，但是由于空间的局部性，它们对 cache 利用率会大不一样，因此列访问效率会不如行访问的效率。<br><br>引用局部性是计算机系统中的一种可预测行为，强局部性的系统非常适合通过 cache、预取等技术进行性能优化。局部性的典型应用有层次化存储。<br>层次化存储是一种利用时间和空间局部性的硬件优化，可用于多个层次的内存结构中。层次化存储系统通过将存储器分为多个层次，每个层次都有不同的速度和容量，来提高内存访问效率。层次化的存储有：<br>
<br>CPU寄存器：存储速度最快，速度有限（8-256个）
<br>L1 Cache：每个核心各有一个，容量32KB-512KB，速度很快
<br>L2 Cache：多个核心共享，容量128KB-24MB，速度比L1 Cache慢
<br>L3 Cache：所有核心共享，容量2MB-64MB，速度比L2 Cache慢
<br>主存储器：容量大，但存取速度慢
<br>磁盘存储：容量巨大，存取速度最慢
<br>云存储：容量无限制，存取速度根据网络状况而定
<br>根据局部性原理，现代计算机会将那些更频繁使用的数据放在高层级的存储器中。如果高层级存储器空间不够用，就会用内存置换算法将存储空间中一部分内容对换到下层次的存储器中。在替换的过程中难免会有cache数据一致性的问题，我们用替换策略和写策略来做相应应对。<br><img alt="Pasted image 20241123015750.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241123015750.jpg"><br><br>缺页是指当程序访问的页面不在物理内存中时，操作系统需要从磁盘中将该页面调入内存的情况。这种情况会导致程序暂时停止执行，直到所需页面被加载到内存中。缺页特指主存到磁盘存储这一层次的页面缺失。缺页会引起中断，使操作系统介入将页面加载到内存里，由于涉及到磁盘这一层次，所以缺页非常慢，高频次的缺页可能引起性能的大幅下降。<br>缺页这类现象并不局限于主存-磁盘的存储结构，从Cache的结构到主存也会涉及到“缺页”相关的现象。只不过主存以下的缺页所带来的系统开销足够小，并不需要引发操作系统的重视。<br><br>缓存写满时，就要再写入新数据就必须选择 cache line 进行替换，此时就要考虑替换策略：<br>
<br>FIFO：最早进入缓存的数据最先被替换。这种策略简单易实现，但可能不总是最优的，因为最早进入的数据不一定是最不常用的。
<br>LRU：最近最少使用的数据被替换。这种策略更符合局部性原理，但实现起来相对复杂，需要维护每个缓存行的使用时间戳或链表。
<br><br>当一个数据再缓存中被修改了，那么对应内存数据如何处理也成了一个问题。一般情况下，我们有这两种写策略：<br>
<br>Write-Through：每次写操作都同时更新缓存和主存。虽然数据一致性高，但由于每次写操作都涉及主存，速度较慢。
<br>Write Back：每次写操作只更新缓存，只有缓存行被替换时才写回主存。这种策略速度快，但需要额外的机制来确保数据一致性，例如脏位（Dirty Bit）来标记哪些缓存行需要写回主存。
<br><br>在之前的学习中，我们了解了虚拟内存和进程的分页机制。这些机制共同作用，使系统可以加载比有限内存大得多的程序。当缺页发生（page fault），操作系统将缺失的页面加载进内存。然而这个过程是有代价的，我们这小节比较不同的算法，看看哪种算法下的hit ratio最高。<br>本小节我们将着重关注当主存空间满时，页面是如何置换到磁盘中，为后来加载的页面腾出空间。当CPU要访问某一页面时，如果页面在内存中就Hit，如果不在页面中就Miss。对于页面置换算法而言，命中率越高，表明性能越好，反之，性能越坏。<br><br>先进先出算法是简单的队列算法(Queue)，队列的数据结构插入和删除发生在不同的端头，在尾部插入，在头部删除(Enqueue in rear, dequeue in front)。这种结构确保了最早进入队列的元素能够得到最早的处理，在进程调度、打印任务管理的场景得到了很好的应用。这种结构是否适用于内存的置换呢？我们最关心的是这种结构的命中率如何。我们接着看。<br>
<img alt="Pasted image 20240923222601.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240923222601.png"><br>
凡是缺页的就标记为Miss，命中的标记为Hit。如果队列满且下一次页面访存Miss，那么FIFO页面置换算法的操作系统就会选择最早进入内存的页面进行置换。我们直觉上就会感到这种算法很不靠谱，事实上也确实不靠谱。FIFO页面置换算法的命中率相比其他算法是相当低的。<br>而且FIFO还会引发反直觉的Bélády's Anomaly现象。主要原因是FIFO只考虑了页面的时间序列，而没有考虑页面的引用局部性。<br><br>贝莱蒂现象得名于匈牙利科学家贝莱蒂发现的一种现象，即随着分配页面的增多缺页率反而增加。这是十分反直觉的，这种现象主要发生在FIFO算法中，原因我们上文也已给出。在OPT算法、LRU算法等考虑时间局部性的算法中几乎不会发生这种现象。<br>下面我们用一个例子直观感受一下这种现象。假设当前的队列为空，我们下面即将访问页面的顺序是：1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5。当队列中页框数为3时，我们有：<br>
<img alt="Pasted image 20240923230804.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240923230804.png"><br>
我们看到，在第一个例子中，我们总共发生了9次page faults。但是下图中，我们将队列中页框数增加到4时，我们看到，缺页次数不但没有下降，反而增加了。<br>
<img alt="Pasted image 20240923230816.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240923230816.png"><br>
“基于栈”的页面置换算法就不会受到贝莱蒂现象的影响。因为这些算法在实现时使用了类似栈的数据结构来管理页面。具体来说，这些算法维护一个页面栈，栈顶的页面是最近使用的，而栈底的页面是最久未使用的。下面我们将会学习到OPT算法和LRU算法，这两个算法都是stack-based algo。<br><br>FIFO算法最大的优点就是实现简单，但是没有考虑到页面的引用局部性是FIFO最大的缺点。我们选用页面置换算法的目的是提高主存的命中率，从而减少访问磁盘的次数，使得机器的存取速度更接近主存的存取速度，进来提高机器的速度。<br><br>时钟算法是FIFO的改进版，除了考虑页面的时间顺序，同时也照顾到了页面的引用情况。在之前的学习中，我们知道页表项(PTE)有一个属性位(Accessed bit)，1表示访问过，0表示未访问过。CLOCK算法就是在FIFO算法的基础上为每个页面增添一个引用位(reference bit)，和页表项中的accessed bit类似。<br>由于是FIFO算法的改进，CLOCK会按照FIFO的顺序检查每一个页面的reference位，如果reference位为1，则将此位设置为0，然后检查下一页。如果页引用位是0，就选中该页作为淘汰页。这种方式给了页第二次驻留内存的机会，因此也称为二次机会算法。<br>当检查完队列最后一个页面就会循环检查第一页。（Like a CLOCK spining again and again）<br><br>CLOCK+算法是对CLOCK算法的一种改进，旨在延迟页面的置换操作。假如某个页面在加载进内存后被修改，那么它的dirty位就会置为1。我们知道，如果页面置换策略采用回写策略(write-back)，则表示只有页面置换到磁盘上时才会将数据进行同步。<br>如果我们结合页的accessed bit和dirty bit两个属性，我们一共可以得到四种情况：<br>1. A = 0, D = 0    // 最先淘汰
2. A = 0, D = 1
3. A = 1, D = 0
4. A = 1, D = 1    // 最后淘汰
<br>
<br>第一轮检查：<br>
按照FIFO顺序检查1类页面，如果检出就淘汰<br>
如果没有检出就进行第二轮检查
<br>第二轮检查：<br>
按照FIFO顺序检查2类页面，如果检出就淘汰<br>
如果没有2类页面，则将后续页面的A位置为0<br>
如果第二轮结束依然没有找到二类页面，则重复第一轮
<br><br>最佳页面置换算法会选择在将来最长时间内不会访问的页面进行替换。这种算法虽然确实是一种 optimal algorithm，理论上这种算法的命中率是最高的，但是这种算法过于理想了。<br>因为预知未来将要访问的页面在实际的应用中几乎是不可能的。而且即使在模拟环境中实现 OPT 算法，所需要的计算资源也将是巨大的。所以 OPT 算法一般就作为 optimal algorithm 来判断一个页面置换算法的优劣。实践上，我们会用 OPT 算法与其他的算法进行比较。为其他算法提供一个理想的参考标准，用来评估其他页面置换算法的性能。<br><br>最近最久未使用算法算得上实际生活中页面置换算法的 GOAT ，LRU会替换最近最少使用的页面。这个根据是程序的局部性原理，即最近使用过的数据在未来一段时间后仍可能被使用。<br>在算法的实现中，尽管我们不能预测未来，但是我们仍然可以观察过去。我们可以通过过去访问的页来指定未来哪些页需要被先置换出去，哪些页应当保留。要实现LRU，我们需要额外地维护一个链表，用于保存刚刚访问的页面及最久未被访问的页面。且每次访问页都可能对链表排序进行调整，又是额外的时间开销。<br><br><br><br><br><br><br><br>我们终于将虚拟内存作为单独的专题进行讨论了。我们先回顾一下为什么我们需要虚拟存储器。在早些时候，内存容量很小，所以我们需要二级存储器来作为后备力量。现在虽然我们有 16/32GB 甚至更大的主存容量，能够容纳大多数的软件，但随着软件数量的增多和体积的增大，这么大的主存容量可能还是不够用。<br>那么如果我们只将程序的一小部分放在主存中呢？根据程序局部性原理，我们当然可以这样做，而且带来的好处远远大于整个程序的 swapping 所带来的系统开销。先导课程中，我们介绍过早期的虚拟内存实现。当程序不再需要完全放在内存中时，这不仅节省了内存空间，还减少了每个程序使用的主存空间，同时也减少了使用 IO 进行 swapping 的次数（每次只需加载程序的一部分）。<br><br>虚拟内存(VM) 是操作系统管理内存的一种技术。通过将小部分的程序调入内存中运行，它可以向应用程序提供一个独享、连续且巨大的内存空间。尽管内存无法容纳所有进程，但虚拟内存提供了一种非常有效的方法来应对这种情况。即按需调入，并将长时间未访问的页调出到磁盘中。利用磁盘的大容量来运行程序。<br>
<img alt="Pasted image 20241112175054.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241112175054.png"><br>
我们前面提到了页表、虚拟内存的思想blablabla，我们来总结一下虚拟内存怎么用：<br>
<br>段检查（Segmentation Check）：每次CPU要访问某个内存地址的指令或数据时，内存管理单元（MMU）都会检查这个内存引用是否有效。MMU会先进行段检查，如果段不可用，就会终止程序并抛出segmentation fault。
<br>缺页处理（Page Fault Handling）：如果段地址有效，但引用的页不在内存中，就会发生缺页。这时操作系统会找到一个空闲页框，请求磁盘读操作（可能还会写），然后将新页面载入内存。
<br>更新页表和恢复执行：当页面加载完成后，操作系统会更新页表并记录相关信息。最后，重新对该虚拟地址进行访问，继续执行程序。
<br><br>由于虚拟内存涉及到内存-磁盘这一层次的存储层次，当部分加载进内存的程序发生缺页时会发生什么呢？当进程访问的虚拟页不在物理内存中就会产生页错误。当缺页发生时，会：<br>
<img alt="Pasted image 20240808004636.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240808004636.png"><br>
<br>发生缺页，向操作系统报告。随后操作系统接管并保存运行上下文，操作系统figure out这是一个缺页中断。随后MMU检查这个地址是否合法。
<br>然后就是决定加载页面到内存的某个页框中（使用页面替换算法）。在选择装入前先检查一下要替换的页面是否被修改过（脏位是否为1）。

<br>如果没有修改过，跳到第 5 步。


<br>如果修改过，先将该页面写回到磁盘中。磁盘写请求放在一个队列中。然后等待磁盘写执行，CPU这时会做其他的事情。当磁盘写完成之后会向操作系统发出中断信号。
<br>保存寄存器和其他进程状态（为什么？）
<br>发起磁盘读操作，将磁盘中的数据读到内存的空闲页框里。和前面的写操作一样，对磁盘读的请求会放在一个队列里，在磁盘读操作进行的过程中，CPU做其他的事情。
<br>当磁盘完成I/O操作后，向系统发生中断信号。
<br>如有必要，保存寄存器和其他进程状态。（为什么？）
<br>最后更新页表，恢复执行。
<br><br>：只有在需要的时候才从外存中加载到内存。（处理 page fault 就是一种 lazy approach）<br><br>尽管缺页需要这么多步才能恢复执行，但实际上对磁盘（HHDs）的操作是最耗费时间的。重启进程和内存的管理需要耗费 1μs - 100μs。然而，磁盘的延迟有 3ms（3000μs），寻道时间 5ms（5000μs），而传输时间只用 0.05ms（50μs）。所以，当缺页中断率很高时，性能会非常非常差。（一般而言，缺页中断率会控制在这个水平）这也就是为什么说page fault is painful。<br>现在，我们大多使用SSDs这种介质作为二级存储器，相比于HHDs，SSDs是一种更快、更可靠的存储解决方案。SSD的延迟通常在几十微秒（μs）范围内，远低于HHDs的毫秒（ms）级别延迟。SSD没有机械部件，因此没有寻道时间，数据传输速度也更快。这使得SSD在处理缺页中断时的性能显著优于HHDs。<br><br>当缺页(page fault)或换入换出(swapping)发生时，操作系统会进入内核态来处理这些事件。处理这些事件会有一定的时间开销。如果缺页的发生频率过高，CPU就不得不花费更多的时间来处理这些换出操作。这种由于换出过于频繁而导致系统性能大幅下降的现象称为抖动。<br>
<img alt="Pasted image 20240924163417.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240924163417.png"><br><br>引发抖动的原因可能有多个，比如并发进程数量、内存分配策略、页面调入策略、页面置换算法等。上节课我们已经了解过页面置换策略是如何影响页命中率的，下面我们着重来看内存分配策略和调页策略对抖动的影响和如何避免抖动。<br><br>OS为进程分配内存有两种策略，固定页框(fixed allocation) 和可变页框(variable allocation)。固定页框是指操作系统为每个进程分配一组固定数量的物理页框（物理内存块），且在进程运行期间一直保持这样数量的物理页框。固定页框策略要求每个进程只能使用自己分配的页框，不能使用其他进程的页框。等分页框(Equal Allocation) 就是一种固定页框，系统中的所有物理页框平均分配给每个进程。<br>可变页框是操作系统根据进程的需求动态地分配物理页框，进程运行期间，页框的数量可以增加或减少。比例分配(Proportional Allocation) 是可变页框的一种，进程分配的页框数量可以根据进程的大小和进程数量来动态调整。<br>固定页框实现简单，管理方便，因为每个进程的内存需求是预先确定的。但是这种分配策略的灵活性很差，可能导致内存利用率不高。如果分配的页框数量过多，会浪费内存；如果分配的页框数量过少，可能导致频繁的缺页中断。<br>可变页框的优缺点完全和固定页框的实现反着来，可变页框灵活性高，可以根据实际需求调整内存分配，提高内存利用率。但是实现复杂，需要更复杂的管理机制来跟踪和调整页框的分配。<br><br>当缺页中断发生，操作系统需要swapping需要置换的页。我们也有两种相关的实现方式：局部置换和全局置换。<br>局部置换是指当发生缺页中断时，操作系统只在当前进程的物理页框中选择一个页面进行置换。这样，每个进程只能使用自己分配的物理页框，不会影响其他进程的内存使用。如果某个进程的内存需求突然增加，可能会频繁发生缺页中断，影响该进程的性能。<br>全局置换是指当发生缺页中断时，操作系统可以在所有进程的物理页框中选择一个页面进行置换。这样，操作系统可以动态调整各个进程的物理页框数量，根据实际需求进行分配。内存利用率更高，可以更好地适应不同进程的内存需求变化。可能导致进程之间的相互干扰，一个进程的内存需求增加可能会影响其他进程的性能。<br>为什么固定页框和全局置换是矛盾的。<br><br>请页式(Demand Paging) 是当进程需要访问某个页面而页面不在内存中时，产生缺页中断，操作系统将该页面从磁盘调入内存的页面调入策略。只在需要时加载页面，减少了不必要的内存占用，但是也可能导致频繁的缺页中断，影响系统性能。<br>调页式(Pre-paging) 是在执行访问页面之前将之后要访问的多个页面提前加载到内存中。如果预测准确，就会减少缺页中断的频率，如果预测不准确就会加载不必要的页面，浪费内存资源。<br><br>除了上面的因素，页面置换算法也是影响系统发生抖动的重要因素。页面置换算法不同，缺页次数就不相同。我们熟悉的LRU就是一种好的页面置换算法。<br><br><br>工作集模型基于局部性原理，根据局部性大致地给出程序之后最有可能访问的页有哪些。根据工作集，我们能够减少缺页的同时尽可能地节省内存页框的资源。<br>为确定工作集窗口，我们检测最近使用过的页，看看哪些页的使用是最频繁的，之后将工作集设置为局部时间Δ内引用最多的页面的集合（如下Δ = 10）。不难理解，如果某页面被频繁地访问，那么就会出现在工作集中。<br><img alt="Pasted image 20240925010529.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240925010529.png"><br>当工作集一旦被确定，系统就可以在进程启动或重启之前，根据这些历史访问数据将工作集中的页面预先调入主存（预调页的实现）。进程开始执行时就能减少缺页中断的发生，提高了系统性能。<br><br>通过之前的学习，我们了解了抖动问题的根源在于频繁的页面置换，即缺页中断频率（PFF）。如果一个进程的PFF过高，这表示系统给该进程分配的页框过少；如果系统给当前进程分配的页框非常多，就不会引起频繁的换入换出。<br><img alt="Pasted image 20240925004924.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240925004924.png"><br>
随着进程分配页框数的增加，PFF一定会减少。我们发现PFF会根据页框数呈类似于1/N的减少趋势。基于此，我们可以划定合适的上限PFF(upper bound)和下限PFF(lower bound)。根据这两个边界划分进程分配页框数。当分配的页框数过少时，PFF会显著增加（超过upper bound），从而影响性能；而当分配的页框数过多时（小于lower bound），会浪费内存页框，性价比不高。<br><br>在程序中申请内存时，我们操作的是虚拟内存。操作系统负责将这些虚拟地址映射到物理内存或磁盘空间（mmap()）。虚拟内存区域的分配只需确保不与现有映射冲突，无需采用物理内存管理中的best fit等算法。由于虚拟内存无需考虑碎片问题，所以通常使用更高效的方式（如 first fit ）管理地址空间。<br>我们有两种不同的虚拟内存分配方式：静态的和动态的。<br><br>静态内存分配是在编译时分配内存的方式。程序在编译时确定变量的虚拟地址布局（如 .data 段、 .text 段等），其生命周期覆盖整个程序运行周期。物理内存的实际分配由操作系统在程序加载时完成，且内存大小不可在运行时调整。<br><br>如果你需要在运行时申请内存（堆内存），你就会用到动态内存分配函数在程序运行时根据需要分配和释放内存。常见的动态内存分配函数包括 malloc 、calloc 、realloc 和 free。动态内存分配的灵活性高，可以根据程序的需要动态调整内存大小。但需要手动管理，容易发生内存泄漏。<br>在 Linux 系统中，malloc 库函数会在底层调用 brk（小于 128KB 的内存申请） 和 mmap 系统调用（大于 128KB 的内存申请）。（一般情况下）<br><br>它们的函数原型如下：<br>#include &lt;unistd.h&gt;
int brk(void *end_data_segment);
/*
Parameters:
	1. end_data_segment: Pointer to the new end of the data segment. This value is interpreted as the new program break.
   
Return value: Returns 0 on success, otherwise -1 and errno is set to indicate the error.
*/

void* sbrk(intptr_t increment);
/* 
Parameters:
	1. increment: The amount by which to increase or decrease the program break. If the value is positive, the break is increased by increment bytes. If the value is negative, the break is decreased by increment bytes.
   
Return value: Returns the previous program break on success. On error, (void *) -1 is returned, and errno is set to indicate the error.
*/
<br><br>mmap 我们在 IPC 章节已经介绍过，这里不在赘述。<br><br><br><br>在本阶段，我们一直讨论有关虚拟内存怎么实现、段、页等等，但是我们的讨论仿佛一直局限在用户程序。你有没有好奇过内核在哪里存放？内核代码也需要向用户代码那样按需分配么？本节课，我们就来回顾前面阶段的知识并探讨下一些重要又有趣的细节。<br>i386采用二级页表<br>CR3 Register用于存储页目录表的基地址。<br><br>0-1M<br><br>物理地址1M往上就是内核代码和数据，再往上就是内核分配的一些数据结构，用来存放页目录或管理物理内存的结构。在往上就是空闲的物理内存。在IA32上，虚拟内存大小为4GB，其中前面的0GB-3GB我们会将其划分给用户程序，将高地址的3GB-4GB划分给内核程序。Linux将内核的1GB内存分为两部分：<br>
<br>低端地址是内核虚拟空间的低896MB，与物理内存一一映射；
<br>空闲的128MB是高端内存(High memory)，用来处理当物理内存大于896MB的情况。
<br>/* This file contains the definitions for memory management in our OS. */

/* *
 *     Virtual memory map:
 *     4G ------------------&gt; +---------------------------------+ 0xFFFFFFFF
 *                            |         High Memory (*)         | 128M
 *     KERNTOP -------------&gt; +---------------------------------+ 0xF8000000
 *                            |    Remapped Physical Memory     | KMEMSIZE(896M)
 *                            |                                 |
 *     KERNBASE ------------&gt; +---------------------------------+ 0xC0000000(3G)
 *                            |        Invalid Memory (*)       | --/--
 *     USERTOP -------------&gt; +---------------------------------+ 0xB0000000
 *                            |           User stack            |
 *                            +---------------------------------+
 *                            |                                 |
 *                            :                                 :
 *                            |         ~~~~~~~~~~~~~~~~        |
 *                            :                                 :
 *                            |                                 |
 *                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                            |       User Program &amp; Heap       |
 *     UTEXT ---------------&gt; +---------------------------------+ 0x00800000
 *                            |        Invalid Memory (*)       | --/--
 *                            |  - - - - - - - - - - - - - - -  |
 *                            |    User STAB Data (optional)    |
 *     USERBASE, USTAB------&gt; +---------------------------------+ 0x00200000
 *                            |        Invalid Memory (*)       | --/--
 *     0 -------------------&gt; +---------------------------------+ 0x00000000
 * (*) Note: The kernel ensures that "Invalid Memory" is *never* mapped.
 *     "Empty Memory" is normally unmapped, but user programs may map pages
 *     there if desired.
 *
 *
 *    physical memory:
 *     4G -------------  ---&gt; +---------------------------------+ 0xFFFFFFFF
 *                            |           外设映射空间            |
 *                            |                                 |
 *     384M ----------------&gt; +---------------------------------+ 0x20000000
 *                            |           空闲内存~382M          |
 *                            |                                 |
 *     pages end -----------&gt; +---------------------------------+ pages end
 *                            |    npages*sizeof(struct Page)   | -- (768KB)
 *     kpgdir end ----------&gt; +---------------------------------+ kpgdir end
 *                            |           kern_pgdir            | -- PGSIZE
 *     bss end -------------&gt; +---------------------------------+ bss end
 *                            |           kernel code           |
 *     1M ------------------&gt; +---------------------------------+ 0x00100000
 *                            |           BIOS ROM              |
 *     960KB ---------------&gt; +---------------------------------+ 0x000F0000
 *                            |           16位外设,扩展ROMS       |
 *     768KB ---------------&gt; +---------------------------------+ 0x000C0000
 *                            |           VGA显示缓存            |
 *     640KB ---------------&gt; +---------------------------------+ 0x000A0000
 *                            |           bootloader            |
 *     0  ------------------&gt; +---------------------------------+ 0x00000000
 *
 * */
<br>由于是一一映射的方式，内核的虚拟地址减去KERNBASE就可以得到内核的物理地址了，方便内核对实际的内核物理内存进行管理。<br><img alt="Pasted image 20240924020957.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240924020957.png"><br>而对用户程序的地址空间会根据用户程序的情况把内存与物理内存空间进行映射。<br>--- 如何知道某个物理页可以释放？页结构体中有一项 uint_16t pp_ref; 表示当前页有多少个虚拟内存映射到该页代表的物理内存页。当 pp_ref 为0时就代表该页可以释放掉了。<br><img alt="Pasted image 20240924021026.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240924021026.png"><br><img alt="Pasted image 20240924023629.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240924023629.png"><br><br>在进程控制块 task_struct 中，我们有 mm_struct，它是一个描述进程内存管理信息的数据结构，用来管理用户进程的虚拟地址空间。mm_struct 包含了进程的地址空间信息，包括代码段、数据段、堆、栈等。它还包含了与内存管理相关的其他信息，如页表、内存映射等。<br>而在用户区的所有段，操作系统都会为其建立一个 vm_area_struct，存放在 mm_struct 中的 mmap 链表中，这是一个描述进程虚拟内存区域的数据结构。vm_area_struct 包含了虚拟内存区域的起始地址、结束地址、权限等信息。<br>对于有些数据，vm_area_struct 会和实际的物理内存建立映射。而如一次性申请 1GB 的堆内存时，操作系统只会分配一个 vm_area_struct 结构体，并不会立马与实际的物理内存建立映射关系。只有当使用时才会与物理内存建立映射关系（缺页异常并建立映射）。<br>此外，mm_struct 中还有一个 mmap_cache 字段，用于缓存最近访问的虚拟内存区域，以提高内存访问的效率。]]></description><link>https://congzhi.wiki/congzhi's-os-series/12.-memory-management.html</link><guid isPermaLink="false">Congzhi's OS Series/12. Memory Management.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 01 Jun 2025 02:01:43 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240728054403.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240728054403.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[15. File Systems]]></title><description><![CDATA[ 
 <br>第一遍重写中<br><br><br>我们使用计算机，对文件并不陌生。文件有许多不同的格式。在完成程序设计实验中，我们就接触了 .c/.cpp 源代码文件，编译生成 exe 或 elf 格式的可执行文件格式，还有一般实验报告递交的 doc 文件格式。我们的生活离不开这些由文件系统管理的不同类别的文件。此外，文件系统还涉及文件的检索和保护，影响系统的性能和可靠性。对于操作系统，文件系统至关重要。<br><br>内存是计算机的核心部件之一，然而内存并不能提供持久性的存储，内存中的数据信息会由于系统的关闭或掉电而丢失掉。为了避免这种情况，我们需要一种能够在掉电后保证数据的完整性的存储介质。实际上，不易失的存储介质有很多，HDD、SSD、CD等。在文件系统的应用上，现多采用HDD和SSD作为文件系统的存储介质。<br><br><br><img alt="ssd.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/ssd.jpg"><br><br>既然我们说到了文件系统，那我们的话题必然离不开文件。我们时时刻刻在使用着各种各样的文件，那文件是什么？文件是存储在永久性存储介质上相关数据的集合。文件可以是常见的文本、照片、视频，还可以是系统配置、代码等信息。文件的功能有：数据存储、数据交换和数据保护等。<br><img alt="Pasted image 20241126212945.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241126212945.jpg"><br><br>文件元数据(File Metadata)，也被称为文件属性(File Attributes)，包含对文件特征的描述信息，文件元数据并不作为文件数据，而是存储在文件系统的目录结构中。常见的文件属性有：<br>
<br>文件名：人类可读文件的标识符，区别于其他文件。
<br>文件标识符：唯一的文件id，与其他文件进行区分。
<br>文件类型：操作系统需要文件类型信息才能提供相应的支持。
<br>文件路径：文件在文件系统中的位置。
<br>文件大小：文件占用存储空间的大小。
<br>时间戳：记录创建、修改的时间等。
<br>链接计数
<br>文件权限
<br>文件所有者和用户组
<br>访问次数等。
<br><br>通过对文件的类型进行分类，文件可以被分为普通文件、目录文件、符号链接文件、特殊文件、套接字文件和管道文件。<br>文件类型是无穷多的，而操作系统并不能直接识别和解析所有类型的文件。虽然操作系统可以识别文件的基本类型（例如，通过文件扩展名或文件头信息），但要打开和正确显示特定类型的文件就需要安装相应的软件。典型的有Java的可执行字节流，这是一种二进制程序文件，但要是系统上没有安装 JVM，操作系统就无法直接运行这些Java字节码文件。<br><br>常规文件是文件系统中标准的文件类型，也是最常见到的文件类型。常规文件类型包括文本文件、二进制文件、图像文件、音频文件、视频文件等。为了操作系统能够加载并执行程序，所以操作系统必须至少完全支持一种可执行文件类型。<br><br>目录文件(directory files) 是一类特殊的文件，它记录着文件目录(file directories) 的信息。文件目录不仅仅包含数据，也包括对其他文件和子目录的引用。在 Linux 中，目录文件会被标注d属性，说明这个文件的内容实则是一个目录项清单，可用ls "directory file name"命令查看这个清单。<br>一般而言，目录文件中的内容我们一般是无法查看的，但我们可用通过ls命令来列出目录文件中引用的文件，也可用cd命令来进入目录文件记录的文件目录中。<br>du@du-virtual-machine:~/Desktop/OS$ ls -alh
total 136K
drwxrwxr-x 3 du du 4.0K  8月 23 00:50 .
drwxr-xr-x 7 du du 4.0K  8月  8 05:09 ..
drwxrwxr-x 2 du du 4.0K  7月  1 00:51 a_file_dir
<br>d属性就表示这是一个目录文件。文件目录以目录文件的显示存储在磁盘上。我们可以通过tree命令以树状结构显示目标目录及其子目录中的所有文件和目录。<br><br>符号链接实际上就是软连接(soft link)，之后再介绍文件目录的时候会提到。<br><br><br>套接字和FIFO文件在IPC的阶段<a data-href="7. Inter-Process Communication" href="https://congzhi.wiki/congzhi's-os-series/7.-inter-process-communication.html" class="internal-link" target="_self" rel="noopener nofollow">7. Inter-Process Communication</a>里我们已经了解过了。虽然它们的地位较为显赫，但此处不做赘述。<br><br><br>在用户眼中，文件系统为我们提供了许多的便利，通过系统为我们提供的窗口（如：open()等的系统调用），用户并不需要了解这一切是怎么实现的。然而，负责文件系统结构设计的系统设计人员要做的可就多了。接下来，让我们从宏观的角度一层一层地了解下文件系统结构的设计。<br><img alt="Pasted image 20240824180225.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240824180225.png"><br><br>文件都是存放在磁盘上的，文件系统的职责是把各种各样的信息数据存放到非易失的存储介质上面。作为I/O设备，我们知道用户任何的请求都会转换成控制磁盘I/O的低级硬件指令交给I/O控制器处理。I/O控制层会收到basic file system layer的高级指令（如：read disk block 1），然后将其转换成磁盘能够理解的指令。又由于外设的低速性，我们对磁盘的操作一般是DMA的I/O控制方式。<br>具体来说，应用程序先是发出读文件的请求，这些请求经过层次转换到磁盘驱动程序将转换后的磁盘控制器命令交给CPU，然后CPU下达DMA控制操作，DMA控制器直接在内存和磁盘之间传输数据，而不需要CPU的持续干预。这种方式大大提高了数据传输效率，减少了CPU的负担。<br><br>基本文件系统负责与硬件交互，是文件系统中开始与硬盘硬件对接的层次。它会向磁盘驱动程序发布指定扇区的读写命令，之后由磁盘控制器驱动程序将命令传给磁盘控制器，控制器执行并返回结果，最终将结果交给操作系统。在这一层次中，对文件的操作是在物理磁盘上的，类似于磁盘1、12号柱面、7号磁道、1号扇区这样精确的磁盘访问请求就发生在这个层次。<br>由于磁盘访问的低速，在系统中实际上存在了许多buffers和caches来应对访问磁盘带来的开销。所以在这一层次，我们还需要考虑这些系统中存在的buffers和caches。只有当前要访问的数据不在这些buffers/caches中时，我们才需要考虑访问磁盘。也因此，我们需要承担突然断电时带来的数据丢失问题。<br><br>用户总希望自己的文件在系统中是连续存放的，正如我们之前学习虚拟存储技术时了解过的一样，即便我们看到的文件是连续的，但文件在磁盘上的存储块不一定是连续的。因此我们需要一个模块将逻辑块转换成底层硬件能够识别的物理块。<br>这种逻辑到物理上的转换就是由文件组织模块(File organization module)，也叫块管理器提供的。它是一个映射器(mapper)，将逻辑块地址映射成物理块地址。文件组织模块维护一个映射表，将文件的逻辑块转换为磁盘上的物理块地址，同时它也管理空闲空间列表，用于跟踪为分配的物理块。<br>除此，这一模块还负责空闲空间的管理，我们之后介绍。<br><br>对于用户而言，一个屏蔽底层硬件、简单、易操作的交互接口和能够对文件的属性进行方便管理的接口是ta们最想要的。而这个为用户提供方便接口的文件系统层次就由逻辑文件系统(Logical file system) 所提供。<br>逻辑文件系统负责管理文件的元数据。管理文件路径解析、文件描述符、权限检查等，并提供文件的操作接口（打开、创建、读和写）为用户使用，用户可以用系统调用（open/read/write等）与该层次交互。该层次也支持对文件的逻辑访问（逻辑上的文件）。<br>文件的元数据存放在一个叫文件控制块的东西里面，即FCB，在Unix中，文件控制块也叫inode。<br><br>文件系统非常多，我们有UFS、AFS、ZFS、NTFS、ext2/3/4、FAT32/64等等。这些文件系统各有不同，但是总有一些共同点，比如跟踪磁盘块的个数，多少个空闲块，多少个已被占用的块；还要管理文件目录的结构和各种文件。<br><br>在<a data-href="5. System Boots Up" href="https://congzhi.wiki/congzhi's-os-series/5.-system-boots-up.html" class="internal-link" target="_self" rel="noopener nofollow">5. System Boots Up</a>阶段中，我们谈到了计算机上电启动的一些内容。要使计算机正常启动，我们至少需要一块磁盘分区(volume)来存放操作系统的相关数据。我们谈到了磁盘分区上的第一个扇区，也叫启动扇区(boot sector)。当我们将视线放宽到整个volume，我们会看到：<br>
<br>引导区(Boot block)：包含启动代码和磁盘信息，有MBR和GPT的实现方式。
<br>目录区(Directory area, FCB)：一级、二级、树形等目录实现方式。
<br>空闲空间表(Free space table)：管理数据区，有位图、空闲链表、成组链接等实现方式。
<br>数据区(Data area)：连续、链接、索引等实现方式。
<br>在FAT的课程中，我们会结合FAT文件系统来进行介绍。<br><br>除了将一块磁盘一整块地使用，我们还可以逻辑上将磁盘分成若干个不同的区块来使用，这种方法也被称为磁盘分区。我们学习过 MBR 和 GPT 两种磁盘分区的方式。简单来说，磁盘分区就是在存储设备上创建一个或多个独立区域的过程，每个区域都可以作为独立的逻辑磁盘被管理使用。<br>为了管理这些不同的分区，我们需要一个分区表(partition table)，有时也被称为超级块(super block) 或 master file table。分区的相关信息就记录在分区表中，这些信息有开始和结束位置、分区类型和大小等。<br>在Windows中，磁盘分区常常与盘符相关联，每一个分区对应一个盘符（如 C: 、D:、E:等）。虽然一个物理磁盘可以分出多个盘符，但通常而言，一个物理磁盘我们当成一个分区使用。而在Linux系统中，磁盘通常会被分成多个分区，每个分区有不同的用途。例如，根分区（/）、引导分区（/boot）、交换分区（swap）等。<br><br>磁盘分区表的位置和大小会随着分区方式的不同而不同。我们学习过 MBR 和 GPT 两种分区模式。我们下面介绍这两种方式分区表存放位置。<br>主引导记录存放在磁盘的第一号扇区中，它包含了引导代码和分区表。MBR的大小通常为512字节，其中的bootloader占据446字节，分区表占据64字节。但如果系统采用 GPT 的分区方式，第一号扇区的大小就可能是 4KB 大小。<br>GPT(GUID Partition Table)：<br>
<br>LBA0：为了保证向下兼容，第一个扇区 LBA0 存放保护性 MBR。
<br>LBA1：存放 GPT header。
<br>LBA2-LBA34：存放GPT分区条目数组（即GPT分区表）。
<br>磁盘末尾：分区表的备份。
<br><br>和我们在一台电脑上可以安装多个操作系统一样，我们同样可以在操作系统上安装多个文件系统。但是每个分区只能有一个文件系统。多个分区我们可以有很多文件系统的话，问题就来了。在用户程序中我们使用的是统一的系统调用。当我们用统一接口的系统调用接口访问不同分区（文件系统）中的文件时，如何保证文件的正确访问？<br>我们的解决方案是：增加一层抽象：虚拟文件系统。我们将不同的文件系统统一交给VFS进行管理。VFS是文件系统之上的一层内核软件层，用于处理 POSIX 文件系统相关的系统调用，给各种不同的文件系统提供统一的操作接口，使得应用程序可以不必关心底层使用哪个文件系统。用户程序可以通过统一的接口来访问不同分区（文件系统）中的文件，而不会遇到兼容性问题。<br><img alt="Pasted image 20240824184231.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240824184231.png"><br>VFS的作用有两个。一个是上面我们所提到的，无所谓下面文件系统是怎么实现的，我们都可以通过VFS提供的统一接口来对文件进行操作。另一个是将系统中全部的文件进行唯一化标识。VFS通过使用 vnode 数据结构来唯一标识系统中的所有文件。<br>在传统文件系统中，FCB用于在分区内唯一标识文件，但在不同的分区中，FCB的唯一性无法得到保证。VFS通过 vnode 数据结构解决了这个问题，每个文件对应一个唯一的 vnode，使得即使在不同文件系统甚至在remote file system中，文件也能够被唯一标识。<br><br><br><br>对于文件，我们有很多操作，如创建文件、读文件、写文件、移动文件指针的位置(repositioning)、删除文件和文件的截断(truncation)。文件系统为我们提供了这些基本的文件操作的系统调用，下面我们来一起看一看。<br><br>当你对文件进行操作，你需要使用文件描述符(file descriptor)来对文件进行操作。在POSIX系统调用open()中，open()会返回给我们一个fd。而在C标准库中，我们会使用fopen()，其会返回一个指向FILE结构的指针。你可以用int fd = fileno(f);将FILE转换成fd。<br>#include &lt;stdio.h&gt;

// Use C standard library opening a file.
FILE* f = fopen(argv[1], "r");
if(f == NULL) {
	printf("Unable to open the file %s.\n", argv[1]);
	return -1;
}
/* Read the file.*/
fclose(f)

/*
Mode options in fopen() function:
r  : Open the file for reading only.(File must exist, or return NULL.)
w  : Create the file for writing. If file exist, it's overwritten.
a  : Open the file, writing data at the end of the file.(a for append)
r+ : Open for read and update.(File must exist, or return NULL.)
w+ : Create the file for reading/writing. If file exist, it's overwritten.
a+ : Same as above, but in rw mode.
*/
<br>#include &lt;stdio.h&gt;
#include &lt;fcntl.h&gt; // For open(), O_RDONLY, O_WRONLY, O_RDWR, and file access modes
#include &lt;unistd.h&gt;// For close(), read(), write(), fsync(), lseek(), and others.

// Use POSIX system call opening a file.
int fd = open(argv[1], O_RDONLY);
if (fd == -1) {
    perror("Unable to open the file");
    return -1;
}
/* Read the file.*/
close(fd);
<br>以上是两种打开文件的形式，第一种使用C标准库，提供了一种更高级易用的接口，而底下我们使用POSIX系统调用，相比之下更为直接，但是不够user friendly。因为，用户并不知情FILE实际上是一个整数。我们之后会了解到，文件描述符是一个表示打开文件的整数。<br>以下是打开和关闭文件的POSIX系统调用原型函数：<br>#include &lt;fcntl.h&gt; // For open(), O_RDONLY, O_WRONLY, O_RDWR, and file access modes
#include &lt;unistd.h&gt;// For close(), read(), write(), fsync(), lseek(), and others.
#include &lt;sys/types.h&gt; // For data types used in some system calls
#include &lt;sys/stat.h&gt;  // For file status and mode information

int open(const char *pathname, int flags, ...);
/* Parameters:

1. pathname: Pointer to the name of the file to be opened.
2. flags: File access modes and other flags.
   Common flags:
   - O_RDONLY: Open for reading only.
   - O_WRONLY: Open for writing only.
   - O_RDWR: Open for reading and writing.
   - O_CREAT: Create the file if it does not exist.
   - O_EXCL: Ensure that this call creates the file (fails if the file exists).
   - O_NOCTTY: If the pathname refers to a terminal, do not make it the controlling terminal.
   - O_TRUNC: Truncate the file to zero length if it already exists and is opened for writing.
   - O_APPEND: Open the file in append mode.
   - O_NONBLOCK: Open the file in non-blocking mode.
   - O_DSYNC: Write operations will complete according to the requirements of synchronized I/O data integrity completion.
   - O_SYNC: Write operations will complete according to the requirements of synchronized I/O file integrity completion.
   - O_RSYNC: Synchronize read operations.
   - O_DIRECT: Minimize or eliminate cache effects of the I/O to and from this file.

Return value: Returns a file descriptor on success, otherwise -1.
*/
<br>int close(int fd);
/* Parameters:

1. fd: File descriptor of the file to be closed.

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>以下是读写文件和移动文件指针的 POSIX 系统调用。<br>off_t lseek(int fd, off_t offset, int whence);
/* Parameters:

1. fd: File descriptor of the file.
2. offset: Offset to set the file position to.
3. whence: Starting point for the offset (SEEK_SET, SEEK_CUR, SEEK_END).

Return value: Returns the resulting offset location on success, otherwise -1.
*/

ssize_t read(int fd, void *buf, size_t count);
/* Parameters:

1. fd: File descriptor of the file to read from.
2. buf: Buffer where the read data will be stored.
3. count: Number of bytes to read.

Return value: Returns the number of bytes read on success, otherwise -1.
*/

ssize_t write(int fd, const void *buf, size_t count);
/* Parameters:

1. fd: File descriptor of the file to write to.
2. buf: Buffer containing the data to be written.
3. count: Number of bytes to write.

Return value: Returns the number of bytes written on success, otherwise -1.
*/

int fsync(int fd);
/* Parameters:

1. fd: File descriptor of the file to be synchronized.

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>remove 函数和 unlink 函数在删除文件时的行为是相同的。当我们使用 remove 或 unlink 删除文件时，实际上是删除了文件系统中的一个硬链接。如果文件有多个硬链接，只有指定的那个链接会被删除，文件的内容仍然存在，直到所有的硬链接都被删除为止。<br>#include &lt;stdio.h&gt;
int remove(const char *filename);
/* Parameters:

1. filename: Pointer to a null-terminated string that specifies the name of the file to be deleted.

Return value: Returns 0 on success, otherwise -1.
*/
<br>#include &lt;unistd.h&gt;
int unlink(const char *pathname);
/* Parameters:

1. pathname: Pointer to a null-terminated string that specifies the name of the file to be deleted.

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>我们用文件描述符来对文件进行操作。而fd只是一个整型数，实际上并不能唯一的标识一个文件。就相当于厕所隔间里面的人，在某一刻，你能够确定厕所里就是这个人，你用xx号厕所来标识这个人，但是当这个人离开厕所，这个文件描述符将会失效。即在不同时刻，不同的文件可能使用相同的fd。<br><br>在第一节课，我们提到了文件的元数据，相当于文件的各种信息，这些信息被存储在文件控制块中。操作这些文件的创建修改删除是logical file system layer的工作。当我们要创建一个新的文件，对应着的，一个文件控制块也会随之被创建。系统内，每个文件的文件控制块都是唯一的。<br><img alt="Pasted image 20241127205724.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241127205724.jpg"><br>当用户想要读取系统中的某个文件时，操作系统就会把文件的文件控制块加载进内存。由于文件控制块中有文件的元数据，所以系统可以根据文件控制块中文件在磁盘上的位置来加载文件信息。<br><br>Inode实际上就是类Unix系统中的文件控制块。我们将在后续ext文件系统的学习中进行介绍。<br><br>由于文件系统要频繁地与磁盘进行交互，为了提升存储的性能，我们当然的会想到局部性原理，即caching来提升性能。实际上，文件系统的确有很多caching策略指导的结构。我们有：<br>
<br>Mount Table
<br>Cache
<br>Global Open File Table
<br>Process Open File Table
<br>Buffers
<br>为了避免频繁的访问I/O，这些文件系统的结构会被加载进内存中。我们将用演示文件的操作过程演示打开文件表是如何提高系统的效率的。<br><br>我们之前说，进程想要操作文件，就需要FCB中的信息。FCB载入内存需要我们使用open()系统调用，但之前，我们看到open系统调用返回的是fd，和FCB有什么关系？我们接着看。当进程打开文件时，OS就会在进程打开文件表里添加一个新表项。之后，操作系统会检查系统级打开文件表，如果没有找到关于该文件的表项，OS就会创建新的系统打开文件表项来跟踪文件的FCB。<br><img alt="Pasted image 20241128230955.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241128230955.jpg"><br>
进程打开文件表中存放着系统打开文件表表项的一个索引（fd），便于进程可以通过这个索引找到全局的系统打开文件表里面的表项，进一步对文件进行操作。进程打开文件表每个进程都有一张，每一个条目都对应着进程打开的文件和指向系统级打开文件表的指针，一般而言，进程打开文件表项数为1024，当然你可以将其设置的更高。include/linux/fdtable.h<br>当FCB加载到内存中后，系统用系统打开文件表对FCBs进行管理，系统打开文件表整个系统只有一张。系统打开文件表项的数据有文件名、文件打开方式、文件偏移量、文件的引用计数和FCB指针等信息。include/linux/fs.h<br>open文件调用会返回一个指向进程打开文件表中一个特定表项的索引，我们称其为文件描述符/文件句柄（unix中叫做file descriptor，而在windows中叫file handle）。<br><br>当进程要访问文件时，系统会根据文件的目录在磁盘上找到该文件的文件控制块。之后，将所需要的FCB都缓存在内核内存的FCB表/Inode表。然后系统在系统打开文件表中由相关的系统调用信息和FCB信息创建新的表项，其中有文件的一些元数据和一些其他信息。<br><img alt="Pasted image 20241018011136.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241018011136.png"><br><br>当我们使用open("example.txt", O_RDONLY);时会发生以下情况：<br>
<br>进程调用 open("example.txt", O_RDONLY)。<br>
2. 操作系统检查系统级打开文件表，发现 "example.txt" 已经被打开。<br>
3. 在进程级打开文件表中增加一个条目，指向系统级打开文件表中的 "example.txt" 条目，然后在全局的系统打开文件表中对文件的引用计数加一。<br>
4. 返回文件描述符 fd，指向进程级打开文件表中新增加的条目。
<br>操作系统检查系统级打开文件表，发现没有打开 "example.txt" 条目
<br>在磁盘目录中搜索 "example.txt"，如果找到了，那就在系统级打开文件表增加一个条目，将FCB加载到FCB表中，初始打开文件表项。最后将系统的打开文件表中对文件的引用计数设置为1。
<br>增加该进程级代开文件表的条目，用指针指向系统级打开文件表的对应条目。
<br>返回文件描述符 fd，指向进程级打开文件表中新增加的条目。
<br>在打开文件表中，除了文件的一些元数据外，还要存储文件引用计数信息，即这个文件被系统内的进程打开了多少次。每当使用open()或close()系统调用时，这个数值就会发生改变。只要不为0，该文件的元数据就会一直保留在内存的内核区中。<br><br>实际上，多个FD(file descriptor) 是可用指向相同的OFD(open file table descriptor)。像如下fd1和fd20都指向偏移为23的OFD（用dup2()复制文件描述符），还有两个进程的fd2都指向偏移为73的OFD（用fork()创建新的子进程）。<br><img alt="Pasted image 20241018011136.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241018011136.png"><br><br>偏移为0的OFD和偏移为86的OFD指向的却是同一个Inode。为什么？这可能是因为两个进程都独立的打开同一个文件。当不同文件描述符的行为不同时，即使多个文件描述符指向同一个文件，每个文件描述符的行为可能因为其具体的使用方式和上下文而有所不同。<br>比如两个文件描述符维护不同的文件偏移量或是不同的文件状态标志，就会出现多个OFD却指向同一个inode的情况。（说的简单一点就是数据结构不再相同了）<br><br><br><br><br>提到目录，大家都不会陌生。在小学一年级，老师就教过我们用字典目录去查汉字，拿到新的语文课本，我们可能会翻阅目录寻找自己感兴趣的文章去阅读。和”目录“这么久的交情，我们不难给目录下一个定义：目录是一张对内容编排和组织的表，便于我们更好的找到特定的内容。<br><br>文件目录就是一张编排组织文件的表（数据结构），便于用户查找相关的文件信息。文件目录容纳其他文件和目录（文件夹），将这些文件和文件夹用某种结构组织起来。有了这种组织，文件和目录管理起来就更加便捷。<br><br>对于文件目录，文件系统需要支持关于目录的很多操作。最基础的就是用文件名来遍历查找文件，这是目录最基本的作用。此外，我们还想知道目录下有哪些文件，所以我们需要支持列出目录文件的功能。要查找文件，我们就得先创建文件，所以还需要支持文件的创建、删除和文件名的修改。<br>在命令行下，我们能见到很多目录相关的命令，例如：<br>
<br>查找文件find、locate
<br>创建文件touch
<br>删除文件rm
<br>显示当前目录pwd
<br>列出目录文件ls
<br>文件重命名mv、rename
<br>创建目录mkdir
<br>删除目录rmdir、rm -r
<br>切换目录cd
<br><br>文件的元数据存放在FCB中，文件系统通过FCB来实际控制管理一个文件；而目录是编排组织文件的表，通过目录中对相关文件的指针索引，实际上让我们得以访问对应文件的FCB，从而获取文件的详细信息并进行操作。文件目录以目录文件的形式存储在磁盘上，目录文件存储着系统目录的数据结构信息。<br><br>为了更好的组织、管理和访问文件，我们选用很多数据结构对文件目录进行记录。以下我们介绍几种构建文件目录时常见的目录形式。<br><br>单级目录是最简单的目录结构，所有的文件都放在同一个根目录下。由于这种结构将所有文件放在一起，所以单级目录要求所有文件名必须是唯一的。这种结构是实现起来最简单的，但是文件的命名唯一而且对文件缺少归纳。导致这种目录管理文件就是一摊鸡毛。<br><img alt="Pasted image 20241202010255.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202010255.jpg"><br>在单级目录结构下，没有文件路径的概念。<br><br>在二级目录结构中，我们有一个主文件目录和子文件目录(sub-directories)。主文件目录可以向下级的用户文件目录提供索引。从而，每个用户可以拥有自己独立的目录来存储和管理文件。解决了一些单级目录中命名冲突问题，并为不同用户提供了文件隔离性。<br>从二级目录结构开始，文件路径有了意义，每个文件的文件路径(file pathname)在系统中都是唯一的。在引用文件时，如果确定当前的工作目录（用户目录），我们检索不包括用户名的文件时，操作系统会假设文件位于当前的工作目录下。这种不包含文件路径的路径名就是文件相对路径。<br>而绝对路径是指从根目录开始的完整路径，它包含了从根目录到目标文件或目录的所有目录名。例如，/user1/cat 就是一个绝对路径。无论当前工作目录是什么，通过绝对路径都可以唯一地定位到目标文件或目录。<br>在用户的目录下，用户无法创建新的目录来对文件进行进一步的归类。<br><img alt="Pasted image 20241202010304.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202010304.jpg"><br><br>树形目录是二级目录结构的升级版，树形结构目录允许子目录创建条目的子目录树，从而消除了文件不能归类的问题。整个目录树有一个根目录，所有的文件或目录都会有包含root的唯一路径名。树形结构为系统带来了更好的灵活性和可变性，在树型结构中，进程可以从一个目录下“跳跃”到另一个目录，也因而，相对目录通常上性能更好（减少了“跳跃”的动作）。在Linux下，我们用"."来指代当前目录（如./hello）。<br><img alt="Pasted image 20241202010310.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202010310.jpg"><br><br>上述的三种目录都不能够使得同一个文件/子目录在多个不同的目录下共享，用户访问其他用户的某些文件的愿望将无法得到满足。无环图目录允许一个文件/子目录同时存在于多个目录下，允许文件/子目录在用户多个间共享（不同目录访问同一个文件得到了实现）。<br><img alt="Pasted image 20241202010318.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202010318.jpg"><br>这种共享性有时候会用符号链接，也就是软连接实现，有时候也会通过硬链接实现。这两种不同的链接形式我们马上就会了解到。<br><br>在无环图目录结构中，目录不可以成”环“。然而这种环状目录在通用图目录结构中是允许的。环状目录或者成环其实就是目录中包含循环到起点的路径，即在下级目录中包含上级目录。<br><img alt="Pasted image 20241202231218.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202231218.jpg"><br>由于这种环状的存在，所以设计检索和遍历算法时就更加困难。这种环状可能导致遍历算法的无限循环，为了避免这种情况，设计者干脆禁止在目录中允许上级目录的存在。<br><br><br>AVL树(AVL Tree)是目录实现中常见的数据结构。<br><br>B树(B-Tree)极其变种也是目录实现中常见的数据结构。<br><br>在文件系统中，硬链接是一个指向数据块的直接引用，出现在inode中。硬链接使得多个文件名指向相同的inode，因此它们共享相同的数据内容。<br>软链接是一种特殊的文件类型，出现在目录项中，它包含指向另一个文件路径的指针，而不是直接指向数据块。因此，软链接相当于文件的快捷方式。<br><img alt="Pasted image 20240831003914.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240831003914.jpg"><br><br>在Linux中，我们用命令ln来创建一个硬链接，例如：<br>du@du-virtual-machine:~/Desktop/OS$ ./Hello
Hello world
du@du-virtual-machine:~/Desktop/OS$ ln Hello Hard_Link
du@du-virtual-machine:~/Desktop/OS$ ./Hard_Link 
Hello world
<br>创建硬链接之后，两个文件会共享相同的inode（如上面例子中的Hello和Hard_Link）。也就是说它们会指向相同的数据内容。我们用stat命令查看Hello文件的元数据，可得：<br>du@du-virtual-machine:~/Desktop/OS$ stat Hello
  File: Hello
  Size: 15960     	Blocks: 32         IO Block: 4096   regular file
Device: 803h/2051d	Inode: 1049216     Links: 2
Access: (0775/-rwxrwxr-x)  Uid: ( 1000/      du)   Gid: ( 1000/      du)
Access: 2024-08-31 00:06:02.431095349 +0800
Modify: 2024-08-31 00:04:56.176093580 +0800
Change: 2024-08-31 00:05:46.451336108 +0800
 Birth: 2024-08-31 00:04:56.128094303 +0800

du@du-virtual-machine:~/Desktop/OS$ stat Hard_Link 
  File: Hard_Link
  Size: 15960     	Blocks: 32         IO Block: 4096   regular file
Device: 803h/2051d	Inode: 1049216     Links: 2
Access: (0775/-rwxrwxr-x)  Uid: ( 1000/      du)   Gid: ( 1000/      du)
Access: 2024-08-31 00:06:02.431095349 +0800
Modify: 2024-08-31 00:04:56.176093580 +0800
Change: 2024-08-31 00:05:46.451336108 +0800
 Birth: 2024-08-31 00:04:56.128094303 +0800
<br>我们看到，这两个文件名所指向的数据都是一样的。我们看到Links字段为2，这表示当前有两个目录项指向这个文件，也是文件的硬链接数。当硬链接数为0时，文件系统就会释放并回收文件资源。<br><br>我们用ln -s命令来创建一个软链接：<br>du@du-virtual-machine:~/Desktop/OS$ ln -s Hello Soft_Link 
du@du-virtual-machine:~/Desktop/OS$ ./Soft_Link 
Hello world
<br>当软链接创建好后，软链接文件和源文件的inode并不相同。软链接本身是一个独立的文件，存储对源文件的路径引用。文件大小就是字符串的长度。我们用ls查看Soft_Link的大小，大小为5字节，正好是Hello文件名的长度。这里引用的是相对路径，但也可以引用绝对路径。<br>du@du-virtual-machine:~/Desktop/OS$ stat Soft_Link 
  File: Soft_Link -&gt; Hello
  Size: 5         	Blocks: 0          IO Block: 4096   symbolic link
Device: 803h/2051d	Inode: 1049994     Links: 1
Access: (0777/lrwxrwxrwx)  Uid: ( 1000/      du)   Gid: ( 1000/      du)
Access: 2024-08-31 00:43:40.242348874 +0800
Modify: 2024-08-31 00:43:40.238349935 +0800
Change: 2024-08-31 00:43:40.238349935 +0800
 Birth: 2024-08-31 00:43:40.238349935 +0800
du@du-virtual-machine:~/Desktop/OS$ ls -l Soft_Link 
lrwxrwxrwx 1 du du 5  8月 31 00:43 Soft_Link -&gt; Hello
du@du-virtual-machine:~/Desktop/OS$ ls -l Hard_Link 
-rwxrwxr-x 2 du du 15960  8月 31 00:04 Hard_Link
<br>当源文件被删除，软链接就会指向一个不存在的路径。这时，软链接会变为无效链接，这就是“断链”现象。<br><br><br>和内存的分配策略一样，文件系统在磁盘空间的利用上也有不同的分配方法。这节课我们将学习三种不同的分配方法，分别是contiguous、linked和indexed。但在此之前，我们先了解一下用户视角上的文件。<br><br>我们生活中的文件类型有很多种。您现在所看的算是一种线性的顺序结构，而当你看到这个系列，你可以直接点击阶段的大标题从而进入你想了解的阶段笔记进行学习，这是一种类似索引的结构。而在这个笔记中，我们有很多大标题小标题，这又可以看作是一种层次式的结构。在我们学习ELF文件时，文件中的数据又可以分段处理，这是一种分段式结构的文件。<br>文件的逻辑结构关注的是文件内容在用户视角下是如何组织的。即如何从软件层面组织和访问文件内容。上面提到过的四种逻辑结构有：<br>
<br>线性顺序结构/流式文件(Sequential Structure)：文件内容按顺序存储和访问，一般用纯 ASCII、Unicode 编写的字符文档的内容是按线性顺序存储的(能够cat的文件)。
<br>索引结构(Indexed Structure)：文件内容按索引表进行组织（如字典），索引表再指向实际数据的位置。索引结构允许快速定位到文件中某个特定数据上，常用于数据库文件或大型文件中，页表就是最常见的索引结构。
<br>层次式结构(Hierarchical Structure)：文件内部被组织成多级结构，每一级可以包含不同的数据段或子文件。层次化的逻辑结构有文件树形目录等。
<br>分段结构(Segmented Structure)：ELF 可执行文件就是这样分段的结构，文件中的内容被划分到各个独立的段中，每个段设置单独的属性分别管理。
<br><br>我们看到的文件（即逻辑上的文件）主要有两种访问方式：顺序访问和随机访问（直接访问）。还有其他的访问方式都是在这两种访问方式的基础上建立的。<br><br>顺序访问要求数据按照存储顺序从文件开头到结尾逐步读取或写入，你想看笔记的最有一小节时，你需要将笔记前面的内容全部加载到内存中。即便你只想要知道最后的内容，你也需要将文件整个加载。<br><img alt="Pasted image 20241202002753.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202002753.jpg"><br><br>随机访问则允许直接跳转到文件中的任意位置进行读取或写入。由于文件内容可以非顺序处理，不需要将不需要的文件部分加载进内存，所以访问速度相较顺序访问要快。由于程序的局部性原理，在我们加载一些大型程序到内存中时，可能并不会将文件整个的加载进内存，这就是一种随机访问的文件访问方式。<br><img alt="Pasted image 20241202002801.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241202002801.jpg"><br><br>我们前面提到了四种文件结构，对于这四种不太文件结构的文件而言，它们会遵循不同的访问方式。对于线性顺序结构的文件来说，由于要读取访问下一个字符数据必须先访问上一个字符数据。所以文件的访问必须是连续的，这不难理解。<br>对于索引结构的文件来说，索引表可以让我们访问文件的不同位置。然而，通过索引表跳转之后，文件的访问转而变成线性顺序的访问（宏观上跳转，微观上连续）。类似的，由于层次化的结构相当于多级索引，我们可以跳转到我们想要数据的最小目录下访问，这个访问是线性连续的。<br>对于分段结构的文件，我们可以将其理解成类索引的顺序结构文件。在 ELF 中，我们将文件中不同的内容划分到各个节中，这些段相当于独立的线性顺序结构的文件，但我们通过节头表定义并索引了文件中各个节的位置及属性。在 ELF 文件中，文件访问同样是节外跳转、节内连续。<br><br>文件存放在持久性的存储设备上，文件的物理结构关注文件在硬盘、SSD等持久存储设备上的实际布局方式。文件的物理结构对于存储介质的特性、文件系统的实现密切相关（SSD可以随机存取；硬盘是随机存取+顺序存取；磁带只能顺序存取），主要包括以下几种形式：<br>
<br>连续分配(Contiguous Allocation)
<br>链式分配(Linked Allocation)
<br>索引分配(Indexed Allocation)
<br>多级索引(Multilevel Index)
<br>混合分配(Hybrid Allocation)
<br><br>连续分配就是指文件占用一组连续的磁盘块。逻辑上相连的块在物理上也相邻。所以这种分配方式带来的好处就是当磁头读到的文件位于磁盘块b时，读取下一个文件块b+1时，磁头并不需要怎么移动，也就不存在寻道时间了（最小的寻道时间）。在这种分配方式下，磁盘的目录项包括：<br>
<br>文件名
<br>第一个块的起始地址，块地址 = 扇区id（块大小 = 扇区大小）
<br>文件长度（块长度）
<br>即你只需要两个参数（起始块地址和文件长度）就能够keep track of this file。如果你需要读取 b+n 磁盘块中的内容，你只需要将磁头移动到相应的位置上，而不需要顺序一格一格地移动磁头，算是一种随机存取。这种方式可以轻易地实现对文件的顺序和直接访问。<br>而且这种方式在检查文件访问是否有效时非常简单，假设文件长度是 n，文件的起始块地址是 b。如果访问的磁盘块地址&gt; b + n，那么就可以断定这个访问是不合法的。<br><img alt="Pasted image 20240825132025.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240825132025.png"><br>享受完顺序分配给我们带来的简单性，我们就需要来承担相应的缺点。在顺序分配方式中，我们如果有一个大小为 N 个磁盘块的文件，在存储这个文件时，我们需要考虑：<br>
<br>磁盘分区中有没有这么大的空间能够被分配？
<br>如果有，我们应该选哪个空间？
<br>在磁盘空间动态分配时也可以选用不同的分配策略（首次适应、最佳适应、最差适应）。显而易见的是，这种分配方式会造成不可避免的外部碎片(external fragmentation)，内部碎片(internal fragmentation)由于被控制在一个块的大小内，所以我们不需要注意。在这种分配方式下，如果文件很大，所有孔的容纳不下，那么文件将无法载入磁盘。<br><br>除了要应对文件太大没有孔能够容纳这么大的文件之外，连续分配还给我们带来动态存储分配(dynamic storage allocation) 的问题。这是不可利用的外部碎片给连续分配方式带来的，这意味着大文件的扩展可能是很困难的。<br>对于外部碎片的处理，我们在内存分配的时候介绍过紧凑技术(compaction)，那对于磁盘而言，通过移动磁盘块，将非空闲的磁盘块紧凑在一块，那将会为我们带来很大的一块空闲空间。然而，磁盘可比内存慢多了，这么做的代价将会非常大。（Disk defragmentation for more）<br><br>好，既然我们不确定文件有多大，也不想任何外部碎片的产生，我们为何不将文件块链式地存放在磁盘上呢？将文件进行分割成块，分配在多个离散的磁盘块中，通过指针连接。不会产生任何的外部碎片，而且文件扩展较为容易。如何？<br>顺序分配和链式分配的关系相当于数组和链表之间的关系。链式分配带来了诸多好处，其中最主要的就是避免了任何的外部碎片。但是由于需要指向下一个磁盘块的指针和不能随机访问的特性，使得链式分配方式会不可避免地占用一点空间用于存储指针，并且速度慢（存在寻道时间）。<br><img alt="Pasted image 20240825132044.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240825132044.png"><br>链式分配方式的一大缺点就是当你要找一个文件块，你就必须得顺着指针链条往上找。所以一般的实现上，通常会结合连续分配和链式分配的优点。即以簇(clusters) 为单位进行连续分配，我们可以设置为4个块1个簇，然后将这些单位链接起来。这种方法可以提高访问速度，同时减少碎片问题。<br><br>如果你是原始链式分配的原教旨主义者，你就会将自己困在无法随机访问带来的访问速度的困境中。然而，我们可以给每个文件建立索引表，把所有的指针都顺序地放在一个磁盘块中，记录文件各逻辑块对应的物理块。这样就可以实现随机访问文件了，吸收了顺序分配的优点；而且不会产生任何的外部碎片，同样结合了链式分配的优点。<br><img alt="Pasted image 20240825132254.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240825132254.png"><br>这种分配方式真不错，既保留了顺序连续分配的优点，也保留了链式分配的优点。但是有一点不足，即当文件变得很大，我们就没有办法对文件进行分配了，因为一个块能够索引的大小很有限。对于这种窘境，人们想出来很多办法。如：将index block最后一个index entry作为下一个index block的entry（linked scheme）；加入上级的index对index block进行索引（multilevel index）；将上述的方法结合起来，这也是inode所使用的方式（combined scheme）。<br><br>下图展示了unix inode的multilevel indexing，在本阶段后续的课程中我们会详细介绍。<br><img alt="Pasted image 20241128030948.jpg" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241128030948.jpg"><br><br><br><br><br>我们学习了文件系统的设计。其中负责空闲空间管理的是文件组织模块/文件组织层。这个层次不单单负责逻辑块到物理块的转换，也负责逻辑上对磁盘存储块的管理，之后对逻辑块的管理进一步映射到物理块上。为了确保对磁盘的空闲空间能得到有效的利用，我们有许多对空闲空间管理的方法。我们接下来一个一个地介绍。<br><br>位图是一连串比特位的集合。在位图法中，我们用一位bit代表一块磁盘块。Bit可以取0和1，因而我们用0表示空磁盘块，用1表示已使用磁盘块。如下的磁盘空间就可以用16bits的位图：1111 0001 1111 1001 来表示。<br><img alt="Pasted image 20240826135306.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240826135306.png"><br>这种方式无论是理解还是实现都非常的简单。一个bit表示一个磁盘块，空间上也很划得来，利用率肯定不低。然而，如果我们暂时只能用磁盘很小一部分区域，位图法就会占有不该占有的内存空间。而且位图的大小是固定的，如果磁盘扩展，就需要重新初始化位图。<br><br>为了扩充磁盘的便利，我们可以使用链表。空闲链表法的实现方式就是将所有空闲块用链表链接起来。空闲块中包含指向下一个空闲块的指针，在下图的实现中，空闲空间链接头指向 Block5，然后 Block5 的指针域指向 Block6 以此类推。<br><img alt="Pasted image 20240826135310.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240826135310.png"><br>相比于位图法，空闲链表法的空间利用效率不会浪费其不该占用的内存空间。而且动态分配很容易，因而也可以动态的增加磁盘空间。但是指针动态分配的便利性也会造成该方法的指针维护很复杂，而且并不适合遍历。<br><br>这是空闲链表法的变种。不同于空闲链表，在分块成组法中，每一个空闲块的地址存放在 n 个磁盘块中，通过将这 n 个磁盘块链接起来，我们就能够轻松的找到那些未分配的磁盘块了（相当于索引+链接）。这种方式看起来很不错，因为我们可以很轻松的分配一大片空闲磁盘块。但是缺点也很明显，每一次的分配都会使得整个列表重新修正整个list。<br><img alt="Pasted image 20240826173012.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240826173012.png"><br><br>在counting的方法中，系统会用特定的格式（start base address, # numbers of free blocks）记录这些连续的空闲块。如果大多数连续空间块的长度超过了1块，那么这个列表将会非常紧凑。为了保证搜索、插入和删除的高效性，系统通常会用一个平衡树中存储这些记录。保证操作的效率。<br><br><br><br><br><br>文件分配表(File Allocation Table, FAT) 是链式分配的变种。最早由微软在1970年代为MS-DOS操作系统开发。由于简单的设计，FAT适用于各种各样的存储设备如软盘、硬盘、移动存储设备等。FAT采用簇链式存储分配，但簇中不再存放下一个簇的指针信息了，FAT会额外使用一块空间专门记录这些簇指针的信息，也就是FAT表，这也是FAT文件系统的特点和命名由来。<br>FAT早期版本包括 FAT12 和 FAT16，应用于存储容量较小的设备上，随着存储设备容量的不断增加，FAT32 被引入，用于支持更大的分区和文件大小。今天，NTFS代替了FAT作为Windows系统上应用的文件系统，但FAT32仍广泛应用在U盘上（兼容性佳）。<br><br>FAT文件系统下，磁盘结构被划分成引导扇区、文件分配表FAT、根目录区和数据区域。我们接下来逐步介绍。<br><br>作为磁盘分区的开始，为了保证计算机的正常启动，传统的，我们将第一个扇区作为引导扇区使用（第二个扇区可能会作为冗余对引导扇区进行备份）。引导扇区包括我们介绍过的启动代码和磁盘参数块(disk parameter block)，在磁盘参数块中包含磁盘的信息和文件系统的基本信息。<br>磁盘信息有：每个扇区多少字节、每个track几个扇区、磁头信息等。文件系统的信息有：FAT表数量、每个FAT表的扇区数、根目录项的数量、簇的信息等。<br><br>在引导扇区之后，作为FAT的重中之重，我们通常会看到两个相同的FAT表用于冗余和数据的恢复。每个FAT表都记录了磁盘上所有簇的状态和链表信息。FAT表是一个数组，其中的每个元素对应磁盘中的一个簇，元素的数值代表了该簇的状态：<br>
<br>值为0：簇空闲（空闲空间管理）
<br>值为正整数：簇已被占用，其中的数字指示下一个簇的编号
<br>值为EOF(-1)：文件结尾
<br><img alt="Pasted image 20240825132134.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240825132134.png"><br>通过把这些指针信息整合到一块，实际上FAT会比传统的簇链式方式要快很多。因为操作系统可以将整个FAT都加入到内存中（FAT不会很大），避免了对I/O的重复访问。<br><br>根目录区域(Root Directory)：存储文件和目录的目录项信息，包含文件名、扩展名、属性、创建时间、起始簇号等。<br><br>之后的一大片空闲的磁盘块就都用来作为数据区域(File and Directory Area)。这个区域存放文件和目录的数据内容。文件的数据被分配到多个簇中，簇之间通过FAT表链接。<br><br>我们常常会看到FAT16/FAT32，FAT后面的数字代表什么呢？这代表着FAT表中表项的位数，即FAT能够寻址的簇数。通过这个信息和簇大小信息，我们就可以得到最大分区大小是多大了。文件大小根据目录表项（FCB）中文件大小字段的位数决定。<br><br>FAT16的FAT表项是16bits，代表着每个表项可以表示  个不同的值，即65536个簇。FAT16通常使用32KB大小的簇。所以最大分区大小是：而目录项有32Bytes，其中文件大小属性占4Bytes（也就是32位）。所以单个文件大小为：虽然单个文件大小能达到4GB，但受限于最大分区大小，FAT16的单个文件最大为2GB。<br><br>同样，在FAT32中，FAT表项是32bits，但是实际上用于表示簇的有效位只有28位，其余的位数用来标记EOF和保留位。其余部分和FAT16相同的情况下，最大分区大小为：这可比4GB大多了，所以在FAT32中单个文件最大是4GB，和文件系统支持的最大文件大小相同。<br><br>我们现在能够明白FAT文件系统分区大小和单文件大小限制的因素有哪些，并且是如何影响FAT最大分区大小和最大文件大小的。然而FAT变种有很多，因此FAT16/32的最大文件大小或最大分区大小并不是固定的。根据不同的实现方式，所得出来的结果也是不一样的。<br><br>扩展文件系统是Linux系统中常见的文件系统，ext系统算是一种类UNIX文件系统的文件系统，ext的出现就是为了克服MINIX文件系统的一些缺点。在ext文件系统中的块(blocks)、索引节点(inodes) 和目录和一些文件所有权/访问权限(ownership/access)、链接(symbolic/hard links) 等都是继承传统UNIX文件系统的概念。<br>ext2是原始版本ext的重写，这些类UNIX文件系统的ext特点就是作为核心地位的“inode”。Ext2文件系统在1990年代初到2000年代初的近十年间，作为Linux的文件系统被广泛使用。后来，它被支持日志功能的文件系统ext3和ReiserFS所取代。<br><br>在ext2文件系统中，磁盘空间被划分为连续的逻辑块(blocks)。块大小并不需要与磁盘的物理扇区大小相同，因此块的大小还能更大，以便优化文件系统的性能和数据管理效率。在ext文件系统中，超级块负责卷空间信息的记录和管理。块大小由超级块中特定的字段所设置。<br>块大小一般会设置为是1024字节、2048字节或4096字节（ext2中块最大可以设置为65536个字节）。一旦文件系统初始化完成，块大小就将固定。块大小越小，存储文件产生的存储碎片就越少，但是相应地就会带来额外的管理开销（而且文件大小和文件系统大小就越小）。当前，主流的默认块大小是4KB。<br><br>块组是许多个块聚合在一起所构成的。磁盘空间被划分成了若干个块组，这样组织有效的避免了文件分散在磁盘的各个位置，从而减小了磁头的寻道时间；同时，也对碎片化有重要的影响。除此之外，块组中包含有一些重要数据的备份（如超级块等），使得文件系统能够在必要的时刻重建。<br><img alt="Pasted image 20241205203358.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241205203358.png"><br>磁盘被划分为若干个块组。每个块组由一定数量的块(Blocks)构成，包含：超级区块(Super Block)、组描述符表(Group Descriptor Table)、区块位图(Block Bitmap)、索引节点位图(Inode Bitmap)、索引节点表(Inode Table)、数据块区(Data Blocks)。<br>当一个文件的数据量超过一个块组的容量时，文件的数据块会分布在多个块组中。inode中的块指针可以指向不同块组中的数据块，从而实现跨块组的存储。一般而言，块组描述符表还会有一个备份，一个标准的块组0磁盘布局如下（N&gt;&gt;n）：<br><br><br>块组描述符表存储每个块组的基本信息，如inode表的起始块号、空闲块位图的起始块号等。<br><br>如果块大小是4KB，那么数据块位图和inode位图就能够表示  个的数据块和inode。在ext2/3中，inode占128B，那么inode表就需要占用  的磁盘空间。剩余数据块理论大小可达  。<br>上面这些是理论值的计算，实际上，我们可以设置inode和块数存在一定的对应关系。比方说一个块组中最多存放32个文件，那么inode表就只需要4KB的磁盘空间。<br><br>ext2文件系统所有核心配置信息都存储在超级块中，它包含了如文件系统中的inode总数、块总数、块大小、空闲块和空闲inode数量等信息，对于文件系统的正常运行和管理至关重要。超级块位于卷偏移1024字节后（boot loader之后），由于超级块的重要性，在最初的ext2版本中，每个块组中都会留有超级块的备份。之后的版本将组号为0、1还有奇数的平方作为超级块的备份块组使用。<br>ext2文件系统的超级块以小端方式存放在磁盘上，所以文件系统在不同机器上是可移植的。<br><br>struct ext2_super_block {
	__le32	s_inodes_count;		/* Inodes count */
	__le32	s_blocks_count;		/* Blocks count */
	__le32	s_r_blocks_count;	/* Reserved blocks count */
	__le32	s_free_blocks_count;	/* Free blocks count */
	__le32	s_free_inodes_count;	/* Free inodes count */
	__le32	s_first_data_block;	/* First Data Block */
	__le32	s_log_block_size;	/* Block size */
	__le32	s_log_frag_size;	/* Fragment size */
	__le32	s_blocks_per_group;	/* # Blocks per group */
	__le32	s_frags_per_group;	/* # Fragments per group */
	__le32	s_inodes_per_group;	/* # Inodes per group */
	__le32	s_mtime;		/* Mount time */
	__le32	s_wtime;		/* Write time */
	__le16	s_mnt_count;		/* Mount count */
	__le16	s_max_mnt_count;	/* Maximal mount count */
	__le16	s_magic;		/* Magic signature */
	__le16	s_state;		/* File system state */
	__le16	s_errors;		/* Behaviour when detecting errors */
	__le16	s_minor_rev_level; 	/* minor revision level */
	__le32	s_lastcheck;		/* time of last check */
	__le32	s_checkinterval;	/* max. time between checks */
	__le32	s_creator_os;		/* OS */
	__le32	s_rev_level;		/* Revision level */
	__le16	s_def_resuid;		/* Default uid for reserved blocks */
	__le16	s_def_resgid;		/* Default gid for reserved blocks */
	/*
	 * These fields are for EXT2_DYNAMIC_REV superblocks only.
	 *
	 * Note: the difference between the compatible feature set and
	 * the incompatible feature set is that if there is a bit set
	 * in the incompatible feature set that the kernel doesn't
	 * know about, it should refuse to mount the filesystem.
	 * 
	 * e2fsck's requirements are more strict; if it doesn't know
	 * about a feature in either the compatible or incompatible
	 * feature set, it must abort and not try to meddle with
	 * things it doesn't understand...
	 */
	__le32	s_first_ino; 		/* First non-reserved inode */
	__le16   s_inode_size; 		/* size of inode structure */
	__le16	s_block_group_nr; 	/* block group # of this superblock */
	__le32	s_feature_compat; 	/* compatible feature set */
	__le32	s_feature_incompat; 	/* incompatible feature set */
	__le32	s_feature_ro_compat; 	/* readonly-compatible feature set */
	__u8	s_uuid[16];		/* 128-bit uuid for volume */
	char	s_volume_name[16]; 	/* volume name */
	char	s_last_mounted[64]; 	/* directory where last mounted */
	__le32	s_algorithm_usage_bitmap; /* For compression */
	/*
	 * Performance hints.  Directory preallocation should only
	 * happen if the EXT2_COMPAT_PREALLOC flag is on.
	 */
	__u8	s_prealloc_blocks;	/* Nr of blocks to try to preallocate*/
	__u8	s_prealloc_dir_blocks;	/* Nr to preallocate for dirs */
	__u16	s_padding1;
	/*
	 * Journaling support valid if EXT3_FEATURE_COMPAT_HAS_JOURNAL set.
	 */
	__u8	s_journal_uuid[16];	/* uuid of journal superblock */
	__u32	s_journal_inum;		/* inode number of journal file */
	__u32	s_journal_dev;		/* device number of journal file */
	__u32	s_last_orphan;		/* start of list of inodes to delete */
	__u32	s_hash_seed[4];		/* HTREE hash seed */
	__u8	s_def_hash_version;	/* Default hash version to use */
	__u8	s_reserved_char_pad;
	__u16	s_reserved_word_pad;
	__le32	s_default_mount_opts;
 	__le32	s_first_meta_bg; 	/* First metablock block group */
	__u32	s_reserved[190];	/* Padding to the end of the block */
};
<br><br>Inode是ext2文件系统中的一个重要概念。Inode本质上是文件控制块，任何在ext2文件系统上的对象都需要用inode来表示，如文件、目录、符号链接等。inode中包含着除了文件名以外的所有文件元数据，如文件的权限(permissions)、所有者、组、flags、大小、使用的块数、访问创建修改删除时间、链接数、ACLs等。<br>每个块组都有一个线性数组来存储inode节点数据，这个数组被称为 inode table。在ext2/3中，每个inode需要128KB的存储。每个inode都属于一个特定的块组，并存储在相应的 inode table 中。<br><br>ext2/3的inode数据结构占用128个字节，ext4扩展为256个字节。以下是ext2/3的inode数据结构：<br><br>在ext2/3的inode数据结构中，最高的16bits数据用来表示类型和权限(Type and Permission)。其中 higher 4 bits 用来表示文件的类型，如：<br><br>Lower 12 bits are used to present file permission as following below：<br><br>我们用ls -alh命令就会看到相关的i_mode信息，如：<br>du@du-virtual-machine:~/Desktop/OS$ ls -alh
total 136K
drwxrwxr-x 3 du du 4.0K  8月 30 02:05 .
drwxr-xr-x 7 du du 4.0K  8月  8 05:09 ..
drwxrwxr-x 2 du du 4.0K  7月  1 00:51 critical_section
-rwxrwxr-x 1 du du  17K  7月  2 13:50 mutex
-rw-rw-r-- 1 du du 1.2K  7月  2 13:50 Mutex_locks.c
......
<br><br>inode本身并不存储数据信息，其会通过存放指向数据块的索引表来引用文件的实际内容。通常情况下，一个inode对应一个文件或目录，但通过硬链接，可以有多个文件名指向同一个inode。<br>索引文件数据块是inode最重要的功能，inode也因此得名（index-node）。inode通过一张索引表来索引文件文件块，这个索引表有15个字段，每个字段4字节，共60字节。记录了指向一个数据块的指针。其中有直接块(direct block)、单级间接块(single indirect)、双级间接块(double indirect)、三级间接块(triple indirect)。<br><img alt="Pasted image 20240825181709.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20240825181709.png"><br>
<br>直接块指针：每个inode包含12个直接块指针，直接块指针直接指向一个数据块。
<br>单级间接块指针：一个单级间接块指针指向一个间接块表，间接块表中包含指向数据块的指针。若一个块4KB，每个指针大小为4字节，那么一个间接块表可以包含1024个直接块指针。
<br>双级间接块指针：一个双级间接块指针指向一个间接块表，间接块表中的每个指针又指向另一个间接块表。这样可以寻址更多的数据块。
<br>三级间接块指针：一个三级间接块指针指向一个间接块表，间接块表中的每个指针指向另一个间接块表，依次类推，直到最终指向数据块。
<br><br>通过多级间接寻址方式，ext2文件系统理论上能够在32位系统上支持高达4TB的文件大小。具体计算如下，假如一个块的大小为4KB：<br>
<br>直接块：12个块
<br>单级间接块：1024个块
<br>双级间接块：1024 * 1024个块
<br>三级间接块：1024  1024  1024个块
<br>总共可以寻址的块数为：总共可以寻址的文件大小为:<br>
<br>然而，实际上文件大小限制还要收到文件系统实现和内核的限制。根据<a data-tooltip-position="top" aria-label="https://docs.kernel.org/filesystems/ext2.html" rel="noopener nofollow" class="external-link" href="https://docs.kernel.org/filesystems/ext2.html" target="_blank">相关的Linux文档</a>，当块大小为1KB时，单个文件大小限制在16GB，2KB的块为256GB的文件大小限制，块大小更大时，单个文件最大大小为2TB。<br><br>在ext2文件系统中，目录也是以文件的形式存储在磁盘上的，目录文件的文件控制块（FCB）也是一个inode。在目录文件中包括许多目录项(Entry)，文件名和其inode编号就存放在目录项中，通过这一对文件信息可以检索到某一个文件。这些目录项使得操作系统可以通过文件名查找到对于文件的inode，进而访问文件的元数据和实际内容。目录项包含以下内容：<br><br>其中文件类型有以下几种：<br><br>ext2文件系统用单链表来存储目录中的文件名；在改进后的版本中，使用文件名的哈希值来进行查找，免去了对整个文件目录的扫描。<br><br>
<br>通过文件名获取inode编号：使用目录项（directory entry）查找文件名对应的inode编号。
<br>读取超级块：获取文件系统的基本信息，包括每个块的大小、每个块组中的块数量、每个块组中的inode数量以及块组描述符表的起始块位置。
<br>确定inode所属的块组：根据inode编号和每个块组中的inode数量，计算出inode所属的块组。
<br>读取块组描述符：读取对应块组的块组描述符，获取该块组的详细信息。
<br>提取inode表的位置：从块组描述符中提取该块组的inode表的起始位置。
<br>确定inode在inode表中的索引位置：根据inode编号和inode表的起始位置，计算出inode在inode表中的具体位置。
<br>读取指定的inode：通过索引inode表，读取出指定的inode，获取文件的元数据和指向数据块的指针。
<br><br><br><br>通过第六课的学习，我们已经对inode有了一定的认识，inode是类Unix系统中用于表示文件的数据结构。Inode意为索引节点，通过inode，我们可以对文件数据进行索引，这是inode的两个最主要的作用。如果有多个进程想要并发地使用同一个文件，我们该怎么做？我们先从文件锁开始介绍。<br><br>对一个文件的读写实际上就是我们在之前在同步问题中学到的读者写者问题。当文件打开时，程序会获得该文件的引用。如果你不想其他的应用程序使用文件，防止数据竞争和不一致问题，我们就可以将文件给锁起来。我们用fcntl系统调用来实现文件锁。<br><br>POSIX有个系统调用flock()，我们可以用它来对文件进行加锁。下面实现了一个对文件的互斥锁（一个写锁）。flock()有两个字段，第二个属性字段除了LOCK_EX之外，还有LOCK_SH用于设置一个共享的锁、LOCK_UN对锁进行解锁。<br>#include &lt;stdio.h&gt;
#include &lt;sys/file.h&gt;
FILE* f = fopen("example.txt", "r");
int fd = fileno(f);
int result = flock(fd, LOCK_EX);
<br>flock()的函数原型如下：<br>#include &lt;sys/file.h&gt;

int flock(int fd, int operation);
/* Parameters:

1. fd: File descriptor of the file to be locked.
2. operation: Operation to be performed (e.g., LOCK_SH for shared lock, LOCK_EX for exclusive lock, LOCK_UN for unlocking).

Return value: Returns 0 on success, otherwise -1.
*/
<br>这种写锁（LOCK_EX）只允许一个进程访问文件，对整个文件都进行了加锁。如此，其他进程不能读写该文件，如果我们想要对文件进行粒度更细的管理怎么办？我们可以使用POSIX提供的fctnl系统调用。 <br><br>fcntl() 提供了更复杂和灵活的文件锁定机制，支持对文件的部分区域进行锁定。通常情况下，部分锁定(partial locking)也会被称为记录锁定(record locking)。我们常常在data record中看到这种记录锁定。我们现在其实并不怎么使用这种部分锁定，因为有数据库在背后帮我们处理这些事情。<br>通过将文件划分成多个部分，例如6个部分，我们最多可以同时有6个程序对文件进行读写操作。fcntl()是一个强大的系统调用，命令字段实际上有很多参数信息。我们在这里只讨论关于文件锁的部分信息。以下是 fcntl() 的原型：<br>#include &lt;fcntl.h&gt;

int fcntl(int fd, int cmd, ... /* struct flock *lock */);
/* Parameters:

1. fd: File descriptor of the file to be locked.
2. cmd: Command to be performed. Common file locking commands include:
   - F_GETLK: Get record locking information.
   - F_SETLK: Set record locking information (non-blocking).
   - F_SETLKW: Set record locking information (blocking).
3. lock: Pointer to a struct flock that specifies the lock parameters (used with F_GETLK, F_SETLK, and F_SETLKW).

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>这个系统调用的第一个参数是文件描述符，第二个参数是一些关于文件锁的一些命令，第三个参数是一个指向flock结构体的指针，这个结构体如下，这些字段并不难理解：<br>struct flock {
	short l_type;    // F_RDLCK, F_WRLCK, F_UNLCK
	short l_whence;  // SEEK_SET, SEEK_CUR, SEEK_END
	off_t l_start;   // offsets in bytes, relative to l_whence
	off_t l_len;     // length in bytes; 0 means lock to EOF
	pid_t l_pid;     // returned lock owner's PID with F_GETLK, otherwise return 0
}
<br>通过设置l_len字段，我们事实上可以将整个文件区域进行锁定，我们需要设置l_len = 0;<br><br>下面，我们来看fcntl的第二个参数。和文件锁相关的有三个命令，第一个命令F_GETLK就是检查指定的区域是否被其他进程所锁定。如果被锁定，那将把l_type设置为不同的锁类型并在l_pid字段返回持有锁进程的ID，如果没有被锁定，那就会将l_type设置为F_UNLCK。但是这个命令作用并不大，因为F_GETLK的检查并不是原子化的（我们检查时可能发生状态变化）。<br>下面的两个命令F_SETLK和F_SETLKW（set lock wait），两个命令的作用实际上非常相似，只是阻塞和非阻塞的区别。F_SETLK会尝试设置指定的锁定区域。如果锁定区域被其他进程锁定，调用将失败并返回 -1，并不会等待区域变为可用（和try_lock很像）；而F_SETLKW会在检查到区域锁定后阻塞并等待，直到锁定区域变为可用或接收到一个信号中断。<br>在应用中，我们并不会先检查指定区域是否可用后将区域锁定，而是通过F_SETLK的方式设置锁，如果当前程序可以等，那就将其阻塞，使用F_SETLKW确保程序后续能够将区域锁定。<br><br>int write_lock_file(int fd) {

	struct flock fl;
	fl.l_type = F_WRLOCK;
	fl.l_start = 0;
	fl.l_whence = SEEK_SET;
	fl.l_len = 0;    // Lock the entire file due to l_len = 0

	return fcntl(fd, F_SETLK, &amp;fl);
}
int unlock_file(int fd) {

	struct flock fl;
	fl.l_type = F_UNLOCK;
	fl.l_start = 0;
	fl.l_whence = SEEK_SET;
	fl.l_len = 0;    // Unlock the entire file

	return fcntl(fd, F_SETLK, &amp;fl);

}
<br>Lock the part of the file:<br>int fd = open("example.txt", O_RDONLY);

struct flock fl;
fl.l_type = F_RDLOCK;
fl.l_start = 1024;
fl.l_whence = SEEK_SET;
fl.l_len = 256;

fcntl(fd, F_GETLK, &amp;fl);

if(fl.l_type == F_UNLCK){
	// Lock is unlocked, we may proceed.
}
else if(fl.l_type == F_WRLOCK){
	printf("File is locked by process ID %d.\n", fl.l_pid);
	return -1;
}
<br><br>使用fcntl有两个缺点，那就是你要用重用结构体的话那就需要将结构体reset，你还需要确保字段的正确设置；此外，代码的逻辑需要额外地注意，当F_SETLK返回-1时，后续的逻辑流就不能够再出现相关F_UNLOCK的操作。<br><br>如果你不想让文件锁定操作那样复杂，那你可以使用lockf系统调用。在一些系统上，lockf是由fcntl系统调用封装而来的，但有的系统可能使用其他的机制。所以当你lock a file时，确保使用同样的系统调用来lock and unlock，避免可能出现的一些未定义行为。<br>#include &lt;unistd.h&gt;

int lockf(int fd, int command, off_t length);
/*
Parameters:

1. fd: File descriptor of the file to be locked.
2. command: Command to be performed. Common commands include:
   - F_LOCK: Lock a section of the file.(blocking)
   - F_TLOCK: Try to lock a section of the file (non-blocking).
   - F_ULOCK: Unlock a section of the file.
   - F_TEST: Test a section of the file for locks held by other processes.
3. length: Length of the section to be locked, in bytes. A value of 0 means to lock from the current position to the end of the file.

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>mandatory locks do exist,but are hard to use and not recommended<br><br>除了上述我们学习过的这些文件锁，我们可以用文件来控制文件的并发访问。将一个文件的存在当作一把锁，如果文件存在，则说明区域已被锁定。Git会通过在特定的目录处放置一个index.lock的文件来指示进行中的操作。通过这个文件，git就能避免多个git客户端同时操作一个repo。<br>为了实现这种机制，我们可以使用以下的系统调用：<br>int open(const char *pathname, int flags);
int rename(const char* old_filename, const char* new_filename);
int remove(const char* filename);
<br>open系统调用有许多选项，在这种情形下，我们并不想使用”如果文件存在，则打开文件“的open系统调用。我们想要系统告诉我们：”如果文件存在，则创建失败并返回错误码“。所以我们要用O_CREAT和O_EXCL这两个flags。前者是创建文件，后者是exclusive的缩写，当两个一起使用时，就会在文件存时返回失败并设置errno为EEXIST。<br>用于open系统调用是原子化的，所以实际上避免了多个进程的并发问题。在操作完成后，我们用remove系统调用来删除文件，让下一个用户进程对文件或repo进行操作。<br>那rename系统调用是做什么的？rename系统调用和open一样，也是原子化的。这样，我们其实可以单单用rename来进行lock和unlock操作。改名就是锁定，解锁呢，就是将文件名改回来。<br><br>前面我们介绍了如何用文件作为锁来控制并发。在版本控制系统中，我们常常会看到两种不同的并发控制策略，分别是Lock-Modify-Unlock 和 Copy-Modify-Merge（也称为 Lock-Modify-Merge），它们在不同的场景下被使用。<br><br>Lock-Modify-Unlock常会用在集中式的版本控制系统中。在开发者修改文件之前，首先锁定文件，防止其他人同时修改（lock）；之后，开发者对文件进行修改（modify）；完成后解锁文件，运行其他人对文件进行修改（unlock）。以下是我们用文件作为锁对这种方式的简单实现：<br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;pthread.h&gt;

#define NUM_THREADS 10

int lock_fd;
int shared = 0;

void* run(void* arg){
    int* id = (int*) arg;
    while(rename("file.lock", "file.unlock") == -1){ // Lock the lock
        // thread is waiting.
    }
    // thread in critical section
    printf("Thread %d is in the critical section.\n", *id);
    printf("Shared incremented from %d", shared);
    shared++;
    printf(" to %d.\n", shared);
    rename("file.unlock", "file.lock"); // Unlock the lock.
    
    free(arg);
    pthread_exit(NULL);
}

void* writer(void* arg){
    /* Write data implementation not shown*/
    pthread_exit(NULL);
}

int main(int argc, char** argv){
    lock_fd = open("file.lock", O_CREAT | O_EXCL, 0644);
    if(lock_fd == -1){
        printf("File creation failed.\n");
        return -1;
    }
    pthread_t threads[NUM_THREADS];
    for(int i = 0; i &lt; NUM_THREADS; i++){
        int* id = malloc(sizeof(int));
        *id = i;
        pthread_create(&amp;threads[i], NULL, run, id);
    }
    for(int i = 0; i &lt; NUM_THREADS; i++){
        pthread_join(threads[i], NULL);
    }
    close(lock_fd);
    remove("file.lock");
    return 0;
}
<br>这种方法确保了在修改期间没有其他人可以修改同一个文件，从而避免了冲突。然而，它也可能导致开发效率降低，因为其他开发者在等待锁释放时无法进行修改。<br><img alt="Pasted image 20241207162851.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241207162851.png"><br><br>而在分布式版本控制系统，如git中，有了分支的概念。多个开发者可以把代码从远程仓库复制到本地，每个开发者可以在自己的分支上独立开发，并行工作（copy-modify）；修改完成后，开发者可以将自己的分支合并到主分支，git会自动处理大部分的合并（merge）。<br>对文件的这些修改会被记录到项目的提交记录中。如果两个分支修改了同一文件的同一部分，那么这些修改将不会被自动合并，我们称之为冲突（conflict）。如果出现冲突，合并操作将暂停，直到冲突被手动解决。<br>为了避免不连续性，合并操作要么成功，要么失败，这是由事务机制实现的。事务(transaction) 相当于要一气呵成完成的一组操作，一个事务有两个开始事务和结束事务两个状态。在merge的过程中，对合并文件的一些检查会用一个log记录下来来进行检查，对事务的检查是不可中断的，如果发生冲突那么合并就会失败并回退到之前的状态（roll-back）并等待开发者的手动解决。<br><br>实际上，inotify应当被单拎出来。inotify是内核为我们提供的监视文件系统是否有事件发生的工具，它可以监控文件或目录的各种事件，如创建、删除、修改等。它本身并不是一个并发控制的工具，但一些情况下，inotify可以在并发控制中起到一些辅助的作用。<br>利用inotify API，你可以让你的程序对一些事件做出反应，比方说当某个文件被打开，程序做出一些响应。要让inotify给你发消息，你需要遵循以下步骤：<br>
<br>
初始化并创建一个管理结构：使用inotify_init函数初始化inotify实例，并获取一个文件描述符，用于管理这些事件。

<br>
添加监控事件：使用inotify_add_watch函数告诉内核你想监控哪些事件，并将这些事件加入到inotify实例中。

<br>
读取事件：内核会通过文件描述符通知进程事件发生的信息。你可以使用read函数读取这些事件，并根据需要进行处理。

<br>
关闭文件描述符：完成后，使用close函数关闭文件描述符，释放资源。

<br>值得注意的是，这个过程不是递归的。<br><br>我们有以下系统调用，初始化inotify的系统调用很好理解，即初始化创建一个inotify实例，一旦实例被创建，内核就会设置必要的数据结构和资源来管理inotify子系统。<br>#include &lt;sys/inotify.h&gt;

int inotify_init(void);
/*
Parameters: None

Return value: Returns a file descriptor for the inotify instance on success, otherwise -1.
*/
<br><img alt="Pasted image 20241207225739.png" src="https://congzhi.wiki/congzhi's-os-series/pics/pasted-image-20241207225739.png"><br>实例创建完之后，我们就要用inotify_add_watch()来指定你想要以监视的文件或目录。系统调用inotify_add_watch()会返回一个监视描述符，用于唯一标识的监视项（文件）。另一个系统调用inotify_rm_watch()是将特定的wd从inotify instance中删去。<br>int inotify_add_watch(int fd, const char *pathname, uint32_t mask);
/*
Parameters:

1. fd: File descriptor returned by inotify_init.
2. pathname: Path to the file or directory to be monitored.
3. mask: Bitmask of events to be monitored. Common events include:
   - IN_ACCESS: File was accessed.(Read/Execute)
   - IN_MODIFY: File was modified.(Write for example)
   - IN_ATTRIB: Metadata changed.
   - IN_CLOSE_WRITE: File opened for writing was closed.
   - IN_CLOSE_NOWRITE: File not opened for writing was closed.
   - IN_OPEN: File was opened.
   - IN_MOVED_FROM: File was moved out of the watched directory.
   - IN_MOVED_TO: File was moved into the watched directory.
   - IN_CREATE: File or directory was created.
   - IN_DELETE: File or directory was deleted.
   - IN_DELETE_SELF: Watched file or directory was deleted.

Return value: Returns a watch descriptor on success, otherwise -1.
*/

int inotify_rm_watch(int fd, int wd);
/*
Parameters:
1. fd: File descriptor returned by inotify_init.
2. wd: Watch descriptor returned by inotify_add_watch.

Return value: Returns 0 on success, otherwise -1.
*/
<br><br>之后你可以read(fd, buf, size)，阻塞直到相应的事件发生。当你要监视的事件发生，inotify会将事件信息填充到 inotify_event 结构体中，并通过初始化得到的文件描述符返回给你。<br>struct inotify_event {
    int         wd;      // Watch descriptor
    uint32_t    mask;    // Bitmask of events that occurred
    uint32_t    cookie;  // Unique cookie associating related events
    uint32_t    len;     // Length of the name field
    char        name[];  // Optional null-terminated name of the file
};
<br>由于最有一个字段是可选的，因而inotify_event结构体大小为：<br>event_size = sizeof(struct inotify_event) + inotify_event.len;
<br>由于长度是不确定的，由此我们想要设置缓冲区时可能会设置的过大或过小。在设置缓冲区之前，我们可以用ioctl(inotify_fd, FIONREAD, &amp;numbytes)先获取文件当前可读取的长度。但一般情况下，我们会用空间换时间。<br><br>#include &lt;sys/inotify.h&gt;
#include &lt;unistd.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;stdbool.h&gt;

const char filename[] = "file.lock";

int main() {
    int lockFD;
    bool our_turn = false;

    while (!our_turn) {
        lockFD = open(filename, O_CREAT | O_EXCL | O_RDWR, 0666);
        if (lockFD == -1) {
            printf("The lock file is existing, and process %ld will wait...\n", (long)getpid());
            int notifyFD = inotify_init();
            uint32_t watch = inotify_add_watch(notifyFD, filename, IN_DELETE_SELF);
            int buf_size = sizeof(struct inotify_event) + strlen(filename) + 1;
            char* event_buffer = malloc(buf_size);
            printf("Setup complete, waiting for event.\n");
	        // Read the file descriptor while the event happens.(blocking)
            read(notifyFD, event_buffer, buf_size);
            struct inotify_event* event = (struct inotify_event*)event_buffer;
            printf("Event occurred!\n");

            free(event_buffer);
            inotify_rm_watch(notifyFD, watch);
            close(notifyFD);
        } 
        else {
            int namelen = sizeof(filename);
            write(lockFD, filename, namelen); 
            close(lockFD); 
            our_turn = true; 
        }
    }
    printf("Process %ld is in the critical area.\n", (long)getpid());
    // remove(filename);
    return 0;
}
<br><br><br><br>我们用文件系统管理磁盘来存储我们所需要的数据。然而，由于某些情况，文件系统中也可能会发生数据的丢失或数据的不连续（如系统掉电而文件还没有来得及写回磁盘）。为了发现文件系统中数据的不连续，我们可以周期性地检查系统中不连续的数据。<br>但由于一个volume可能很大，而且磁盘又很慢，所以检查一个volume上是否有不连续性的数据会很耗费时间。因此，系统在运行时并不会主动扫描磁盘。一般而言，系统启动的时候或者用户下达命令时才会扫描volume。UNIX中，用户可以使用 fsck 系统调用来扫描磁盘；Windows中，则可以使用 chkdsk/scandisk。<br><br>那么，什么是不连续状态？文件不连续后操作系统会如何反应？我们提到了系统掉电而文件没有来得及写回磁盘的情况。当系统重新上电后，我们会发现一部分文件块由于掉电时在内存中而遗失掉了，这就是文件的不连续状态。不连续的文件可能会导致一些严重的问题。<br>例如，我们本来要创建一个大小10个块的文件，在FCB中文件大小字段中就将其标识为10个磁盘块。然而，之后的文件链表中只包含有5个块。当使用这些系统调用工具检查出不连续状态时，我们希望系统能够把遗失的块找回来并链在一起，但文件系统可能并不能这么做，文件系统会修改相关的信息（如FCB的文件大小字段修改为5），从而保证文件的连续性。<br>系统视角下的recovery和我们想象中的recovery好像并不太一样，但不论怎样，现在文件是连续的了，系统内没有错误了。任务完成！<br><br>我们当然希望避免不连续问题的发生，以防因此出现严重的系统故障。避免因一些错误导致的数据不连续，你可能会想到原子操作。实际上，我们要使用避免数据不连续的方法事务其实我们可以看作原子操作的一种变种。事务使得操作要么完美地完成，要么就什么都不做。当今几乎所有的文件系统都会使用事务来避免data inconsistency的发生。<br>在版本控制系统和数据库中，为了使每个版本的数据都是连续的，这些软件会在修改文件相关结构之前，先列出一个待办事项清单。当这个待办事项清单中的所有待做项都完成之后，我们才会认为这个事务结束了，系统随之修改相关的数据结构。<br><br>ZFS使用single-atomic-update的事务机制来避免磁盘上的不连续性，这种机制类似于copy-modify-merge模型。数据先是从磁盘拷贝到内存，之后修改拷贝的磁盘块，最后将拷贝写回磁盘。这些修改后的新数据写回磁盘是并不会覆盖原旧数据块，而是将拷贝写到新磁盘块。这样其实为操作提供了冗余，如果写磁盘不连续，我们可以抛弃写回的数据块（即什么也不做）。如果操作一切顺利，那么我们就可以将旧磁盘块的引用用新的磁盘块所替代。<br>如果磁盘空间满了怎么办呢？那就买一块更大的后备硬盘吧！<br><br>APFS引入了文件系统快照(snapshots)，记录了某一时刻的文件系统状态。快照可以用于备份和恢复，确保在发生数据损坏或丢失时能够快速恢复到之前的状态。<br><br><br><br>在NTFS中，所有对文件元数据的修改都会顺序地放到一个日志文件(log文件)中，一旦修改写入日志文件后，系统才会实际修改文件的元数据。这种机制被称为日志记录(journaling)。当系统修改完文件的元数据后，系统会将日志文件中标记为“已完成”的日志记录（事务）进行相应的清理。<br>之后，当系统崩溃，日志文件中就可能包括0个或多个事务待处理。0个当然最好，你不需要担心任何事；如果届时日志文件中有多个事务还没有处理，就意味着仍有事务没有完成。我们将有两种解决方案：前进和回退。<br><br>当系统上电后，如果这个清单可以接着之前的做，那当然最好，我们实际上并没有损失什么。比如下载软件到一半之后，系统掉电，但是上电后我们可以接着之前的继续下载。<br><br>如果事务不能接着之前的做，系统就会回退到之前的版本，也就是在版本控制系统中常见的方式。<br><br>在NTFS中，日志记录的实现如下：<br>
<br>
当需要对文件元数据进行修改时，首先将这些修改记录在缓存中的日志文件里。这一步确保了所有修改操作都有一个记录，以防在实际写入磁盘之前发生故障。

<br>
在日志文件中记录修改后，系统会在缓存中进行实际的卷修改。这意味着这些修改还没有真正写入物理磁盘，但已经在内存中准备好了。

<br>
缓存管理器（cache manager）负责将缓存中的日志文件写入到物理磁盘上。这一步非常关键，因为它确保了日志记录的持久性，即使系统在此后崩溃，日志文件中的修改记录也不会丢失。

<br>
日志文件写入磁盘后，缓存管理器会开始将缓存中的卷修改写入到物理磁盘上。这一步确保了文件系统的一致性，因为即使在卷的实际修改过程中发生了系统故障，日志文件中的记录仍然可以用于恢复未完成的修改。

<br><br><br><br>]]></description><link>https://congzhi.wiki/congzhi's-os-series/15.-file-systems.html</link><guid isPermaLink="false">Congzhi's OS Series/15. File Systems.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 26 May 2025 22:25:45 GMT</pubDate><enclosure url="https://congzhi.wiki/congzhi's-os-series/pics/ssd.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/congzhi's-os-series/pics/ssd.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[16. Asynchronous IO]]></title><description><![CDATA[ 
 <br>第一遍重写中<br>需要注意！
本章内容有待明晰同步IO和异步IO的区别。内容暂未修正。
<br><br><br><br>异步——Asynchronous这个单词由否定前缀 "A-" ，表示一起、同的前缀 "SYN-" 和表示时间的希腊词根 "CHRON" 构成。字面意思就是不同时/不同步的意思，也就是我们所说的异步。当然，你也可以理解为随机。<br>由于I/O完成的时间是随机的，我们总会担心I/O操作完成后的中断被错过，我们可能就会想到阻塞/自旋等待I/O操作的完成，这种等待I/O操作完成的操作就被称为同步I/O。但这段阻塞/自旋的时间就会被白白浪费，这不是我们想要的。<br>而通过异步I/O，CPU就可以在I/O操作完成前继续执行其他任务，不需要阻塞等待整个I/O操作完成。其实就相当于等公交，你并不会在等公交时傻傻地盯着远处一直看，而是在等待的过程中做自己的事情，当公交车到了时，司机会提醒你上车，这时才会停下手上的活去挤公交。<br><br>学过线程，我们可以用多个线程来处理多个I/O操作。如果线程被I/O阻塞了我们就重新创建一个线程。这确实是可行的，但是额外的线程就意为着多一份资源消耗和复杂性，滥用多线程可能并不是一个好的选择。况且有的语言并不支持多线程，如JavaScript，只有一个线程，我们别无选择。<br><br>在大多数情况下，我们都希望使用一个线程来处理数个 I/O 请求。在学习真正的异步 I/O 之前，我们来学习一下同步 I/O 复用。还是公交车的例子，同步 I/O 复用就相当于是等公交车的时候每隔一段时间抬头看一看车到没到。但抬头的这段时间就会影响到玩手机的时间。<br><br>select 是一个有些过时的同步 I/O 复用 API，在大多数系统上都广泛支持。<br><br>不是所有的I/O操作都能够异步完成，在文件系统的章节中，我们会学到read()系统调用。read()就是一个阻塞的 I/O 操作，意味着当程序调用 read() 时，它会等待数据被读取到内存buffer后才继续执行后续的代码，当遇到阻塞的I/O操作，你只能同步等待I/O操作完成。<br>虽然我们还学到了许多 flags ，比如 O_NONBLOCK 的文件flag选项，也就是说用非阻塞的方式打开文件。但当设置文件的操作是O_NONBLOCK后，read()可能仍然会阻塞当前的线程。为什么？<br>#include &lt;fcntl.h&gt; // For open(), O_RDONLY, O_WRONLY, O_RDWR, and file access modes
#include &lt;unistd.h&gt;// For close(), read(), write(), fsync(), lseek(), and others.

// Non-blocking file read. It still blocks, not what we expected.

int fd = open("example.txt", O_RDONLY | O_NONBLOCK);
int bytes_read = read(fd, buffer, numbytes);
// Do something here.
close(fd);
<br>我们所想的是使用read() 时，无论数据是否被放到缓冲区里，read()系统调用都应当立即返回一个返回值。但其实只有当使用 O_NONBLOCK 标志打开文件且没有数据可读，read() 调用才会立即返回。然而，对于磁盘文件，数据通常是立即可用的，因为内核会通过页缓存把数据从磁盘缓存到内存中，这个过程仍然会阻塞。<br><br>不同于读文件的确定性，对于 socket 或者 pipe 这类特殊文件，系统也不能确定 socket 什么时候能有数据，数据的大小是多少，这时 O_NONBLOCK 通常是可行的。<br>既然对文件的读取不会再阻塞了，我们是否可以每隔一段时间就轮询一遍，这样就可以实现在一个线程里面同步复用多个 I/O 了。这确实是可行的。实际上，select 使用的就是类似的机制。<br>int sockfd = socket(AF_INET, SOCK_STREAM, 0);
fcntl(sockfd, F_SETFL, O_NONBLOCK);
<br>当设置非阻塞套接字后，accept(), recv() 和 recvfrom()都将是非阻塞的。这些系统调用将返回 -1 并设置 errno 为 EAGAIN 或 EWOULDBLOCK。<br><br>那设置 non-blocking 的 sockets 有什么用处？我们的服务器总是要服务许多的客户端，假设每个客户端对应一个 socket 连接，那么服务器就需要响应监听这么多的 socket 连接。而这些网络相关的系统调用往往是阻塞的，我们并不想使用多线程来监听这些连接。<br>通过设置非阻塞 socket，我们就不用被 socket 相关的系统调用所阻塞。进而，我们可以在主线程中轮询监听这些 sockets，也就是紧轮询。只在检测到客户端的连接时创建线程来响应服务客户端，免去浪费线程资源来处理无意义的 IO 阻塞。<br><br>Tight polling(轮询)我们在<a data-tooltip-position="top" aria-label="13. IO Subsystem > 1.3.1 Tight Polling" data-href="13. IO Subsystem#1.3.1 Tight Polling" href="https://congzhi.wiki/congzhi's-os-series/13.-io-subsystem.html#1.3.1_Tight_Polling" class="internal-link" target="_self" rel="noopener nofollow">IO 阶段</a>中了解过。我们了解了 tight polling 会不停的循环问询浪费CPU资源，尽管简单，但是无法避免地 CPU 做无用功。所以我们并不想这么做。<br>内核为我们提供了一种更好的办法：select。select 使得我们可以设置一个监视器来监视一组的套接字，并告诉我们组中每个套接字的状态。socket状态可以是待读、待写和发生异常情况三种状态。select()会根据socket的状态将sockets放到可读、可写和一些异常的三种集合中进行管理。<br>下面是select()系统调用的函数原型：<br>#include &lt;sys/select.h&gt; // For select() and related macros
#include &lt;sys/time.h&gt;   // For struct timeval
#include &lt;sys/types.h&gt;  // For data types used in some system calls
#include &lt;unistd.h&gt;     // For close(), read(), and other unix system calls

int select( int nfds, 
		    fd_set *_Nullable restrict readfds,
            fd_set *_Nullable restrict writefds,
            fd_set *_Nullable restrict exceptfds,
            struct timeval *_Nullable restrict timeout);
/* Parameters:

1. nfds: The highest-numbered file descriptor in any of the three sets, plus 1.
2. readfds: Pointer to an fd_set that will be checked for readability.
3. writefds: Pointer to an fd_set that will be checked for writability.
4. exceptfds: Pointer to an fd_set that will be checked for exceptional conditions.
5. timeout: Pointer to a struct timeval that specifies the maximum interval to wait for any file descriptor to become ready. If NULL, select() will block indefinitely.

Return value: 
- On success, returns the number of file descriptors contained in the three returned descriptor sets (that is, the total number of bits that are set in readfds, writefds, and exceptfds).
- On error, returns -1 and sets errno to indicate the error.
*/
<br><br>我们来看第一个参数，nfds 是要监视文件描述符集合中最大文件描述符的值加 1。它用于告诉 select() 要检查的文件描述符的范围。具体来说，select() 会监控文件描述符从 0 到 nfds - 1 的文件描述符，等待一个或多个文件描述符变为"ready"状态。<br>在许多老系统中，将进程打开文件表表项的最大数量设置为 1024 ，也就是每个进程最多可以打开的文件数。因此，select() 的监视集合 fd_set 能够最多监视 1024 个文件描述符。fd_set 是用固定大小的位图 bit field (bit array) 来实现的，每个文件描述符对应一个 bit。<br>通过设置 nfds，在 select() 遍历时，内核遍历到最大的监视项后就会立即停止遍历。现在很多系统中，一个进程可能需要打开的文件数量可能要比 1024 个大得多。而 select() 最多也只能监视 1024 个文件描述符，这也是 select() 的缺点。在<a data-tooltip-position="top" aria-label="https://www.man7.org/linux/man-pages/man2/select.2.html" rel="noopener nofollow" class="external-link" href="https://www.man7.org/linux/man-pages/man2/select.2.html" target="_blank">select Linux manual</a>中，也推荐使用<a data-tooltip-position="top" aria-label="16. Asynchronous IO > 1.4 Alternative Poll" data-href="16. Asynchronous IO#1.4 Alternative Poll" href="https://congzhi.wiki/congzhi's-os-series/16.-asynchronous-io.html#1.4_Alternative_Poll" class="internal-link" target="_self" rel="noopener nofollow">poll</a>或<a data-tooltip-position="top" aria-label="16. Asynchronous IO > 1.5 epoll I/O Event Notification Facility" data-href="16. Asynchronous IO#1.5 epoll I/O Event Notification Facility" href="https://congzhi.wiki/congzhi's-os-series/16.-asynchronous-io.html#1.5_epoll_I/O_Event_Notification_Facility" class="internal-link" target="_self" rel="noopener nofollow">epoll</a>来获取更大的文件句柄监视范围。<br><br>我们用fd_set结构来定义监视类型的结构：<br>#include &lt;sys/select.h&gt;

fd_set readfds;
fd_set writefds;
fd_set exceptfds;
<br>fd_set 用于表示一个文件描述符的集合。在 POSIX 下，fd_set最多能够容纳的文件描述符的数量在 FD_SETSIZE 宏中定义，我们前面提到了，这个数字在很多系统中都是 1024，可能不会改变。所以通常而言，fd_set 数据结构都是一个大小为 1024 字节的位图。<br>select()提供了一些设置fd_set的函数，如下：<br>#include &lt;sys/select.h&gt; // For select() and related macros

void FD_ZERO(fd_set *set);
/* Parameters:

1. set: Pointer to an fd_set structure that will be initialized to have zero bits set, meaning no file descriptors are part of the set.
*/
<br>FD_ZERO(fd_set *set)用于初始化或清除一个fd_set结构，将其所有位清零，表示集合中没有任何文件描述符。由于 select 要循环地检查这些文件描述符，当要复用文件描述符时，你要重新进行初始化这些集合。<br>void FD_SET(int fd, fd_set *set);
/* Parameters:

1. fd: The file descriptor to be added to the set.
2. set: Pointer to an fd_set structure where the file descriptor will be added.
*/
<br>FD_SET(int fd, fd_set *set)用于将一个文件描述符添加到 fd_set 结构中。它会设置相应的位，表示该文件描述符现在是集合的一部分。<br>void FD_CLR(int fd, fd_set *set);
/* Parameters:

1. fd: The file descriptor to be removed from the set.
2. set: Pointer to an fd_set structure from which the file descriptor will be removed.
*/
<br>FD_CLR(int fd, fd_set *set)用于从 fd_set 结构中移除一个文件描述符。它会清除相应的 bit 位，表示该文件描述符不再是集合的一部分。<br>int FD_ISSET(int fd, fd_set *set);
/* Parameters:

1. fd: The file descriptor to be checked.
2. set: Pointer to an fd_set structure that will be checked to see if the file descriptor is part of the set.

Return value:
- Returns a non-zero value if the file descriptor is part of the set, otherwise returns 0.
*/
<br>FD_ISSET(int fd, fd_set *set)用于检查一个文件描述符是否在 fd_set 结构中。如果该文件描述符在集合中，则返回非零值，否则返回 0。<br><br>select会将文件描述符归为三类，分别是可读(readfds)、可写(writefds)和异常(excaptfds)。这三类参数是可选的，当你不需要某一类时，你可以将那个参数设置为NULL。这三个参数也是select 最重要的参数。<br><br>最后一个参数是timeval，这是一个结构体，用于指定 select() 等待文件描述符变为就绪的最大时间间隔。在这个等待时间内，select 会阻塞直到：<br>
<br>一个文件描述符准备好了；
<br>被信号所中断；
<br>时间到了。
<br>timeval结构体的定义如下：<br>#include &lt;sys/time.h&gt;

struct timeval {
    long tv_sec;    /* seconds */
    long tv_usec;   /* microseconds */
};
<br>当你将两个字段都设置为0，select()就会立刻返回。要是sockets中有数据可读，select()就会告诉你，不然就会告诉你监视的fds都没有准备好呢。如果timeval为 NULL ，select() 将无限期地阻塞，直到至少有一个文件描述符变为就绪。<br>再用等公交车的例子说明一下，设置 timeval 的作用就是控制你抬头看公交车到没到的时间。在抬头的这段时间里，你实际上是被硬控阻塞的。<br><br>无论因为何种原因导致select()的返回，除了 nfds 之外的一些参数可能会被更新。<br><br>当我们调用 select() 时，传入的文件描述符集合（readfds、writefds 和 exceptfds）会被内核修改，以反映哪些文件描述符在调用期间变为就绪状态。这是因为 select() 需要告诉你哪些文件描述符可以进行I/O操作，而不会阻塞。<br>例如，你传入一个包含多个文件描述符的 readfds 集合，select() 会在返回时修改这个集合，只保留那些在调用期间变为可读的文件描述符。这样，你可以通过检查返回的集合来确定哪些文件描述符可以进行读取操作。<br><br>如果在调用 select() 时传入了 timeval 结构体，内核可能（不）会修改这个结构体，以反映在调用期间实际经过的时间。如果你设置了一个 5 秒的超时时间，但 select() 在 2 秒后返回，有的操作系统内核实现可能会将 timeval 结构体中的剩余时间更新为 3 秒；而有些系统则会保留原先 5 秒的超时。因此，重新使用 timeval 结构体是不安全的，应该在每次调用 select() 前重新设置。<br><br>当select()返回时，它会将哪些可以直接进行的I/O操作保留在fd_sets里。要知道有哪些 I/O 操作，我们就需要迭代遍历检查我们之前添加过的所有文件描述符。这时，FD_ISSET 宏的作用就显现出来了。<br>因为 select() 调用会修改传入的参数，以反映哪些文件描述符已经准备好进行 I/O 操作，以及实际经过的时间。所以在处理完成一轮的 select() 调用之后，我们需要重新设置 fd_sets、timeval 甚至 nfds。<br><br>void listen_for_connections(int client_sock1, int client_sock2, int client_sock3);
{
	int nfds = 1 +  (client_sock1 &gt; client_sock2 
	                ?
	                (client_sock1 &gt; client_sock3 ? client_sock1 : client_sock3)
	                : 
	                (client_sock2 &gt; client_sock3 ? client_sock2 : client_sock3));

	fd_set s;
	struct timeval tv;
	ptintf("Startup complete!\n");
	while(!quit){
		FD_ZERO(&amp;s);
		FD_SET(service1_sock, &amp;s);
		FD_SET(service2_sock, &amp;s);
		FD_SET(service3_sock, &amp;s);
		tv.tv_sec = 30;
		tv.tv_usec = 0;

		int res = select(nfds, &amp;s, NULL, NULL, &amp;tv);
		// An Error occurred.
		if(res == -1){
			printf("An error occurred in selest(): %s.\n", strerror(errno));
			quit = 1;
		}
		else if(res == 0){
			printf("Still waiting events...\n");
		}
		else{
			if(FD_ISSET(service1_sock, &amp;s)){
				service1_activate();
			}
			if(FD_ISSET(service2_sock, &amp;s)){
				service2_activate();
			}
			if(FD_ISSET(service3_sock, &amp;s)){
				service3_activate();
			}
		}
	}
}
<br>从上面的代码中，你实际上可以感受到 select 有多么低效。比如我们的进程打开了 560 个文件，但是只需要监视 550、540、539 号文件，这时，你传入的 nfds 是 551 （0号文件到550号文件共551个文件） 。猜猜 select 会帮你做什么？每次调用 select() 时，它都会遍历进程打开文件表中从 0 到 nfds-1 的文件描述符。即便其他 548 个描述符完全无关。<br>而且，如果文件打开超过 1024 个文件，那就完蛋了。<br><br>除了select()，我们还有一个pselect()函数。pselect()的最后两个参数与select()有所不同。用于定义时间间隔的结构体timespec是一个const类型的参数，pselect()保证不对这个结构体的任何改变。另一个大的改变就是sigmask参数，用于定义哪些信号在等待期间被屏蔽。<br>pselect()的函数原型如下：<br>#include &lt;sys/select.h&gt; // For select() and related macros
#include &lt;sys/time.h&gt;   // For struct timeval
#include &lt;sys/types.h&gt;  // For data types used in some system calls
#include &lt;unistd.h&gt;     // For close(), read(), write(), and other system calls
#include &lt;signal.h&gt;     // For sigset_t and related functions

int pselect(int nfds, 
			fd_set *_Nullable restrict readfds,
            fd_set *_Nullable restrict writefds,
            fd_set *_Nullable restrict exceptfds,
            const struct timespec *_Nullable restrict timeout,
            const sigset_t *_Nullable restrict sigmask);
/* Parameters:

1. nfds: The highest-numbered file descriptor in any of the three sets, plus 1.
2. readfds: Pointer to an fd_set that will be checked for readability.
3. writefds: Pointer to an fd_set that will be checked for writability.
4. exceptfds: Pointer to an fd_set that will be checked for exceptional conditions.
5. timeout: Pointer to a struct timespec that specifies the maximum interval to wait for any file descriptor to become ready. If NULL, pselect() will block indefinitely.
6. sigmask: Pointer to a sigset_t that specifies the signal mask to be used during the wait. If NULL, the signal mask is not changed.

Return value: 
- On success, returns the number of file descriptors contained in the three returned descriptor sets (that is, the total number of bits that are set in readfds, writefds, and exceptfds).
- On error, returns -1 and sets errno to indicate the error.
*/
<br><br>pselect()中的timespec结构体容许我们设置更精细的时间间隔，它的结构体原型如下：<br>#include &lt;sys/time.h&gt;

struct timespec{
	long tv_sec; /* seconds */
	long tv_nsec; /* nanoseconds */
};
<br><br>允许你在调用pselect()的同时原子化地修改signal mask。在这个并发的世界中，我们对原子化的爱是毋庸置疑的。下面的一条pselect()语句：<br>ready = pselect(nfds, &amp;readfds, &amp;writefds, &amp;exceptfds, timeout, &amp;sigmask);
<br>和原子化的<br>sigset_t origmask;
pthread_sigmask(SIG_SETMASK, &amp;sigmask, &amp;origmask); // Mask the signals in sigmask.
ready = select(nfds, &amp;readfds, &amp;writefds, &amp;exceptfds, timeout);
pthread_sigmask(SIG_SETMASK, &amp;origmask, NULL); // Back to origmask stage.
<br>是等价的。<br><br>另一个与 select 相近的 API 叫poll()，它也是一种同步 IO 复用的 API 。相比于select()，poll()要求的参数更少。现在，你不再需要计算最大的nfds是多少了，也不用再提供三种集合了。但由于它们实现上的相近，作为select()的表亲，并没有带来性能上的加成，它们都很慢。<br>poll()的函数原型如下：<br>#include &lt;poll.h&gt; // glibc library for poll() and related macros

int poll(struct pollfd *fds, nfds_t nfds, int timeout);
/* Parameters:

1. fds: Pointer to an array of struct pollfd, which specifies the file descriptors to be monitored.
2. nfds: The number of items in the fds array.
3. timeout: The maximum number of milliseconds that poll() will block. A negative value means an infinite timeout, while zero means poll() will return immediately.

Return value: 
- On success, returns the number of file descriptors with events, which may be zero if the timeout expired.
- On error, returns -1 and sets errno to indicate the error.
*/
<br><br>poll()将所有的监视项都放在了一个pollfd结构体类型的数组中。在使用时，你还需要提供数组的项数（你想让它监视多少项），这比select()方便了不少。你想指定的事件需要在pollfd结构体中说明，这个结构体的原型如下：<br>#include &lt;poll.h&gt;

struct pollfd{
    int fd;         /* file descriptor */
    short events;   /* requested events */
    short revents;  /* returned events */
};
/*
events/revents:
- POLLIN: Look for if there is data to read.
- POLLOUT: Writing is now possible without blocking.
- POLLRDHUP: Stream socket peer closed connection, or shut down writing half of connection.
- POLLPRI: There is urgent data to read.

revents specific(be ignored in events):
- POLLERR: Error condition.
- POLLHUP: Hang up, otherside closed socket.
- POLLNVAL: Invalid request: fd not open.

compiling with _XOPEN_SOURCE(additional bits):
- POLLRDNORM: equivalent to POLLIN
- POLLRDBAND
- POLLWRNORM
- POLLWRBAND
*/
<br>fd表示要监视的文件描述符。events是一个bit mask用于指定要监视何种事件，这是一个输入变量。revents是内核所设置的return events，用于标识当poll() 返回时，实际发生的事件。如果fd是一个负值，那么字段events就会被忽略，而且revents会返回0值。<br><br><br>void listen_for_connections(int client_sock1, int client_sock2, int client_sock3)
{
	struct pollfd pollfds[3];
	pollfds[0].fd = client_sock1;
	pollfds[0].events = POLLIN;
	pollfds[1].fd = client_sock2;
	pollfds[1].events = POLLIN;
	pollfds[2].fd = client_sock3;
	pollfds[2].events = POLLIN;
	int timeout = 30 * 1000;
	printf( "Going to start listening for socket events.\n" );

	while(!quit){
		int res = poll(&amp;pollfds, 3, timeout);
		// Error checking.
		if(res == -1){
			printf
			quit = 1;
		}
		// 0 sockets had events occur
		else if(res == 0){
			printf("Still waiting for events...\n");
		}
		// things happened
		else{
			if(pollfds[0].revents &amp; POLLIN){
				service0_activate();
			}
			if(pollfds[1].revents &amp; POLLIN){
				service1_activate();
			}
			if(pollfds[2].revents &amp; POLLIN){
				service2_activate();
			}						
		}
	}
}
<br><br>poll和ppoll的关系就如同select和pselect一样。同pselect一样，ppoll允许应用safely wait直到其中一个文件描述符准备好了或者有信号被捕获。<br>#define _GNU_SOURCE         /* See feature_test_macros(7) */
#include &lt;poll.h&gt;

int ppoll(struct pollfd *fds, nfds_t nfds,
		  const struct timespec *_Nullable tmo_p,
          const sigset_t *_Nullable sigmask);
<br><br>epoll()是一个 Linux-specific API，功能和poll()差不多，都通过监视一组文件描述符来看 I/O 是否可用。但 epoll API 和 poll 却有本质上的区别。epoll API 提供了边沿触发和电平触发两种使用接口。在高并发情境下，epoll API 的性能要好得多。<br><br>epoll instance 是 epoll API 中最重要的概念。epoll instance 是一个内核数据结构。在用户程序眼中，他就是一个包含两个列表的容器。这两个列表是兴趣列表 (interest list) 和就绪列表 (ready list)。从名字中你就能大致知道这两个列表的作用。<br>
<br>
Interest list 也叫 epoll set，是一个包含一些文件描述符的集合。在 interest list 中的文件描述符是用户进程所感兴趣的一些监视项。通过将这些感兴趣的文件描述符添加到 interest list 中，我们就能用 epoll API 让操作系统帮我们监视这些文件描述符的状态。（红黑树）

<br>
Ready list 是第二个列表。这个列表包含了那些准备好了进行 I/O 操作的文件描述符，ready list 是 interest list 的一个子集。一旦 interest list 中的文件描述符有 I/O 活动，内核就会动态地把相应的文件描述符添加到 ready list 中。（双向链表）

<br><br>为什么 epoll 这么快呢？我们得从 epoll instance 开始谈起。 epoll instance 中维护两个 "list" ，其中，interest list 是我们想让 epoll 帮我们监视的文件描述符集合，这是一个红黑树。而红黑树使得 epoll 在插入、删除、查找文件描述符的时间复杂度均为  ，适合管理大量的文件描述符。<br>此外，我们前面学习的 select 和 poll 模型需要不断地遍历所有监听的文件描述符（时间复杂度  ），而 epoll 会通过回调机制仅仅跟踪活跃的文件描述符。调用 epoll_wait 时直接返回就绪的文件描述符（时间复杂度 ），性能并不会随监听的文件描述符数量增长而下降。<br>epoll 使用回调机制就是当某个文件描述符（如 socket）发生 I/O 事件（如可读、可写）时，内核会主动通知 epoll，而不是由 epoll 主动轮询检查所有 fd。和 select/poll 那样时不时地看一眼公交车有没有到相比，让司机通知你肯定更好一些。<br><br>现在，我们先了解了解用来创建和管理 epoll instances 的系统调用。<br><br>要创建一个 epoll instance，我们有两种方式：epoll_create 和 epoll_create1。epoll_create 会创建一个 epoll instance。在 Linux 2.6.8 之后，内核会动态地检查 epoll instance，所以这个系统调用参数 size 会被忽略，但为了兼容新版本，size 必须比 0 大。<br>它的系统调用原型如下：<br>#include &lt;sys/epoll.h&gt;

int epoll_create(int size);
/* Parameters:

1. size: initial size (ignored in modern implementations, but must be &gt; 0)

Returns a file descriptor for the new epoll instance, or -1 on error.
*/
<br>在 Linux 2.6.27 后，内核提供了另一种创建 epoll instance 的系统调用。我们不再需要担心 size 是否大于 0 的问题了，而且还引入了 flags 参数。flags 可以是 0 或 EPOLL_CLOEXEC，用于在执行 exec 系列函数时自动关闭 epoll 文件描述符。<br>#include &lt;sys/epoll.h&gt;

int epoll_create1(int flags); // create a new epoll instance with flags
/* Parameters:

1. flags: epoll instance flags (0 or EPOLL_CLOEXEC)

Returns a file descriptor for the new epoll instance, or -1 on error.
*/
<br>当不再需要这些文件描述符后，记得用 close() 系统调用关闭掉相关的文件描述符。epoll 使用引用计数来管理 instance 的生命周期，当 epoll instance 中的所有文件描述符都关闭后，内核就会销毁 instance 并释放相关联的资源。<br><br>在 epoll instance 创建好之后，我们就可以通过创建返回的 epfd 往 interest list 中添加、修改或删除 list entries（也就是文件描述符和 event 字段中特化的事件）。<br>以下是 epoll_ctl 系统调用的原型：<br>#include &lt;sys/epoll.h&gt;

int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 
/* Parameters:

1. epfd: file descriptor returned by epoll_create or epoll_create1
2. op: operation to be performed 
	- EPOLL_CTL_ADD: Add an entry to the interest list of the epfd.
	- EPOLL_CTL_MOD: Change the settings associated with fd in the interest list
		             to the new settings specified in event.
	- EPOLL_CTL_DEL: Remove the target file descriptor fd from the interest list.
3. fd: file descriptor to be added, modified, or removed
4. event: pointer to epoll_event structure (can be NULL for EPOLL_CTL_DEL)

Returns 0 on success, or -1 on error.
*/
<br>除了我们刚才提到的 epfd 参数外，我们还有三个参数，分别是 op, fd, 和一个指针指向结构体 epoll_event。最开始的时候，epoll instance  为空，我们就需要在 op 上填入 EPOLL_CTL_ADD 来向 interest list 中添加相关的文件描述符，同时，在 epoll_event 结构体中选择让内核按何种方式来监视这些文件描述符。<br>op 字段提供将某个文件描述符加入到 interest list 中进行监视、fd 字段指示监视项、event 主要是一些监视选项，它是 epoll 机制中用于事件监视和传递用户数据的核心结构体。 struct epoll_event 的原型如下：<br>#include &lt;sys/epoll.h&gt;

struct epoll_event {
    uint32_t      events;  /* Epoll events */
    epoll_data_t  data;    /* User data variable */
};
/*
events/revents:
- EPOLLIN: Look for if there is data to read.
- EPOLLOUT: Writing is now possible without blocking.
- EPOLLRDHUP: Stream socket peer closed connection, or shut down writing half of connection.
- EPOLLPRI: There is urgent data to read.

revent specific:
- EPOLLERR: Error condition happened.
- EPOLLHUP: Hang up, otherside closed socket.
- EPOLLNVAL: Invalid request: fd not open.

additional epoll-specific flags:
- EPOLLET: Requests edge-triggered notification.
- EPOLLONESHOT: Requests one-shot notification.
- EPOLLWAKEUP: Prevent system suspend while event is pending.
- EPOLLEXCLUSIVE: Sets exclusive wakeup mode.
*/
union epoll_data {
    void     *ptr;
    int       fd;
    uint32_t  u32;
    uint64_t  u64;
};
<br>这里的联合体 epoll_data 是用户的自定义数据，当事件触发时，内核就会将此数据返回给应用。<br><br>在上面，我们了解到用户感兴趣的 instances 会被添加到兴趣列表中提供给 epoll 来监视，如果有事件发生，epoll 会将可用的文件描述符放到 ready list 中。epoll API 给我们提供了下面的函数来监视 I/O 事件发生：epoll_wait, epoll_pwait 和 epoll_pwait2。下面是这三个函数的原型：<br>#include &lt;sys/epoll.h&gt;

/* Waits for events on an epoll file descriptor. */
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
/*
Parameters:
1. epfd: The epoll file descriptor obtained from epoll_create or epoll_create1.
2. events: Pointer to an array of epoll_event structures where the ready events will be stored.
3. maxevents: The maximum number of events that can be returned (must be &gt; 0).
4. timeout: Timeout in milliseconds:
   - -1: Block indefinitely until an event occurs.
   - 0: Return immediately.
   - &gt; 0: Maximum wait time in milliseconds.

Return value:
- Returns the number of ready file descriptors.
- On failure, returns -1 and sets errno.
*/

/* Waits for events with the ability to temporarily block signals. */
int epoll_pwait(int epfd, struct epoll_event *events, int maxevents, int timeout,  
				const sigset_t *_Nullable sigmask);
/*
Parameters:
1. epfd, events, maxevents, timeout: Same as epoll_wait.
2. sigmask: Signal mask to be temporarily applied during the wait. Can be NULL to behave like epoll_wait.

Return value:
- Same as epoll_wait.
*/

/* Waits for events with nanosecond-resolution timeout and signal mask support. */
int epoll_pwait2(int epfd, struct epoll_event *events, int maxevents, 
				 const struct timespec *_Nullable timeout, 
				 const sigset_t *_Nullable sigmask);
/*
Parameters:
1. epfd, events, maxevents: Same as epoll_wait.
2. timeout: High-resolution timeout using timespec structure (supports nanosecond precision). NULL indicates indefinite blocking.
3. sigmask: Signal mask to be temporarily applied during the wait. Can be NULL to behave like epoll_wait.

Return value:
- Same as epoll_wait.
*/

<br>我们以 epoll_wait 为例，解释一下每个字段的含义：<br>
<br>epfd：这是需要被监视的 epoll instance 。
<br>events：这是用户分配的 epoll_event 数组指针，用于接收就绪事件。
<br>maxevents：events 数组的最大容量（必须大于0）
<br>timeout：超时时间。<br>
如果是 -1 ，则阻塞线程，直到有事件发生；<br>
如果是 0，则立即返回（非阻塞模式）。<br>
如果 &gt; 0，那么 epoll_wait 就会阻塞这么长时间后返回（毫秒）。
<br>你可能会好奇，epoll instance 中明明有一个 ready list，为什么还需要人为设置一个 events 数组？这时因为 ready list 是一个内核中的数据结构，我们在用户态是访问不到的，所以我们要提供一个存放文件描述符的数组，让内核在事件发生后将 ready list 中的 fd 拷贝到 events 数组中。拷贝完成后， ready list 将会被清空。<br><br>epoll 支持两种不同的触发方式：水平触发 (LT) 和边沿触发 (ET) 。下面，我们将用一个简单的例子来说明这两种不同的触发方式。<br>假设我们现在通过一个 socket 来请求远方的数据，然后把接收方的文件描述符注册到一个 epoll instance 上。一旦我们调用 epoll_wait，线程就会被阻塞，直到操作系统通知线程有数据到达（通过回调）。假如到达 5M 数据，而我们只读取 3MB，buffer 中剩余 2MB。当我们读取完成后继续调用 epoll_wait，两种不同的触发方式会发生什么？<br><br>水平触发是一种状态驱动的模式，也就是说只要文件描述符满足某种状态条件，epoll_wait 就会持续返回该 fd 的事件。这类似于水位报警器——只要水位高于警戒线，警报就会持续触发。<br>如果缓冲区中有未读的滞留数据，那么再次调用的 epoll_wait 会立即返回。即使你没有发生方没有再发送数据。水平触发方式时 epoll 默认情况下的触发方式，适用于大多数应用场景。<br>当 epoll 使用默认的 LT 时，他的行为就类似于 poll，只是 epoll 的速度会更快一些。所以在任何情况下的开发，你都应当优先考虑 epoll，因为思想上差不多的。<br><br>边沿触发是一种事件驱动的模式，只在 fd 的状态发生变化时触发事件通知（如：从无数据到有数据）。如果没有新的数据到达，即使缓冲区仍然有数据，epoll_wait() 仍然会继续阻塞等待数据的到来（从“无”到”有“）。 <br>关键在于，在 ET 模式下，线程必须持续读数据，直到 read() 或 write() 返回 EAGAIN（完全读完缓冲区），否则就可能错过滞留数据。又由于 ET 不会重复通知缓冲区的未读数据，因此它需要配合非阻塞 IO 使用，避免长时间不必要的阻塞。<br><br>继续 socket 的例子，我们知道 TCP 的数据可能分片到达，这种分批次到达的情况对于 LT 而言可能轻松应对。然而对于 ET 来说，由于数据分批次，所以一个事件可能被多次触发。如果你不想让 epoll_wait() 反复处理该 fd，我们就可以启用 EPOLLONESHOT 标志。<br>它的作用就是在事件触发时自动禁用该 fd 的后续通知，防止一个 fd 被 epoll_wait 反复触发。这个标志我们在前面的 epoll_ctl 中有提到。<br><br>如果 fd 是 ET 模式被添加到了 epoll instance 中，那么在多个线程同时调用 epoll_wait 监听同一个 epoll instance 时，epoll 可以保证只有一个线程被唤醒。避免了”惊群效应“，即多个线程被同时唤醒，导致资源的浪费。<br><br>在 Linux 系统中，/sys/power/autosleep 允许设备进入自动休眠模式以节省电力。当系统处于自动休眠模式时，某些事件的发生（网络数据到达，用户输入）会唤醒设备。但是设备仅仅维持该事件被加入到事件队列中，随后可能再次进入休眠状态。<br>如果你希望设备在事件发生后一直保持唤醒，知道事件被处理完毕，那么就需要把 EPOLLWAKEUP 标志启用。这个标志在 epoll_ctl 中有提到。（如果希望设备在第二次 epoll_wait 之前保持唤醒，就需要使用额外的 wake_lock ）。<br><br>在 ET 模式下：<br>
<br>Non-Blocking Sockets
<br>Continuing Reading/Writing
<br>Recording States
<br><br><br>第一节课的时候，我们学习了三个用于服务器上的异步I/O处理方式。本节课，我们来了解一些如何通过Client URL来处理网络客户端上的异步I/O。<br>cURL 是对 socket 接口的封装，它是一个用户程序，可能需要你自己下载安装相关的开发包。随后，通过包含头文件 curl/curl.h 并链接 -lcurl ，你就能在你的程序中使用libcurl库来请求HTTP服务、下载文件和一些其他的网络服务。<br>由于大多数的资源在服务器的手中，客户端想要某些资源就需要向服务器发送响应的请求。同样，当你使用 cURL 进行请求时，默认的 cURL API（curl_easy_perform()）会阻塞我们的程序。而通过上节课，我们知道，这种阻塞并不是我们想要的。我们想不浪费资源的同时提高程序的效率。<br>幸运的是，libcurl库中有能够让我们同时handle多个异步IO的API。curl_multi允许客户端同时进行多个网络操作请求，而且不会阻塞用户程序。但再此之前，我们先来看看默认的 cURL API。<br><br>在使用这些libcurl的库函数之前，我们需要调用curl_global_init()进行全局初始化，并在程序结束时调用curl_global_cleanup()进行全局清理。<br>下面是它们的函数原型：<br>#include &lt;curl/curl.h&gt;

CURLcode curl_global_init(long flags);
/* Initializes the cURL library globally.
   Parameters:
   1. flags: Bitmask of options to initialize. Commonly used values include:
      - CURL_GLOBAL_DEFAULT: Initialize all supported features (equivalent to CURL_GLOBAL_SSL | CURL_GLOBAL_WIN32).
      - CURL_GLOBAL_SSL: Initialize SSL.
      - CURL_GLOBAL_WIN32: Initialize Windows-specific features.
      - CURL_GLOBAL_ALL: Equivalent to CURL_GLOBAL_DEFAULT.
      - CURL_GLOBAL_NOTHING: Initialize no features.
   Return value:
   - On success, returns CURLE_OK.
   - On failure, returns a CURLcode error value. Those error values include:
	   - CURLE_UNSUPPORTED_PROTOCOL
	   - CURLE_FAILED_INIT
	   - CURLE_URL_MALFORMAT
	   - CURLE_COULDNT_RESOLVE_HOST
	   - CURLE_COULDNT_CONNECT
	   - CURLE_OPERATION_TIMEDOUT
	   - CURLE_SSL_CONNECT_ERROR
	   - CURLE_PEER_FAILED_VERIFICATION
*/
<br>curl_global_init(long flags);为我们提供了一些可选项。一般我们会使用在flags字段中填入CURL_GLOBAL_DEFAULT，表示初始化所有cURL支持的功能，它和CURL_GLOBAL_ALL是等价的。在初始化完成之后，用户会得到CURLcode类型的返回值，用于反馈操作结果。<br>在程序结束或我们不再需要cURL库的服务时，我们就可以使用curl_global_cleanup()全局清理cURL库。这个函数不需要任何参数，也不会有返回值。<br>void curl_global_cleanup(void);
/* Cleans up the cURL library globally.
   Parameters: None.
   Return value: None.
*/
<br><br>libcurl库为我们提供了一些库函数用于与cURL进行交互，这些函数使得我们可以方便地进行网络请求和数据传输。通过使用这些库函数，我们可以实现HTTP、HTTPS、FTP等多种协议的支持。在cURL Easy API Interface中，我们会使用cURL easy handle来获取cURL的服务。<br><br>在全局初始化完成之后，我们就可以获取cURL handle并设置相应的选项来获取cURL的服务。其中我们用curl_easy_init()初始化并获取一个cURL easy handle，用curl_easy_setopt()设置handle的属性。<br>下面是这两个函数的原型：<br>CURL *curl_easy_init(void);
/* Initializes a CURL easy handle.
   Return value:
   - On success, returns a pointer to a CURL easy handle.
   - On failure, returns NULL.
*/
<br>curl_easy_init()并不需要什么参数，如果初始化成功完成，它会返回一个指向初始化handle的指针，如果失败，它会返回NULL。<br>CURLcode curl_easy_setopt(CURL *handle, CURLoption option, ...);
/* Sets options for a CURL easy handle.
   Parameters:
   1. handle: The CURL easy handle.
   2. option: The option to set. Common options include:
      - CURLOPT_URL: The URL to fetch.
      - CURLOPT_POSTFIELDS: The data to send in a POST request.
      - CURLOPT_HTTPHEADER: A linked list of HTTP headers to include in the request.
      - CURLOPT_WRITEFUNCTION: A callback function to handle data received from the server.
      - CURLOPT_WRITEDATA: A pointer to pass to the write callback function.
      - CURLOPT_READFUNCTION: A callback function to handle data sent to the server.
      - CURLOPT_READDATA: A pointer to pass to the read callback function.
      - CURLOPT_TIMEOUT: The maximum time in seconds that the request is allowed to take.
   3. ...: The value to set for the option, depends on the option being set.
   Return value:
   - On success, returns CURLE_OK.
   - On failure, returns a CURLcode error value.
*/
<br>curl_easy_setopt()要复杂的多。这个库函数会根据不同的设置项来配置&nbsp;cURL&nbsp;easy&nbsp;handle&nbsp;的行为。而且每个选项后面的参数类型会根据不同的选项而有所不同。<br><br>所有的所有都完成之后，我们就可以使用curl_easy_perform()来执行请求了。<br>CURLcode curl_easy_perform(CURL *handle);
/* Performs the file transfer.
   Parameters:
   1. handle: The CURL easy handle.
   Return value:
   - On success, returns CURLE_OK.
   - On failure, returns a CURLcode error value.
*/
<br><br>完成请求后，我们使用curl_easy_cleanup()来清理handle。<br>void curl_easy_cleanup(CURL *handle);
/* Cleans up a CURL easy handle.
   Parameters:
   1. handle: The CURL easy handle to clean up.
   Return value: None.
*/
<br><br>在上面，我们看到好几个函数都会返回的CURLcode用于指示函数是否正常运行。我们可以通过库函数curl_easy_strerror()&nbsp;来查看具体的错误信息。这个函数会返回一个描述错误的字符串，帮助我们理解问题之所在。<br>以下是&nbsp;curl_easy_strerror()&nbsp;的函数原型：<br>const char *curl_easy_strerror(CURLcode errornum);
/* Returns a string describing the CURLcode error.
   Parameters:
   1. errornum: The CURLcode error value.
   Return value:
   - Returns a pointer to a null-terminated string describing the error.
*/
<br><br>在下面的例子中，我们通过设置 curl_easy_setopt(curl, CURLOPT_URL, "https://congzhi.wiki/"); 来请求网站的 index.html 。<br>#include &lt;stdio.h&gt;
#include &lt;curl/curl.h&gt;

int main(int argc, char** argv){
    CURL *curl;
    CURLcode res;

    curl_global_init(CURL_GLOBAL_DEFAULT);

    curl = curl_easy_init();
    if(curl){
        curl_easy_setopt(curl, CURLOPT_URL, "https://congzhi.wiki/");
        res = curl_easy_perform(curl);
    }
    if(res != CURLE_OK){
        fprintf(stderr, "curl_easy_perform() failed: %s\n",
				curl_easy_strerror(res));
        curl_easy_cleanup(curl);
    }
    curl_easy_cleanup(curl);
    curl_global_cleanup();
    return 0;
}
<br>cURL easy 什么都好，但和其他请求 IO 的函数一样，res = curl_easy_perform(curl); 这一步会阻塞程序，这就是 easy perform 最大的缺点。<br><br>为了确保一个线程能够操作多个异步I/O，libcurl库提供了 cURL&nbsp;multi&nbsp;API 。cURL&nbsp;multi 用于管理一组easy&nbsp;handles，通过将多个easy&nbsp;handles放到一个队列中，并检查它们的状态来实现异步I/O。我们将包含多个easy handles队列的这样一个结构叫做multi handle。<br><br>因为 cURL multi 是在 cURL easy 的基础上建立起来的，所以在使用cURL multi时，我们仍然需要进行cURL全局上的初始化、清理等。此外，由于我们引入了新的结构，我们需要一个新的类型来表示multi handle。在libcurl中，这个结构叫CURLM。<br><br>curl_multi&nbsp;内部维护了两个队列，分别是正在进行的传输队列和已经完成的传输队列。当curl multi handle初始化完成后，两个队列也随之初始化。<br>此外，curl_multi_init 还会初始化内部状态和数据结构、分配必要的资源和内存等。<br>函数原型如下：<br>#include &lt;curl/multi.h&gt; // Included &lt;curl/curl.h&gt;

CURLM *curl_multi_init(void);
/* Initializes a CURL multi handle.
   Return value:
   - On success, returns a pointer to a CURL multi handle.
   - On failure, returns NULL.
*/
<br><br>我们有了新的结构后，我们可以往里面添加任意数量的 easy handle 。我们所添加的 easy handles 就会被放到正在进行的传输队列中。<br>CURLMcode curl_multi_add_handle(CURLM *multi_handle, CURL *easy_handle);
/* Adds a CURL easy handle to a CURL multi handle.
   Parameters:
   1. multi_handle: The CURL multi handle.
   2. easy_handle: The CURL easy handle to add.
   Return value:
   - On success, returns CURLM_OK.
   - On failure, returns a CURLMcode error value.
*/
<br><br>在添加完需要添加的 easy handles 之后，我们就可以使用 curl_multi_perform() 来启动多个并发的网络请求（第一次使用），这个函数会立即返回正在进行传输队列中的 easy handles 的数量，所以这些请求不会阻塞当前程序。<br>其函数原型如下：<br>CURLMcode curl_multi_perform(CURLM *multi_handle, int *still_running_handles);
/* Performs the transfers for all added handles.
   Parameters:
   1. multi_handle: The CURL multi handle.
   2. still_running_handles: Pointer to an integer that will be set to the number of running handles.
   Return value:
   - On success, returns CURLM_OK.
   - On failure, returns a CURLMcode error value.
*/
<br>启用后，当我们使用curl_multi_perform()时，它会更新参数still_running_handles用于指示multi handle中仍在进行传输的easy handles的数量。当这个参数指向的数字变成0，你就知道IO全部完成了。而这就意味着我们需要多次调用curl_multi_perform()，这是否意味着我们在轮询呢？<br><br>尽管我们实现了在一个线程中处理多个IO，我们仍不希望无意义的轮询占用太多CPU时间。为了解决这个问题，libcurl 提供了 curl_multi_wait() 函数，它可以在有数据可读或可写之前阻塞一段时间，让紧轮询变成松轮询。<br>下面是它的函数原型：<br>CURLMcode curl_multi_wait(CURLM *multi_handle, struct curl_waitfd extra_fds[], 
						  unsigned int extra_nfds, int timeout_ms, int *numfds);
/* Waits for activity on any of the curl easy handles within a multi handle.
   Parameters:
   1. multi_handle: The CURL multi handle.
   2. extra_fds: An array of extra fds to wait on.(NULL)
   3. extra_nfds: The number of extra file descriptors.(0)
   4. timeout_ms: The maximum time to wait in milliseconds.
   5. numfds: Pointer to an integer number of "interesting" events occurred.
   Return value:
   - On success, returns CURLM_OK.
   - On failure, returns a CURLMcode error value.
*/
<br>同样的，我们可以用curl_mulri_strerror()来查看发生了什么错误。<br>const char *curl_multi_strerror(CURLMcode errornum);
/* Returns a string describing the CURLMcode error.
   Parameters:
   1. errornum: The CURLMcode error value.
   Return value:
   - Returns a pointer to a null-terminated string describing the error.
*/
<br><br>当加入cURL multi的cURL easy handle完成传输任务后，curl multi就会将其添加到一个已完成传输的队列中。之后我们就可以通过curl_multi_info_read()读取easy handle的状态了。<br>curl_multi_info_read()的函数原型如下：<br>CURLMsg *curl_multi_info_read(CURLM *multi_handle, int *msgs_left);
/* Reads information about completed transfers.
   Parameters:
   1. multi_handle: The CURL multi handle.
   2. msgs_left: Pointer to an integer that will be set to the number of messages left in the queue.
   Return value:
   - On success, returns a pointer to a CURLMsg structure.
   - On failure, returns NULL.
*/
<br>当读取成功时，curl_multi_info_read()会返回一个结构体并将所读取的curl easy从完成传输队列中移除。CURLMsg是一个结构体，用于提供有关传输完成的详细信息。它的原型如下：<br>typedef struct {
    CURLMSG msg;       /* What this message means */
    CURL *easy_handle; /* The handle it concerns */
    union {
        void *whatever; /* Message-specific data */
        CURLcode result; /* Return code for transfer */
    } data;
} CURLMsg;
<br>这个结构体的第一个字段CURLMSG表示消息的类型。这是一个枚举类型，可能的值有CURLMSG_DONE表示传输完成和CURLMSG_ERR表示有发生错误。<br>第二个字段表示与当前消息相关联的easy handle。<br>第三个字段是一个联合体data，用于存储不同的类型，其中CURLcode result;表示curl easy传输的结果，我们前面见过。<br><br>当一个easy handle完成后，我们就可以将它从multi handle中移除出去了。我们可以通过msg-&gt;easy_handle在结构体CURLMsg中获取到确切的easy_handle。然后通过相关的库函数将其移除并进行清理。（curl_easy_cleanup(CURL* eh)）<br>curl_multi_remove_handle()的函数原型如下：<br>CURLMcode curl_multi_remove_handle(CURLM *multi_handle, CURL *easy_handle);
/* Removes an easy handle from a multi handle.
   Parameters:
   1. multi_handle: The CURL multi handle.
   2. easy_handle: The CURL easy handle to remove.
   Return value:
   - On success, returns CURLM_OK.
   - On failure, returns a CURLMcode error value, such as:
     - CURLM_BAD_HANDLE: The provided multi handle is invalid.
     - CURLM_BAD_EASY_HANDLE: The provided easy handle is invalid.
     - CURLM_OUT_OF_MEMORY: Memory allocation failed.
*/
<br><br>当我们不再需要curl multi时，就可以通过curl_multi_cleanup()将其清理掉，其函数原型如下：<br>CURLMcode curl_multi_cleanup(CURLM *multi_handle);
/* Cleans up a CURL multi handle.
   Parameters:
   1. multi_handle: The CURL multi handle to clean up.
   Return value:
   - On success, returns CURLM_OK.
   - On failure, returns a CURLMcode error value.
*/
<br><br><br>Reuse the curl easy handles all the time.<br><br><br><br><br>在学习异步 I/O 的过程中，我们希望 I/O 操作能够避免阻塞当前线程。然而，通过之前的学习，我们发现，尽管文件 fd 的读取涉及磁盘 I/O，与 socket 或 pipe 这类 fd 操作不同，文件操作通常能够确保数据始终可读。因此，即使将文件的 fd 设置为 O_NONBLOCK，它实际上仍然会导致阻塞。<br>我们想要的是让文件加载到内存中之后再读，而不是阻塞硬控线程 5ms，该怎么办？POSIX AIO 闪亮登场！在发起文件 I/O 请求后，AIO 可以让线程立即返回，不被阻塞。并且当 I/O 操作完成时，通过回调或通知机制处理数据。完美！<br><br>AIO 会创建一个 aiocb (AIO Control Block) 控制块，带有可选回调，这是一个操作描述符，管理 AIO 的具体操作（告诉操作系统你想要干什么）。<br><br>#include &lt;aio.h&gt;

struct aiocb {
    int             aio_fildes;   /* File descriptor */
    off_t           aio_offset;   /* File offset */
    volatile void  *aio_buf;      /* Buffer for data */
    size_t          aio_nbytes;   /* Number of bytes for I/O */
    int             aio_reqprio;  /* Request priority */
    struct sigevent aio_sigevent; /* Signal or function to call when I/O completes */
    int             aio_lio_opcode; /* Operation to be performed (LIO_READ, LIO_WRITE, etc.) */
};
<br>#include &lt;signal.h&gt;

struct sigevent {
    int          sigev_notify;              /* Notification type */
    int          sigev_signo;               /* Signal number (for signal-based notification) */
    union sigval sigev_value;               /* Data passed with notification */
    void       (*sigev_notify_function)(union sigval); /* Callback function (if applicable) */
    void        *sigev_notify_attributes;   /* Attributes for the thread (if using thread notification) */
};

union sigval {
	int          sival_int;
	void*        sival_ptr;
};
<br><br><br><br>]]></description><link>https://congzhi.wiki/congzhi's-os-series/16.-asynchronous-io.html</link><guid isPermaLink="false">Congzhi's OS Series/16. Asynchronous IO.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 19 Aug 2025 17:13:44 GMT</pubDate></item><item><title><![CDATA[Congzhi's OS Series]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">🔙Go Back Home</a><br><br><br>This series is intended solely for educational purposes. Some images used in this series are sourced from the internet. If any of these images conflict with your creation principles, please contact me at <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com." rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com." target="_blank">duzhi_02@qq.com.</a> I would address the issue ASAP.<br>所有内容均由我独自完成，所有学习资料来源如下：<br>
<br><a data-tooltip-position="top" aria-label="https://space.bilibili.com/286191426/channel/collectiondetail?sid=2293786" rel="noopener nofollow" class="external-link" href="https://space.bilibili.com/286191426/channel/collectiondetail?sid=2293786" target="_blank">Y4NGY操作系统课程</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/playlist?list=PLFCH6yhq9yAHFaI00FrrgG0dPg8a5SjTJ" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/playlist?list=PLFCH6yhq9yAHFaI00FrrgG0dPg8a5SjTJ" target="_blank">ECE 252: Systems Programming and Concurrency</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/playlist?list=PLFCH6yhq9yAHFUGyk4U5KaoA24gnnDJA-" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/playlist?list=PLFCH6yhq9yAHFUGyk4U5KaoA24gnnDJA-" target="_blank">ECE 350: Real-Time Operating System</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=cQP8WApzIQQ&amp;list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=cQP8WApzIQQ&amp;list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB" target="_blank">MIT 6.824: Distributed Systems</a>
<br><a data-tooltip-position="top" aria-label="https://zh.z-lib.gs/book/25277046/1b1f19/operating-system-concepts.html?ts=1905" rel="noopener nofollow" class="external-link" href="https://zh.z-lib.gs/book/25277046/1b1f19/operating-system-concepts.html?ts=1905" target="_blank">Operating System Concepts from Z-Lib</a>
<br><a data-tooltip-position="top" aria-label="https://www.cs.csustan.edu/~john/Classes/CS3750/" rel="noopener nofollow" class="external-link" href="https://www.cs.csustan.edu/~john/Classes/CS3750/" target="_blank">Prof. John Sarraillé's CS3750 course notes</a> and <a data-tooltip-position="top" aria-label="https://www.cs.csustan.edu/~john/Classes/CS3750/Slides/" rel="noopener nofollow" class="external-link" href="https://www.cs.csustan.edu/~john/Classes/CS3750/Slides/" target="_blank">slides</a>
<br><a data-tooltip-position="top" aria-label="https://wiki.osdev.org" rel="noopener nofollow" class="external-link" href="https://wiki.osdev.org" target="_blank">OSDev Wiki</a>
<br><a data-tooltip-position="top" aria-label="https://www.man7.org/linux/man-pages/index.html" rel="noopener nofollow" class="external-link" href="https://www.man7.org/linux/man-pages/index.html" target="_blank">Linux man pages online</a>
<br><a data-tooltip-position="top" aria-label="https://docs.kernel.org/" rel="noopener nofollow" class="external-link" href="https://docs.kernel.org/" target="_blank">The Linux Kernel documentation</a>
<br><a data-tooltip-position="top" aria-label="https://space.bilibili.com/202224425/lists/4823953?type=season" rel="noopener nofollow" class="external-link" href="https://space.bilibili.com/202224425/lists/4823953?type=season" target="_blank">操作系统原理（2025） - 蒋炎岩</a>
<br>...
<br><br><br>本系列将从一个相对全面的角度向您揭开操作系统神秘的面纱。系列的编排参考 Operating System Concepts，以一个个相对独立的 Parts 逐步深入、层层拆解操作系统的各个模块。经过学习，您将能够：<br>
<br>轻松表述 OS 在计算机系统中的层次地位
<br>深入理解操作系统的关键组件与核心理念
<br>掌握主流桌面和移动操作系统的架构原理
<br>探索现代操作系统的前沿技术与发展现状
<br>......
<br>系列仍在施工中......在 2025 年内，将计划第一遍重写优化整个系列，旨在调整所有阶段的内容编排并做相应的补充。更详细地补充将在之后第二遍重写时更新。<br>	 |ˉˉˉˉˉˉˉ|
	 |Salute!|
	 |__  ___|
	    |/
	(◍•ᴗ•◍)ゝ
<br>后续，系列的结构还会不断调整更新。<br><br><br>目前阶段划分如下，其中 x(extend) 表示拓展模块或小节。部分章节内容尚待补充，但不影响整体阅读体验：<br>Congzhi's OS Series
PART ONE: INTRODUCTION
<a data-href="1. Introduction to Operating Systems" href="https://congzhi.wiki/congzhi's-os-series/1.-introduction-to-operating-systems.html" class="internal-link" target="_self" rel="noopener nofollow">1. Introduction to Operating Systems</a><br>
<a data-href="2. Evolution of Operating Systems" href="https://congzhi.wiki/congzhi's-os-series/2.-evolution-of-operating-systems.html" class="internal-link" target="_self" rel="noopener nofollow">2. Evolution of Operating Systems</a><br>
<a data-href="3. Operating System Architectures" href="https://congzhi.wiki/congzhi's-os-series/3.-operating-system-architectures.html" class="internal-link" target="_self" rel="noopener nofollow">3. Operating System Architectures</a>

PART TWO: INTERRUPTS &amp; SYSTEM BOOT
<a data-href="4. Interrupts and System Calls" href="https://congzhi.wiki/congzhi's-os-series/4.-interrupts-and-system-calls.html" class="internal-link" target="_self" rel="noopener nofollow">4. Interrupts and System Calls</a><br>
<a data-href="5. System Boots Up" href="https://congzhi.wiki/congzhi's-os-series/5.-system-boots-up.html" class="internal-link" target="_self" rel="noopener nofollow">5. System Boots Up</a>

PART THREE: PROCESS MANAGEMENT
<a data-href="6. Processing The Processes" href="https://congzhi.wiki/congzhi's-os-series/6.-processing-the-processes.html" class="internal-link" target="_self" rel="noopener nofollow">6. Processing The Processes</a><br>
<a data-href="7. Inter-Process Communication" href="https://congzhi.wiki/congzhi's-os-series/7.-inter-process-communication.html" class="internal-link" target="_self" rel="noopener nofollow">7. Inter-Process Communication</a><br>
<a data-href="8. Threads and Concurrency" href="https://congzhi.wiki/congzhi's-os-series/8.-threads-and-concurrency.html" class="internal-link" target="_self" rel="noopener nofollow">8. Threads and Concurrency</a><br>
<a data-href="9. CPU Scheduling" href="https://congzhi.wiki/congzhi's-os-series/9.-cpu-scheduling.html" class="internal-link" target="_self" rel="noopener nofollow">9. CPU Scheduling</a>

PART FOUR: SYNCHRONIZATION
<a data-href="10. Synchronization and Mutex" href="https://congzhi.wiki/congzhi's-os-series/10.-synchronization-and-mutex.html" class="internal-link" target="_self" rel="noopener nofollow">10. Synchronization and Mutex</a><br>
<a data-href="11. Deadlock" href="https://congzhi.wiki/congzhi's-os-series/11.-deadlock.html" class="internal-link" target="_self" rel="noopener nofollow">11. Deadlock</a><br>
<a data-href="11.5 Advanced Concurrency Problems" href="https://congzhi.wiki/11.5 Advanced Concurrency Problems" class="internal-link" target="_self" rel="noopener nofollow">11.5 Advanced Concurrency Problems</a>

PART FIVE: MEMORY MANAGEMENT
<a data-href="12. Memory Management" href="https://congzhi.wiki/congzhi's-os-series/12.-memory-management.html" class="internal-link" target="_self" rel="noopener nofollow">12. Memory Management</a>

PART SIX: I/O &amp; MASS STORAGE &amp; FILE-SYSTEM MANAGEMENT
<a data-href="13. IO Subsystem" href="https://congzhi.wiki/congzhi's-os-series/13.-io-subsystem.html" class="internal-link" target="_self" rel="noopener nofollow">13. IO Subsystem</a><br>
<a data-href="14. Mass Storage" href="https://congzhi.wiki/congzhi's-os-series/14.-mass-storage.html" class="internal-link" target="_self" rel="noopener nofollow">14. Mass Storage</a><br>
<a data-href="15. File Systems" href="https://congzhi.wiki/congzhi's-os-series/15.-file-systems.html" class="internal-link" target="_self" rel="noopener nofollow">15. File Systems</a><br>
<a data-href="16. Asynchronous IO" href="https://congzhi.wiki/congzhi's-os-series/16.-asynchronous-io.html" class="internal-link" target="_self" rel="noopener nofollow">16. Asynchronous IO</a>

PART SEVEN: ADVANCED TOPICS
<a data-href="17. Virtualization and Container" href="https://congzhi.wiki/congzhi's-os-series/17.-virtualization-and-container.html" class="internal-link" target="_self" rel="noopener nofollow">17. Virtualization and Container</a><br>
<a data-href="18. Security and Protection" href="https://congzhi.wiki/18. Security and Protection" class="internal-link" target="_self" rel="noopener nofollow">18. Security and Protection</a>

PART EIGHT: CASE STUDIES
<a data-href="C. GNU/Linux" href="https://congzhi.wiki/C. GNU/Linux" class="internal-link" target="_self" rel="noopener nofollow">C. GNU/Linux</a><br>
<a data-href="C. Windows" href="https://congzhi.wiki/C. Windows" class="internal-link" target="_self" rel="noopener nofollow">C. Windows</a><br>
<a data-href="C. macOS" href="https://congzhi.wiki/C. macOS" class="internal-link" target="_self" rel="noopener nofollow">C. macOS</a><br>
<a data-href="C. Android" href="https://congzhi.wiki/C. Android" class="internal-link" target="_self" rel="noopener nofollow">C. Android</a><br>
<a data-href="C. iOS" href="https://congzhi.wiki/C. iOS" class="internal-link" target="_self" rel="noopener nofollow">C. iOS</a>

PART T: TOOLS
<a data-href="T. GDB for Debugging" href="https://congzhi.wiki/T. GDB for Debugging" class="internal-link" target="_self" rel="noopener nofollow">T. GDB for Debugging</a><br>
<a data-href="T. Valgrind and Helgrind" href="https://congzhi.wiki/congzhi's-os-series/t.-valgrind-and-helgrind.html" class="internal-link" target="_self" rel="noopener nofollow">T. Valgrind and Helgrind</a>

Distributed Systems -  Moments to Remember
<a data-href="Lecture 1 - Introduction" href="https://congzhi.wiki/congzhi's-os-series/lecture-1-introduction.html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 1 - Introduction</a><br>
<a data-href="Lecture 2 - RPC and Threads" href="https://congzhi.wiki/congzhi's-os-series/lecture-2-rpc-and-threads.html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 2 - RPC and Threads</a>

<br><br><br>如果您发现任何知识上的疑问、错误和错别字问题，也欢迎通过邮箱 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/congzhi's-os-series/congzhi's-os-series.html</link><guid isPermaLink="false">Congzhi's OS Series/Congzhi's OS Series.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 31 Jul 2025 02:12:48 GMT</pubDate></item><item><title><![CDATA[Lecture 1 - Introduction]]></title><description><![CDATA[ 
 <br><br>MIT 6.824 Distributed Systems - Moments to Remember 用于标记我在学习分布式系统过程中认为比较重要和个人感兴趣的知识，同时记录我疑惑和思考的过程。<br>在最新的<a data-tooltip-position="top" aria-label="http://nil.csail.mit.edu/6.5840/2024/schedule.html" rel="noopener nofollow" class="external-link" href="http://nil.csail.mit.edu/6.5840/2024/schedule.html" target="_blank">课程表</a>中，虽然课程名称变成 MIT 6.5840 了，但是内容差不多，都是分布式系统设计。<br><br>什么是分布式系统？简单来说，分布式系统就是一个由多个节点（独立的计算机、虚拟机、容器）组成的计算机网络。这些节点通过协作完成复杂的任务。<br><br>人多力量大，我们需要分布式系统的一大原因就是单独主机能够提供的性能不足以满足任务的处理。而分布式系统可以通过增加节点引入更多的核心，更多的内存，从而分担计算压力 (parallelism computing)。<br>而且多节点提供的容错性 (fault tolerance) 和可扩展性也是单独主机所满足不了的。因为分布式系统往往有多个节点，当某个或部分节点故障 (partial failure)，其他节点 (replica servers) 可以接管任务，不用担心单节点造成整个系统的故障 (total failure)。同时，系统节点数量可以根据负载情况来动态地增加和减少。<br>另外，虽然数据在多个节点之间传输的延迟（socket API and underlyng hardwares）肯定会远大于在一台主机上的数据传输（进程间通信），但是分布式系统可以将任务放在离用户更近的节点来减少远程传输延迟 (bring it home)，降低用户端到服务器之间数据传输所需要的时间。<br>但这样就一定会带来一些问题跟挑战，比如如何保证数据在多个节点中的一致性？如何设计容错机制来确保整个系统的健壮性，避免部分故障的发生？如何协调这些节点最大化系统性能？<br><br>这个课程的目标就是用 Go 语言设计并实现一个 infrastructure for applications。通过这个 infrastructure，底层分布式的服务器得以隐藏其细节，对应用透明化，即应用可以无感知地使用分布式服务器，而无需关注实现细节。<br>为实现这样一个系统，分布式系统设计将聚焦于三个方面：存储 (storage)、通讯 (communication)和计算 (computation)。通过抽象，我们想让分布式系统就像一台主机一样操作。<br><br>Fault Tolerance<br>Availability<br>
Recoverability (roll-back.....) non-volatile storage(expensive)/replication<br>Consistency<br>put replicas as independent as possible(not even in one power cable)<br>weak consisitence or strong?<br><br>📌 可靠传输：如何避免消息丢失或重复？（如 TCP vs. QUIC） 📌 负载均衡：如何在多个服务器间分发请求？（如 RPC 代理、反向代理） 📌 状态维护：如何处理长连接、会话状态？（如 gRPC、WebSocket）<br>🔹 抽象思路：让分布式通讯看起来像本地进程通信，应用层不需要关心底层网络，而是像调用本地 API 一样与远程服务器交互。<br><br>📌 任务调度：如何分配计算任务，避免某些节点过载？（如 Kubernetes 调度）<br>
📌 资源隔离：如何保证不同用户或任务不会干扰？（如 Cgroup、Namespace） 📌 弹性扩展：如何根据负载动态增加或减少计算资源？（如 Serverless 计算）<br>🔹 抽象思路：让计算资源看起来像一个统一的 CPU 池，无论计算在哪个服务器上执行，应用层都能像调用本地线程一样使用它。<br><br>📌 存储：提供一个“虚拟磁盘”，让数据像本地存储一样访问 📌 通讯：封装 RPC，让远程服务器像本地 API 一样调用 📌 计算：构建统一调度，让所有计算任务像本地 CPU 线程一样执行<br>这样，整个分布式系统在应用层看来像是一台强大的单机，而底层复杂的分布式机制被隐藏起来。]]></description><link>https://congzhi.wiki/congzhi's-os-series/lecture-1-introduction.html</link><guid isPermaLink="false">Congzhi's OS Series/Lecture 1 - Introduction.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:32:29 GMT</pubDate></item><item><title><![CDATA[Lecture 2 - RPC and Threads]]></title><description><![CDATA[ 
 <br><br>MIT 6.824 Distributed Systems - Moments to Remember 用于标记我在学习分布式系统过程中认为比较重要和个人感兴趣的知识，同时记录我疑惑和思考的过程。<br>在最新的<a data-tooltip-position="top" aria-label="http://nil.csail.mit.edu/6.5840/2024/schedule.html" rel="noopener nofollow" class="external-link" href="http://nil.csail.mit.edu/6.5840/2024/schedule.html" target="_blank">课程表</a>中，虽然课程名称变成 MIT 6.5840 了，但是内容差不多，都是分布式系统设计。<br><br><br>Good support to thread and locking and synchronization to thread <br>分布式系统需要并行性，而线程是我们实行并行性程序的主要方式（I/O Concurrency -  一个程序可以有多个 goroutine 来响应多个 RPC （分布式）。<br>
还有就是多CPU上的“真”并行化）<br>Goroutine and threads<br>
M:N<br>
Everything is just a goroutine<br>低调度开销<br><br>mutexlocks<br>
channels<br>
sync.Cond<br>
waitGroup<br><br><br>Type safe(all compilation-time) and memory safe (garbage collection)<br>]]></description><link>https://congzhi.wiki/congzhi's-os-series/lecture-2-rpc-and-threads.html</link><guid isPermaLink="false">Congzhi's OS Series/Lecture 2 - RPC and Threads.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 21 Jun 2025 16:11:55 GMT</pubDate></item><item><title><![CDATA[CS50 SQL]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br><a data-tooltip-position="top" aria-label="https://www.youtube.com/cs50" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/cs50" target="_blank">CS50</a>本身是 Harvard 开设的面向大众的计算机科学入门课程(Introduction to Computer Science)，由 David J. Malan 教授进行授课，其中就包含 SQL 和其他的内容。现在，你可以在 CS50 上学到很多 Harvard 和 Yale 所开设的一些小而精的计算机科学课程，很多都作为 CS50 Lectures 内容的延申，其中就包括这个 folder 下的 CS50's Introduction to Databases with SQL。<br><br><br>这个系列是对 CS50 数据库课程的一些笔记。内容结构会基本按照 CS50 课程的编排走，内容也会相对基础。参考来源有：<br>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/" target="_blank">CS50's Introduction to Databases with SQL</a>
<br>...
<br><br><br><a data-href="CS50 SQL" href="https://congzhi.wiki/cs50-sql/cs50-sql.html" class="internal-link" target="_self" rel="noopener nofollow">CS50 SQL</a> 的目录结构如下，你可以在 <a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/" target="_blank">CS50</a> 学到 Lecture 0 - Lecture 6 的所有知识。你还能在官网上做 CS50 的课后作业并提交。Lecture X 是另一些课程中的内容。<br>CS50 SQL (NC Stand for Not Covered)

<br><a data-href="Lecture 0 - Querying" href="https://congzhi.wiki/cs50-sql/lecture-0-querying.html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 0 - Querying</a>
<br><a data-href="Lecture 1 - Relating (NC)" href="https://congzhi.wiki/cs50-sql/lecture-1-relating-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 1 - Relating (NC)</a>
<br><a data-href="Lecture 2 - Designing (NC)" href="https://congzhi.wiki/cs50-sql/lecture-2-designing-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 2 - Designing (NC)</a>
<br><a data-href="Lecture 3 - Writing (NC)" href="https://congzhi.wiki/cs50-sql/lecture-3-writing-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 3 - Writing (NC)</a>
<br><a data-href="Lecture 4 - Viewing (NC)" href="https://congzhi.wiki/cs50-sql/lecture-4-viewing-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 4 - Viewing (NC)</a>
<br><a data-href="Lecture 5 - Optimizing (NC)" href="https://congzhi.wiki/cs50-sql/lecture-5-optimizing-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 5 - Optimizing (NC)</a>
<br><a data-href="Lecture 6 - Scaling (NC)" href="https://congzhi.wiki/cs50-sql/lecture-6-scaling-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Lecture 6 - Scaling (NC)</a>

<br><br><br>如有疑问或内容问题，欢迎通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/cs50-sql/cs50-sql.html</link><guid isPermaLink="false">CS50 SQL/CS50 SQL.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:24:56 GMT</pubDate></item><item><title><![CDATA[<a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#lecture-5" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#lecture-5" target="_blank">Lecture 5</a>]]></title><description><![CDATA[ 
 <br><br>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#introduction" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#introduction" target="_blank">Introduction</a>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#index" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#index" target="_blank">Index</a>

<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#questions" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#questions" target="_blank">Questions</a>


<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#index-across-multiple-tables" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#index-across-multiple-tables" target="_blank">Index across Multiple Tables</a>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#space-trade-off" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#space-trade-off" target="_blank">Space Trade-off</a>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#time-trade-off" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#time-trade-off" target="_blank">Time Trade-off</a>1
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#partial-index" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#partial-index" target="_blank">Partial Index</a>

<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#questions-1" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#questions-1" target="_blank">Questions</a>cg怒于n、


<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#vacuum" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#vacuum" target="_blank">Vacuum</a>

<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#questions-2" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#questions-2" target="_blank">Questions</a>


<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#concurrency" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#concurrency" target="_blank">Concurrency</a>

<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#transactions" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#transactions" target="_blank">Transactions</a>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#race-conditions" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#race-conditions" target="_blank">Race Conditions</a>
<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#questions-3" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#questions-3" target="_blank">Questions</a>


<br><a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/#fin" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/#fin" target="_blank">Fin</a>j
<br><br>
<br>This week, we will learn how to optimize our SQL queries, both for time and space. We will also learn how to run queries concurrently.
<br>We will do all of this in the context of a new database — the Internet Movies Database, or IMDb as it is more popularly known. Our SQLite database is compiled from the large online database of movies you may have seen before at&nbsp;<a data-tooltip-position="top" aria-label="https://cs50.harvard.edu/sql/2024/notes/5/imdb.com" rel="noopener nofollow" class="external-link" href="https://cs50.harvard.edu/sql/2024/notes/5/imdb.com" target="_blank">imdb.com</a>.
<br>Take a look at these statistics to get a sense of how big this database is! It has much more data than any of the other databases we have worked with so far.
  <img alt="&quot;Statistics about the IMDb database&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/3.jpg" referrerpolicy="no-referrer"><br>

<br>Here is the ER Diagram detailing the entities and their relationships.
  <img alt="&quot;IMDb ER Diagram — people, movies, and ratings entities&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/4.jpg" referrerpolicy="no-referrer"><br>

<br><br>
<br>Let us open up this database called&nbsp;movies.db&nbsp;in SQLite.
<br>.schema&nbsp;shows us the tables created in this database. To implement the many-to-many relationship between the entities Person and Movie from the ER Diagram, we have a joint table here called&nbsp;stars&nbsp;that references the ID columns of both&nbsp;people&nbsp;and&nbsp;movies&nbsp;as foreign key columns!
<br>To peek into the&nbsp;movies&nbsp;table, we can select from the table and limit results.
SELECT * FROM "movies" LIMIT 5;


<br>To find the information pertaining to the movie Cars, we would run the following query.
SELECT * FROM "movies"
WHERE "title" = 'Cars';


<br>Say we want to find how long it took for this query to run. SQLite has a command&nbsp;.timer on&nbsp;that enables us to time our queries.
<br>On running the above query to find Cars again, we can see three different time measurements displayed along with the results.
<br>“real” time indicates the stopwatch time, or the time between executing the query and obtaining the results. This is the measure of time we will focus on. The time taken to execute this query during lecture was roughly a tenth of a second!


<br>Under the hood, when the query to find Cars was run, we triggered a&nbsp;scan&nbsp;of the table&nbsp;movies&nbsp;— that is, the table&nbsp;movies&nbsp;was scanned top to bottom, one row at a time, to find all the rows with the title Cars.
<br>We can optimize this query to be more efficient than a scan. In the same way that textbooks often have an index, databases tables can have an index as well. An index, in database terminology, is a structure used to speed up the retrieval of rows from a table.
<br>We can use the following command to create an index for the&nbsp;"title"&nbsp;column in the&nbsp;movies&nbsp;table.
CREATE INDEX "title_index" ON "movies" ("title");


<br>After creating this index, we run the query to find the movie titled Cars again. On this run, the time taken is significantly shorter (during lecture,almost eight times faster than the first run)!


<br>In the previous example, once the index was created, we just assumed that SQL would use it to find a movie. However, we can also explicitly see this by using a SQLite command&nbsp;EXPLAIN QUERY PLAN&nbsp;before any query.
<br>To remove the index we just created, run:
DROP INDEX "title_index";


<br>After dropping the index, running&nbsp;EXPLAIN QUERY PLAN&nbsp;again with the&nbsp;SELECT&nbsp;query will demonstrate that the plan would revert to scanning the entire database.


<br><br>
Do databases not have implicit algorithms to optimize searching?
<br>
<br>They do, for some columns. In SQLite and most other database management systems, if we specify that a column is a primary key, an index will automatically be created via which we can search for the primary key. However, for regular columns like&nbsp;"title", there would be no automatic optimization.
<br>
Would it be advisable to create a different index for every column in case we need it?
<br>
<br>While that seems useful, there are trade-offs with space and the time it takes to later insert data into tables with an index. We will see more on this shortly!
<br><br>
<br>We would run the following query to find all the movies Tom Hanks starred in.
SELECT "title" FROM "movies"
WHERE "id" IN (
    SELECT "movie_id" FROM "stars"
    WHERE "person_id" = (
        SELECT "id" FROM "people"
        WHERE "name" = 'Tom Hanks'
    )
);


<br>To understand what kind of index could help speed this query up, we can run&nbsp;EXPLAIN QUERY PLAN&nbsp;ahead of this query again. This shows us that the query requires two scans — of&nbsp;people&nbsp;and&nbsp;stars. The table&nbsp;movies&nbsp;is not scanned because we are searching&nbsp;movies&nbsp;by its ID, for which an index is automatically created by SQLite!
<br>Let us create the two indexes to speed this query up.
CREATE INDEX "person_index" ON "stars" ("person_id");
CREATE INDEX "name_index" ON "people" ("name");


<br>Now, we run&nbsp;EXPLAIN QUERY PLAN&nbsp;with the same nested query. We can observe that

<br>all the scans are now searches using indexes, which is great!
<br>the search on the table&nbsp;people&nbsp;uses something called a&nbsp;COVERING INDEX


<br>A covering index means that all the information needed for the query can be found within the index itself. Instead of two steps:

<br>looking up relevant information in the index,
<br>using the index to then search the table, a covering index means that we do our search in one step (just the first one).


<br>To have our search on the table&nbsp;stars&nbsp;also use a covering index, we can add&nbsp;"movie_id"&nbsp;to the index we created for&nbsp;stars. This will ensure that the information being looked up (movie ID)&nbsp;and&nbsp;the value being searched on (person ID) are both be in the index.
<br>First, let us drop the existing implementation of our index on the&nbsp;stars&nbsp;table.
DROP INDEX "person_index";


<br>Next, we create the new index.
CREATE INDEX "person_index" ON "stars" ("person_id", "movie_id");


<br>Running the following will demonstrate that we now have two covering indexes, which should result in a much faster search!
EXPLAIN QUERY PLAN
SELECT "title" FROM "movies" WHERE "id" IN (
    SELECT "movie_id" FROM "stars" WHERE "person_id" = (
        SELECT "id" FROM "people" WHERE "name" = 'Tom Hanks'
    )
);


<br>Making sure that we have run&nbsp;.timer on&nbsp;let us execute the above query to find all the movies Tom Hanks has starred in, and observe the time it takes to run. The query now runs a&nbsp;lot&nbsp;faster than it did without indexes (in lecture, an order of magnitude faster)!
<br><br>
<br>Indexes seem incredibly helpful, but there are trade-offs associated — they occupy additional space in the database, so while we gain query speed, we do lose space.
<br>An index is stored in a database as a data structure called a B-Tree, or balanced tree. A tree data structure looks something like:
  <img alt="&quot;Tree data structure&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/20.jpg" referrerpolicy="no-referrer">

<br>Notice that the tree has many&nbsp;nodes, each connected to a few others by arrows. The root node, or the node from which the tree originates, has three&nbsp;children. Some nodes at the edge of the tree do not point to any other nodes. These are called&nbsp;leaf&nbsp;nodes.


<br>Let us consider how an index is created for the&nbsp;"title"&nbsp;column of the table&nbsp;movies. If the movie titles were sorted alphabetically, it would be a lot easier to find a particular movie by using&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Binary_search_algorithm" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Binary_search_algorithm" target="_blank">binary search</a>.
<br>In this case, a copy is made of the&nbsp;"titles"&nbsp;column. This copy is sorted and then linked back to the original rows within the&nbsp;movies&nbsp;table by pointing to the movie IDs. This is visualized below.
  <img alt="&quot;Index: Sorted copy of titles pointing to original movie IDs&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/40.jpg" referrerpolicy="no-referrer"><br>

<br>While this helps us visualize the index for this column easily, in reality, the index is not a single column but is broken up into many nodes. This is because if the database has a lot of data, like our IMDb example, storing one column all together in memory might not be feasible.
<br>If we have multiple nodes containing sections of the index, however, we also need nodes to navigate to the right sections. For example, consider the following nodes. The left-hand node directs us to the right section of the index based on whether the movie title comes before Frozen, between Frozen and Soul, or after Soul alphabetically!
  <img alt="&quot;Index nodes broken into sections&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/46.jpg" referrerpolicy="no-referrer"><br>

<br>The above representation is a B-tree! This is how indexes are stored in SQLite.
<br><br>
<br>Similar to the space trade-off we discussed earlier, it also takes longer to insert data into a column and then add it to an index. Each time a value is added to the index, the B-tree needs to be traversed to figure out where the value should be added!
<br><br>
<br>This is an index that includes only a subset of rows from a table, allowing us to save some space that a full index would occupy.
<br>This is especially useful when we know that users query only a subset of rows from the table. In the case of IMDb, it may be that the users are more likely to query a movie that was just released as opposed to a movie that is 15 years old. Let’s try to create a partial index that stores the titles of movies released in 2023.
CREATE INDEX "recents" ON "movies" ("titles")
WHERE "year" = 2023;


<br>We can check that searching for movies released in 2023 uses the new index.
EXPLAIN QUERY PLAN
SELECT "title" FROM "movies"
WHERE "year" = 2023;

  This shows us that the&nbsp;movies&nbsp;table is scanned using the partial index.<br>

<br><br>
Are indexes saved in the schema?
<br>
<br>Yes, in SQLite, they are! We can confirm this by running&nbsp;.schema&nbsp;and we will see the indexes created listed in the database schema.
<br><br>
<br>There are ways to delete unused space in our database. SQLite allows us to “vacuum” data — this cleans up previously deleted data (that is actually not deleted, but just marked as space being available for the next&nbsp;INSERT).
<br>To find the size of&nbsp;movies.db&nbsp;on the terminal, we can use a Unix command
du -b movies.db


<br>In lecture, this command showed us that the size of the database is something like 158 million bytes, or 158 megabytes.
<br>We can now connect to our database and drop an index we previously created.
DROP INDEX "person_index";


<br>Now, if we run the Unix command again, we see that the size of the database has not decreased! To actually clean up the deleted space, we need to vacuum it. We can run the following command in SQLite.
VACUUM;

  This might take a second or two to run. On running the Unix command to check the size of the database again, we can should see a smaller size. Once we drop all the indexes and vacuum again, the database will be considerably smaller than 158 MB (in lecture, around 100 MB).<br>

<br><br>
Is it possible to vacuum faster?
<br>
<br>Each vacuum can take a different amount of time, depending on the amount of space we are trying to vacuum and how easy it is to find the bits and bytes that need to be freed up!
<br>
If a query to delete some rows doesn’t actually delete them, but only marks them as deleted, could we still retrieve these rows?
<br>
<br>People trained in forensics are able to find data we think is deleted but is actually still on our computers. In the case of SQLite, after performing a vacuum, it would not be possible to find deleted rows again.
<br><br>
<br>Thus far, we have seen how to optimize single queries. Now, we will look at how to allow not just one query, but multiple at a time.
<br>Concurrency is the simultaneous handling of multiple queries or interactions by the database. Imagine a database for a website, or a financial service, that gets a lot of traffic at the same time. Concurrency is particularly important in these cases.
<br>Some database transactions can be multi-part. For example, consider a bank’s database. The following is a view of the table&nbsp;accounts&nbsp;that stores account balances.
  <img alt="&quot;Accounts table in a bank's database. Alice sends $10 to Bob.&quot;" src="https://cs50.harvard.edu/sql/2024/notes/5/images/66.jpg" referrerpolicy="no-referrer">

<br>One transaction could be sending money from one account to the other. For example, Alice is trying to send $10 to Bob.
<br>To complete this transaction, we would need to add $10 to Bob’s account and also subtract $10 from Alice’s account. If someone sees the status of the&nbsp;accounts&nbsp;database after the first update to Bob’s account but before the second update to Alice’s account, they could get an incorrect understanding of the total amount of money held by the bank.


<br><br>
<br>To an outside observer, it should seem like the different parts of a transaction happen all at once. In database terminology, a transaction is an individual unit of work — something that cannot be broken down into smaller pieces.
<br>Transactions have some properties, which can be remembered using the acronym ACID:

<br>atomicity: can’t be broken down into smaller pieces,
<br>consistency: should not violate a database constraint,
<br>isolation: if multiple users access a database, their transactions cannot interfere with each other,
<br>durability: in case of any failure within the database, all data changed by transactions will remain.


<br>Let’s open up&nbsp;bank.db&nbsp;in our terminal so we can implement a transaction for transferring money from Alice to Bob!
<br>First, we want to see the data already in the&nbsp;accounts&nbsp;table.
SELECT * FROM "accounts";

  We note here that Bob’s ID is 2 and Alice’s ID is 1, which will be useful for our query.<br>

<br>To move $10 from Alice’s account to Bob’s, we can write the following transaction.
BEGIN TRANSACTION;
UPDATE "accounts" SET "balance" = "balance" + 10 WHERE "id" = 2;
UPDATE "accounts" SET "balance" = "balance" - 10 WHERE "id" = 1;
COMMIT;

  Notice the&nbsp;UPDATE&nbsp;statements are written in between the commands to begin the transaction and to commit it. If we execute the query after writing the&nbsp;UPDATE&nbsp;statements, but without committing, neither of the two&nbsp;UPDATE&nbsp;statements will be run! This helps keep the transaction&nbsp;atomic. By updating our table in this way, we are unable to see the intermediate steps.<br>

<br>If we tried to run the above transaction again — Alice tries to pay Bob another $10 — it should fail to run because Alice’s account balance is at 0. (The&nbsp;"balance"&nbsp;column in&nbsp;accounts&nbsp;has a check constraint to ensure that it has a non-negative value. We can run&nbsp;.schema&nbsp;to check this.)
<br>The way we implement reverting the transaction is using&nbsp;ROLLBACK. Once we begin a transaction and write some SQL statements, if any of them fail, we can end it with a&nbsp;ROLLBACK&nbsp;to revert all values to their pre-transaction state. This helps keep transactions&nbsp;consistent.
BEGIN TRANSACTION;
UPDATE "accounts" SET "balance" = "balance" + 10 WHERE "id" = 2;
UPDATE "accounts" SET "balance" = "balance" - 10 WHERE "id" = 1; -- Invokes constraint error
ROLLBACK;


<br><br>
<br>Transactions can help guard against race conditions.
<br>A race condition occurs when multiple entities simultaneously access and make decisions based on a shared value, potentially causing inconsistencies in the database. Unresolved race conditions can be exploited by hackers to manipulate the database.
<br>In the lecture, an example of a race condition is discussed wherein two users working together can exploit momentary inconsistencies in the database to rob the bank.
<br>However, transactions are processed in&nbsp;isolation&nbsp;to avoid the inconsistencies in the first place. Each transaction dealing with similar data from our database will be processed sequentially. This helps prevent the inconsistencies that an adversarial attack can exploit.
<br>To make transactions sequential, SQLite and other database management systems use&nbsp;locks&nbsp;on databases. A table in a database could be in a few different states:

<br>UNLOCKED: this is the default state when no user is accessing the database,
<br>SHARED: when a transaction is reading data from the database, it obtains shared lock that allows other transactions to read simultaneously from the database,
<br>EXCLUSIVE: if a transaction needs to write or update data, it obtains an exclusive lock on the database that does not allow other transactions to occur at the same time (not even a read)


<br><br>
How do we decide when a transaction can get an exclusive lock? How do we prioritize different kinds of transactions?
<br>
<br>Different algorithms could be used to make these decisions. For example, we could always choose the transaction that came first. If an exclusive transaction is needed, no other transaction can run at the same time, which is a necessary downside to ensure consistency of the table.
<br>
What is the granularity of locking? Do we lock a database, a table or a row of a table?
<br>
<br>This depends on the DBMS. In SQLite, we can actually do this by running an exclusive transaction as below:
BEGIN EXCLUSIVE TRANSACTION;

  If we do not complete this transaction now, and try to connect to the database through a different terminal to read from the table, we will get an error that the database is locked! This, of course, is a very coarse way of locking because it locks the entire database. Because SQLite is coarse in this manner, it has a module for prioritizing transactions and making sure an exclusive lock is obtained only for the shortest necessary duration.<br>

<br><br>
<br>This brings us to the conclusion of Lecture 5 about Optimizing in SQL!
]]></description><link>https://congzhi.wiki/cs50-sql/lecture-5-optimizing-(nc).html</link><guid isPermaLink="false">CS50 SQL/Lecture 5 - Optimizing (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Mon, 07 Jul 2025 11:27:52 GMT</pubDate><enclosure url="https://cs50.harvard.edu/sql/2024/notes/5/images/3.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://cs50.harvard.edu/sql/2024/notes/5/images/3.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Data Structure and Algorithm]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br>DSA 系列上会放一些数据结构（DS）、排序算法（SA）和 Leecode 算法题（L）等。下面是本系列的目录：<br>Congzhi's DSA Series (NC for Not Covered)
Data Structure

<br><a data-href="DS. AVL Tree (An Intro, NC)" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-avl-tree-(an-intro,-nc).html" class="internal-link" target="_self" rel="noopener nofollow">DS. AVL Tree (An Intro, NC)</a>
<br><a data-href="DS. B-Tree" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-b-tree.html" class="internal-link" target="_self" rel="noopener nofollow">DS. B-Tree</a>
<br><a data-href="DS. Binary Tree (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-binary-tree-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">DS. Binary Tree (NC)</a>
<br><a data-href="DS. Bloom Filter (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-bloom-filter-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">DS. Bloom Filter (NC)</a>
<br><a data-href="DS. Red-Black Tree" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-red-black-tree.html" class="internal-link" target="_self" rel="noopener nofollow">DS. Red-Black Tree</a>
<br><a data-href="DS. Hash Table" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-hash-table.html" class="internal-link" target="_self" rel="noopener nofollow">DS. Hash Table</a>
<br><a data-href="DS. Heap (Priority Queue)" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-heap-(priority-queue).html" class="internal-link" target="_self" rel="noopener nofollow">DS. Heap (Priority Queue)</a>
<br><a data-href="DS. Trie (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-trie-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">DS. Trie (NC)</a>


Algorithms, Algorithm Concepts and Sorting Algorithms

<br><a data-href="A. Least Recently Used Algorithm (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/a.-least-recently-used-algorithm-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">A. Least Recently Used Algorithm (NC)</a>
<br><a data-href="AC. Recursion" href="https://congzhi.wiki/data-structure-and-algorithm/ac.-recursion.html" class="internal-link" target="_self" rel="noopener nofollow">AC. Recursion</a>
<br><a data-href="SA. Bubble Sort" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-bubble-sort.html" class="internal-link" target="_self" rel="noopener nofollow">SA. Bubble Sort</a>
<br><a data-href="SA. Insertion Sort" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-insertion-sort.html" class="internal-link" target="_self" rel="noopener nofollow">SA. Insertion Sort</a>
<br><a data-href="SA. Selection Sort (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-selection-sort-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">SA. Selection Sort (NC)</a>
<br><a data-href="SA. Merge Sort (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-merge-sort-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">SA. Merge Sort (NC)</a>
<br><a data-href="SA. Quick Sort (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-quick-sort-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">SA. Quick Sort (NC)</a>
<br><a data-href="SA. Counting Sort (NC)" href="https://congzhi.wiki/data-structure-and-algorithm/sa.-counting-sort-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">SA. Counting Sort (NC)</a>


Leecode Algo Questions

<br><a data-href="L1. Contains Duplicate" href="https://congzhi.wiki/data-structure-and-algorithm/l1.-contains-duplicate.html" class="internal-link" target="_self" rel="noopener nofollow">L1. Contains Duplicate</a>
<br><a data-href="L2. Valid Anagram" href="https://congzhi.wiki/data-structure-and-algorithm/l2.-valid-anagram.html" class="internal-link" target="_self" rel="noopener nofollow">L2. Valid Anagram</a>
<br><a data-href="L3. Two Sum" href="https://congzhi.wiki/data-structure-and-algorithm/l3.-two-sum.html" class="internal-link" target="_self" rel="noopener nofollow">L3. Two Sum</a>
<br><a data-href="L4. Group Anagrams (Star)" href="https://congzhi.wiki/data-structure-and-algorithm/l4.-group-anagrams-(star).html" class="internal-link" target="_self" rel="noopener nofollow">L4. Group Anagrams (Star)</a>
<br><a data-href="L5. Top K Frequent Elements (Star)" href="https://congzhi.wiki/data-structure-and-algorithm/l5.-top-k-frequent-elements-(star).html" class="internal-link" target="_self" rel="noopener nofollow">L5. Top K Frequent Elements (Star)</a>
<br><a data-href="L6. Product of Array Except Self (Star)" href="https://congzhi.wiki/data-structure-and-algorithm/l6.-product-of-array-except-self-(star).html" class="internal-link" target="_self" rel="noopener nofollow">L6. Product of Array Except Self (Star)</a>
<br><a data-href="L7. Valid Sudoku (Medium)" href="https://congzhi.wiki/data-structure-and-algorithm/l7.-valid-sudoku-(medium).html" class="internal-link" target="_self" rel="noopener nofollow">L7. Valid Sudoku (Medium)</a>
<br><a data-href="L8. Encode and Decode Strings (Midium)" href="https://congzhi.wiki/data-structure-and-algorithm/l8.-encode-and-decode-strings-(midium).html" class="internal-link" target="_self" rel="noopener nofollow">L8. Encode and Decode Strings (Midium)</a>
<br><a data-href="Lx. GreapCity" href="https://congzhi.wiki/data-structure-and-algorithm/lx.-greapcity.html" class="internal-link" target="_self" rel="noopener nofollow">Lx. GreapCity</a>


<br><br><br>你可以通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我！]]></description><link>https://congzhi.wiki/data-structure-and-algorithm/data-structure-and-algorithm.html</link><guid isPermaLink="false">Data Structure and Algorithm/Data Structure and Algorithm.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:25:17 GMT</pubDate></item><item><title><![CDATA[DS. Binary Tree (NC)]]></title><description><![CDATA[ 
 <br><br><br>根节点-&gt;左子树-&gt;右子树<br><br>左子树-&gt;根节点-&gt;右子树<br><br>左子树-&gt;右子树-&gt;根节点]]></description><link>https://congzhi.wiki/data-structure-and-algorithm/ds.-binary-tree-(nc).html</link><guid isPermaLink="false">Data Structure and Algorithm/DS. Binary Tree (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 29 Mar 2025 15:52:51 GMT</pubDate></item><item><title><![CDATA[DS. Bloom Filter (NC)]]></title><description><![CDATA[ 
 <br>Do this at first: <a data-href="DS. Hash Table" href="https://congzhi.wiki/data-structure-and-algorithm/ds.-hash-table.html" class="internal-link" target="_self" rel="noopener nofollow">DS. Hash Table</a><br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eCUm4U3WDpM" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=eCUm4U3WDpM" target="_blank">Bloom Filters - Part 1 of 3</a><br>
一个哈希函数<br>三个哈希函数：更小的哈希冲突<br>更多的哈希]]></description><link>https://congzhi.wiki/data-structure-and-algorithm/ds.-bloom-filter-(nc).html</link><guid isPermaLink="false">Data Structure and Algorithm/DS. Bloom Filter (NC).md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 16 Apr 2025 16:04:12 GMT</pubDate></item><item><title><![CDATA[A Tour of Go]]></title><description><![CDATA[ 
 <br><br><br>如果你没有下载 Go ，你可以在<a data-tooltip-position="top" aria-label="https://go.dev/doc/install" rel="noopener nofollow" class="external-link" href="https://go.dev/doc/install" target="_blank">这里</a>查阅官方的下载安装文档。此外，官方还提供 <a data-tooltip-position="top" aria-label="https://go.dev/play/" rel="noopener nofollow" class="external-link" href="https://go.dev/play/" target="_blank">Go Playground</a> ，这是一个可以在线运行和测试 Go 代码的工具，你不需要下载任何软件就可以直接在浏览器中编写、运行你的 Go 程序。<br><br><br>为了快速上手，官方提供 "<a data-tooltip-position="top" aria-label="https://go.dev/tour/list" rel="noopener nofollow" class="external-link" href="https://go.dev/tour/list" target="_blank">A Tour of Go</a>" 的交互式在线教程，分模块地介绍了 Go 的一些核心特性，提供较全面的入门指导。<br>在 Tour 中，你可以直接在 Go Playground 上运行代码实例并练习你所学习到的知识。<br><br>在 Go 中，字符串默认用 UTF-8 表示，UTF-8 的字符字节长度不固定，大小在 1-4 字节。所以 Go 可以直接处理一些非 ASCII 字符，如下：<br>package main

import fmt

func main() {
	fmt.Println("Hello, 世界🌍")
}
<br>这里的 import fmt 表示导入标准库 fmt 包（format 缩写），用于格式化的输出和输入。Println() 的函数（print line 的缩写）用于输出内容并自动换行。<br>To run Go, you just need go run your program, that's it：<br>go run yourProgram.go
<br><br><br><br>Go 是模块化的，所有 Go 程序都是由一个或多个包构成的。单个包就是单个的函数库，承担着不同的功能。在 <a data-tooltip-position="top" aria-label="A Tour of Go > Hello, 世界🌍" data-href="A Tour of Go#Hello, 世界🌍" href="https://congzhi.wiki/let's-go/a-tour-of-go.html#Hello,_世界🌍" class="internal-link" target="_self" rel="noopener nofollow">Hello, 世界🌍</a> 的代码中，我们就使用到了（import fmt）。<br>每个包都规定要有唯一的名称，起到类似 C++ namespace 的功能，Go 语言直接使用包名作为其作用域标识，避免不同包的同名函数发生冲突。在使用某个函数时，Go 规定你只能通过 包名.成员 的方式来访问，正如我的上面使用 fmt.Println() 那样。<br>在 Go 中，规定包名与导入路径的最后一部分相同，比如：<br>package main

import (
	"fmt"
	"math" // Package name is math
	"math/rand" // Package name is rand
)

/* as same as:
import "fmt"
import "math"
import "math/rand"
*/

func main() {
	fmt.Println("Random number:", rand.Intn(100))
	fmt.Println("Square root of 16:", math.Sqrt(16))
}
<br>Go 程序会从 main 包开始执行，在 Go 中，只有包含 package main 的程序才能被编译为可执行程序。其中，main 函数作为程序的入口。<br>其实你可能已经发现，包中的函数名首字母都是大写，这是一种语言层面的 name convention，Go 规定，在一个包中，只有大写字母开头的命名符号才可导出（即在其他包中使用），所有小写字母开头的符号都只能在包内使用，算是 Go 语言中的 public 和 private。<br><br>Go 语言使用后置类型 (postfix type)，参数类型和函数类型声明都适用后置语法。<br>虽然 C++ 的函数也支持一些个后置类型，但是基本都用于自动推导返回类型。<br>auto MyFuntion(auto x, auto y) -&gt; decltype(auto) {}
<br>在 Go 中，后置类型是语言设计风格的一部分。假如我要定义一个函数，在 Go 中，我就要：<br>package main

import "fmt"

func add(x int, y int) int {
	return x + y
}

func main() {
	fmt.Println(add(10, 10))
}
<br>因为 x 和 y 的类型是相同的，在 Go 中，你可以将相同函数参数合并声明来简化代码，如：<br>func add(x, y int) int {
	return x + y
}
<br><br>Go 也支持类似元祖的多值返回，当一个函数有多个返回值时，你可以：<br>func swap(x, y string) (string, string) {
	return y, x
}
<br><br>Go 允许给返回值命名，比如下面的 z 。在函数开始时，z 被视为已声明的变量，最后返回的时候，如果 return 不带参数，那么就会自动返回当前命名返回值的内容。<br>func add(x, y int) (z int) {
	z = x + y
	return // Return z automatically, this is so called "naked return"
}
<br><br>在 Go 中，我们用 var 关键字来声明一个或多个变量。其语法和函数参数类似，变量名在前，类型在后，比如：<br>var i int
var j, k int = 0, 0
var c, python, java bool
<br>var 声明的变量可以出现在包级别（全局变量）或者函数内部（局部变量），如：<br>package main

import "fmt"

var c, python, java bool
var i int

func main() {
    var j, k int = 0, 0
    fmt.Println(i, c, python, java)
}
<br>Go 语言使用 const 关键字来声明常量，如：<br>const i int = 10
<br>因为 Go 的变量支持自动类型推导，所以你可以在变量初始化时省略变量类型，如：<br>var i, name = 10, "congzhi"
<br><br>Go 支持短变量声明，可以在不显式写出类型的情况下同时完成变量的声明和初始化，如：<br>i := 10
// same as var i int = 10
<br>但请注意，Go 规定在包级作用域中，每条语句都要以关键字开头（如 func, var），所以不能使用短变量声明。只有在函数中，你才可以使用短变量声明。<br><br>Go 中，基本数据类型有：<br>bool

string

int  int8  int16  int32  int64
uint uint8 uint16 uint32 uint64 uintptr

byte // alias for uint8

rune // alias for int32
     // represents a Unicode code point

float32 float64

complex64 complex128
<br><br>声明后为初始化的变量会被赋予一个零值。（0, false ""）<br><br>你可以用 T(v) 把 v 值转为类型 T。如：<br>i := 10 // i is int type
f := float64(i)
<br><br>Go 语言只有一种循环结构—— for 循环。和 C/C++ 一样，Go 中的 for 循环也包含三部分：<br>
<br>初始化变量
<br>条件判断语句
<br>迭代更新
<br>这三部分用分号隔开。不过注意，Go 不需要额外的大括号来包裹这三部分。Go 的初始化变量一般会用短变量声明，如下：<br>for i:= 0; i &lt; 10; i++ {}

for i:= 0; i &lt; 10; {
	i++
}
<br>因为 Go 没有 while 关键字，所以我们只能用 for 循环来模拟 while 的行为。比如：<br>/* C
int main() {
	int i = 0;
	while(i &lt; 10){
		i++;
	}
}
*/
package main

func main() {
	i := 0
	for i &lt; 10 {
		i++
	}
}
<br>因为没有 while，想要实现类似 while(1) {} 这样的死循环，你需要：<br>package main

func main() {
	for {
	}
}
<br><br>在 Go 中，我们仍然使用 if 语句来条件分支判断，同样也支持 else if 和 else，与 C/C++ 类似。但 Go 不需要括号来包裹条件表达式。如：<br>package main
import "fmt"

func main() {
	if x &gt; 10 {
		fmt.Println("x is greater than 10")
	} else if x == 10 {
		fmt.Println("x is equal to 10")
	} else {
		fmt.Println("x is less than 10")
	}
}
<br>Go 还允许你在 if 语句中先声明变量然后判断，比方我想看看三条线段是否能够组成一个三角形，我就可以：<br>package main

import "fmt"

func isTriangle(short_edge1, short_edge2, long_edge int) bool {
    if length := short_edge1 + short_edge2; length &lt; long_edge {
        return false
    }
    return true
}

func main() {
    result := "No"
    if isTriangle(2, 3, 4) {
        result = "Yes"
    }
    fmt.Println(result)
}
<br><br>在 Go 中，switch 语句的使用很灵活，你可以使用常量或枚举值：<br>package main

import (
	"fmt"
	"runtime"
)

func main() {
	switch os := runtime.GOOS; os {
	case "darwin":
		fmt.Println("macOS)
	case "linux"
		fmt.Println("Linux)
	default:
		fmt.Println("%s\n", os)
	}
}
<br>此外，在 Go 中，你还可以使用函数的返回值作为 switch 语句的判断值。同样，case 条件也不一定是一个常量表达式，也可以是一个动态的表达式或函数调用。<br>package main

import "fmt"
import "math/rand"

func getValue() int {
    return 2
}

func main() {
    switch randNum := rand.Intn(3) + 1; randNum {
    case 1:
        fmt.Println("Value is 1")
    case getValue():
        fmt.Println("Value is 2")
    default:
        fmt.Println("Value is unknown")
    }
}
<br>在 Go 中，switch 还可以不带任何条件，相当于 switch true，如：<br>package main

import "fmt"

func main() {
    x := 10
    switch {
    case x &lt; 5:
        fmt.Println("x is less than 5")
    case x &gt;= 5 &amp;&amp; x &lt; 15:
        fmt.Println("x is between 5 and 15")
    default:
        fmt.Println("x is greater than 15")
    }
}
<br><br>在 Go 中，我们可以使用 defer 关键字延迟执行函数。可以用于资源的清理，比如：<br>package main

import "fmt"

func main() {
	defer fmt.Println("exit")
	fmt.Println("start execution")
}
<br>如果有多个 defer，这些 defer 语句会逆序执行（类似栈），也称为 stacking defer。比如：<br>package main

import "fmt"

func main() {
	fmt.Println("Counting start:\n")
	for i:= 0; i &lt; 10; i++ {
		defer fmt.Println(i)
	}
}
<br><br>Go 语言支持指针，但不同与 C/C++ 给你最大的自由，Go 只允许你使用指针取地址、解引用，但不允许你使用指针算数，如：<br>package main

import "fmt"

func main() {
	var myInt int = 10 // myInt := 10
	var varptrToInt *int = &amp;myInt // ptrToInt := &amp;myInt
	*ptrToInt = 20
	fmt.Println(myInt) // 20
	/*
	prtToInt++ // Not allowed
	*/
}
<br>有了指针的帮助，我们就可以在一个函数中修改调用方传入的变量值本身，而不仅仅是拷贝它的副本。这样就能避免不必要的内存复制。如：<br>package main

import "fmt"

func IncrementByValue(n int) {
	n += 1 // 仅修改副本
}

func IncrementByPointer(n *int) {
    *n += 1 // 根据指针修改值
}

func main() {
    value := 10
    IncrementByValue(value)
	fmt.Println(value) // 打印10，因为传递的是值的副本
	IncrementByPointer(&amp;value)
    fmt.Println(value) // 打印11，因为传递的是值的地址
}
<br>需要注意的是，在使用指针函数时，传入的参数必须是指针类型的（也就是变量地址）。<br>package main

func IncrementByPointer(n *int) { // 传入指针
    *n += 1
}

func main() {
    var value int = 10 // 值类型的 value
	IncrementByPointer(value) // error!
	IncrementByPointer(&amp;value) // ok
}
<br><br>Go 的结构体（struct）和 C 一样，都用来将许多字段整合成一个结构。在 Go 中，你需要用 type 关键字来定义新的命名类型，如：<br>package main

import "fmt"

type Vertex struct {
	X int
	Y int
}

func main() {
	var v: Vertex
	v.X = 1
	v.Y = 2
	fmt.Println(v)
	fmt.Println(Vertex{1, 2})
}
<br>Go 的字段访问也是通过点号（.）来实现的，和 C/C++ 一样。但不同的是，在 Go 中，即使你有一个指向结构体的指针，你仍然可以直接通过 . 来访问字段，而不需要 -&gt; 运算符，如：<br>type Vertex Struct {
	X int
	Y int
}

func main() {
	v.X = 0
	v.Y = 0
	v := Vertex{1, 2} // A struct literals
	v.X = 10
	fmt.Println(v)

	p := &amp;v
	p.Y = 20
	fmt.Println(v)

	(*p).Y = 10 // You could use this, not recommanded
	fmt.Println(v)
	
}
<br>在初始化时，你可以只给部分字段赋值，如：<br>v1 = Vertex{} // X = 0 &amp; Y = 0
v2 = Vertex{X: 100, Y: 100}
v3 = Vertex{Y: 100} // X = 0 &amp; Y = 100
<br><br>因为 Go 是后置类型的语言，所以你得用 [n]T 来定义一个数组，其中 n 表示数组的长度， T 表示数组元素的类型。在 Go 中，数组的长度也是其类型的一部分，即 [5]int 和 [6]int 不是一种类型，所以在 Go 中，数组一旦定义，就不能修改其长度。<br>Go 中数组的使用和 C++ 非常类似：<br>package main

import "fmt"

func main() {
	var intArrayOfFive [5]int
	intArrayOfFive[0] = 0
	for i:= 1; i &lt; 5; i++ {
		intArrayOfFive[i] = i
	}
	var intArrayOfSix [6]int = [6]int {0, 1, 2, 3, 4, 5}
	fmt.Println(intArrayOfFive)
	fmt.Println(intArrayOfSix)
}
<br><br>数组的长度固定，是不可变长的，而切片是动态可变长的。虽然说切片说可变长的，但不同与 C++ 中的 std::vector（本身就是一个数组），切片本身并不是数组，而只是对 Go 底层数组的视图 (view)（帮助描述底层数组），即部分引用，这点却更接近于 C++ 的 std::span。既然是对底层数组的引用，所以多个 slices 可以共享同一个底层数组。<br>我们可以用 []T 来定义一个 T 元素的切片。因为切片是对底层数组的（部分）引用，所以在创建切片时，还需要提供两个 indices，用冒号分隔开，如：underlyingArray[low : high]，表示从数组 underlyingArray 中选取索引 [low...high) 的半开区间的所有元素。这使得切片长度永远等于 high - low 便于推到并防止越界。<br>比如：<br>package main

import "fmt"

func main() {
	var prime [10]int = [10]int{2, 3, 5, 7, 11, 13, 17, 19, 23, 29}
	var primeSlice []int = prime[1 : 4]
	fmt.Println(primeSlice)
}
<br>输出：<br>[3 5 7]
<br>对于切片的修改会反馈到原始数组上（因为是引用）。如果一个数组的多个切片引用数组的同一部分，那么某一个切片的修改也会导致其他切片的联动修改。因为本质上，Go 的切片是一个包含指针的三元结构：指向数组被切片位置的指针 (ptr)、切片中元素个数 (len) 和底层数组的容量 (cap)。<br>Go 提供一些内建函数来查看这些属性：<br>package main

import "fmt"

func main() {
	var prime [10]int = [10]int{2, 3, 5, 7, 11, 13, 17, 19, 23, 29}
	var primeSlice []int = prime[1 : 4]
	fmt.Println(primeSlice)
	fmt.Println("First pointer to slice:", &amp;primeSlice[0])
	fmt.Println("Length of slice:", len(primeSlice))
	fmt.Println("Capacity of slice:", cap(primeSlice))
}
<br>cap 是从切片的起始位置 primeSlice[0] 到底层数组末尾的元素个数，即：<br>cap = len(prime) - 1
<br><br>在切片数组时，你需要提供上界 (high) 和下界 (low)，但是你可以不提供上下界而使用默认的界限。在 Go 中如果你有一个数组：<br>var a [10]int
<br>这些切片都是一样的，都表示对上面整个数组的引用：<br>var s1 []int = a[0 : 10]
var s2 []int = a[  : 10]
var s3 []int = a[0 :  ]
var s4 []int = a[ : ]
<br>但如果不是创建切片，缺省上界或下界还有其他的用途——扩充或缩减已有的切片，如：<br>package main

import "fmt"

func main() {
	a := [10]int {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
	s := a[ : ]
	fmt.Println(s)
	
	// Truncating its length to 0
	s = s[ : 0]
	fmt.Println(s)

	// Extending its length to 4
	s = s[ : 4]
	fmt.Println(s)

	// Droping its first two elements
	s = s[2 : ]
	fmt.Println(s)
}
<br><br>如果你嫌创建切片前还要手动创建数组麻烦，那你可以使用切片字面量，在语法上，它就像没有长度的数组字面量：<br>// Array literals
var myArray [3]int = [3]int {1, 2, 3}
myArray := [3]int {1, 2, 3} // Short variable declaration
// Slice literals
var mySlice []int = []int {1, 2, 3}
mySlice []int = []int {1, 2, 3}
<br>如此，Go 就会自动创建一个匿名数组，供切片引用使用。<br><br>对于切片来说，零值就是 nil。我们前面提到，切片是一个三元结构，所以零值 nil 就用来表示切片的长度、容量都为零而且没有底层数组的情况。<br>package main

import "fmt"

func main() {
	var mySlice []int
	if mySlice == nil {
		fmt.Println("mySlice is nil")
	} else {
		fmt.Println("mySlice is not nil")
	}
}
<br><br>那么，切片如何实现底层数组的动态扩容？数组本身并不能扩容，所以在许多语言中，动态扩容就是申请一片更大的内存空间并删除原数组，我们提到过，Go 的切片只是对底层数组的引用，并不拥有资源，所以 Go 的切片通过一种不同的方式来扩容底层数组。<br>假如我们使用切片字面量创建了一个切片：<br>package main

import "fmt"

func main() {
	// Anonymous array 1
	mySlice := []int {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
	fmt.Println(mySlice)
	// Anonymous array 2
	mySlice = append(mySlice, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)
	fmt.Println(mySlice)
}
<br>这里，Go 首先创建了一个匿名数组并让 mySlice 持有这个匿名数组的引用，随后，我们使用内建函数 append() 追加元素、扩充数组。如果底层数组容量不足，那么 Go 会分配一块更大的内存空间，并将原数组的内容复制过去，创建一个新的底层数组。此时，append 会返回一个新的切片（因为 cap 改变了）。由于先前的匿名数组已经不被任何切片引用（被标记为不可达），所以 Go 的垃圾回收器 (GC) 会在合适的时机将其自动释放掉。<br><br>虽然你完全可以用 append 自动扩容底层数组，但 Go 推荐你使用另一个专门用于初始化引用类型的内建函数—— make() 来创建底层数组并返回对数组引用的切片。原因是，make 函数可以控制底层数组的初始容量。你可以通过 make([]T, 0, N) 来创建制定容量但初始长度为 0 的切片。如果缺省第三个参数，那么容量就会等于初始长度。<br><br>Go 提供 range 关键字用于支持更简洁的遍历语法（ranged-for loop），无需你提供显式地设置循环条件，即可自动地遍历数组、切片、字符串等集合类型。每次遍历都会有两个返回值——当前元素的索引和值（该索引元素的拷贝）。如：<br>package main

import "fmt"

func main() {
	myArray := [5]int {1, 2, 3, 4, 5}
	for i, v := range myArray {
		fmt.Printf("The value of index %d of myArray is %d.\n", i, v)
	}
	mySlice := myArray[ : ]
	for i, v := range mySlice {
		fmt.Printf("The value of index %d of mySlice is %d.\n", i, v)
	}
}
<br>如果你只需要使用一个返回值，那么你可以用一个占位符来缺省掉你不需要的那个元素，如：<br>package main

import "fmt"

func main() {
	myArray := [5]int {1, 2, 3, 4, 5}
	for i, v := range myArray {
		fmt.Printf("The value of index %d of myArray is %d.\n", i, v)
	}
	for i, _ := range myArray { // or just `for i := range myArray`
		fmt.Printf("Current index is %d.\n", i)
	}
	for _, v := range myArray {
		fmt.Printf("Current value is %d.\n", v)
	}
	for range myArray { // you could also do `for _ = range myArray`
		fmt.Printf("Hello.\n")
	}
}
<br><br>在 Go 中，map 是一种存储键值对 (Key-Value) 映射的数据结构，用于通过一个键来快速查找、插入或删除对应的值。Go 中的映射表是哈希表 (hash table) 实现的，所以访问速度非常快，时间复杂度为 O(1)。<br>在 Go 中，map 是引用类型，其零值是 nil，即没有键和值。如果一个 map 没有初始化，那么 Go 就不会分配底层的哈希表结构，这时的 map 可读不可写。<br>package main

import "fmt"

func main() {
	var myMap map[string]int
	fmt.Println(myMap) // Ok
	// map["apple"] = 5 // Runtime panic
}
<br>你可以通过 map[K]V {} 的 map literal 来创建并初始化 map，如：<br>package main

import "fmt"

func main() {
	var myMap = map[string]int {
		"apple":  5,
		"banana": 3,
		"cherry": 7,
	}
	myMap["durian"] = 9
	for key, value := range myMap {
		fmt.Printf("Key: %s, Value: %d\n", key, value)
	}
}
<br>为了安全地创建并初始化底层哈希表，推荐的方法是使用 make 初始化 map。因为 map 是一种引用类型，其底层哈希表在使用前必须被初始化，否则就可能导致运行时错误。而 make 可以分配必要的内存并初始化一个空的哈希表（和 slice 使用 make 初始化一样）。比如：<br>// var m map[string]int
// m["apple"] = 5 // Runtime panic

m := make(map[string]int) // Ok
m["apple"] = 5              // Ok
<br><br>上面，如何插入或更新元素我们已经学习过了，下面我们来接着看看如何删除键值对还有如何检测 key 是否存在：<br>package main

import "fmt"

func main() {
	myMap := make(map[string]int)
	myMap["apple"] = 5
	myMap["banana"] = 3
	for key, value := range myMap {
		fmt.Printf("Key: %s, Value: %d\n", key, value)
	}
	delete(myMap, "apple")
	_, ok := myMap["apple"]
	if !ok{
		fmt.Println("apple is not avaliable")
	}
}
<br>Go 中，你可以用 delete() 删除指定 key 的键值对，即使 key 不存在也不会报错。此外，Go 允许你用双赋值的方式来取值（elem, ok = myMap[key]）。如果 key 存在，那么就会返回对应的 value 和为 true 的 ok。否则返回两个零值。<br><br>函数本质上就是指向可执行代码段段指针（引用）。因为 Go 中并没有函数指针的概念，所以 Go 函数是引用类型的值。在 Go 中，函数是一等公民，因此函数可以像普通变量一样被传递、赋值或作为返回值。比如：<br>package main

import "fmt"

func main() {
	myFunc := func(){
		fmt.Println("Hello, World!")
	}
	yourFunc := myFunc
	myFunc()
	yourFunc()
}
<br>这里，我们将一个 lambda 匿名函数（Go 中称为 function literal）赋给 myFunc 变量，其本质上就是将一个函数值像任何普通变量一样赋值给 yourFunc，并多次调用。<br><br>闭包是一个”绑定“了其定义环境中变量的函数。因为 Go 中的函数是一等公民，你可以通过嵌套函数 (nested function) 来实现闭包。如：<br>package main

import "fmt"

func counter() func() int {
	count := 0
	return func() int {
		count += 1
		return count
	}
}

func main() {
	cnt := counter()
	for i := 0; i &lt; 10; i++ {
		fmt.Println(
			cnt(),
		)
	}
}
<br><br><br>因为 Go 中没有类，所以它不算是一个典型的面向对象的语言。但是 Go 提供一些机制实现面向对象的核心原则——封装、继承和多态。Go 通过结构体和方法来抽象行为，Go 通过结构体和方法来对数据和行为进行封装。此外，Go 还使用基于标识符首字母大小写来实现包范围的可见性。尽管并不支持 is-a 的类继承机制、但 Go 可以通过范型和 interface 支持多态。<br><br>在 Go 中，方法就是函数，不过它带有一个特殊的接收处理的参数，Go 将方法处理的参数称为接收者 (receiver)，接受者类型一般情况下都是用户自定义的结构体类型。在调用方法时，其行为和其他 OOP 语言别无二致。<br>例如：<br>package main

import "fmt"

type Vertex struct {
	X, Y float64
}

func (v Vertex) Add(v1 Vertex) Vertex {
	v.X += v1.X
	v.Y += v1.Y
	return v
}
func (v Vertex) Print () {
	fmt.Println(v.X, v.Y)
}
func main() {
	v1 := Vertex{3, 4}
	v2 := Vertex{5, 6}
	v1 = v1.Add(v2)
	v1.Print() // Output: 8 10
}
<br>这里，我们定义了一个结构体 Vertex，然后还定义了两个方法。和过去我们学习的函数不同，方法的函数名前面总是有一个“前置的参数”，也就是我们说的接收者。它的作用就是让某个函数绑定到特定的类型（类似于面向对象的方法）。同时，这个接受者也作为显式函数参数传入方法。这种语法让这些方法看上去好像就是 Vertex 自带的成员函数。<br>Go 方法的本质就是绑定在某个类型上的函数，最主要的区别就在于调用的方式。简单把之前的方法改写成功能完全相同的函数就如下所示：<br>func Add(v1 Vertex, v2 Vertex) Vertex {
	v1.X += v2.X
	v1.Y += v2.Y
	return v1
}
func Print (v Vertex) {
	fmt.Println(v.X, v.Y)
}
func main() {
	v1 := Vertex{3, 4}
	v2 := Vertex{5, 6}
	v1 = Add(v1, v2)
	Print(v1)
}
<br>在使用方法时，你需要注意接收者类型不可以是内建类型（如 int, float64 等），但你可以绕个弯子，通过创建别名的方式来定义内建类型的方法。<br><br>接受者就是特殊的函数参数，既然函数参数可以是值或者指针，那么接受者当然也可以是值接受者或指针接受者。前面的例子中，我们使用值接受者，它的写法类似于 (变量名 类型)，使用值接收者调用方法时，方法的栈帧会复制一份对象内容。如果使用如 (变量名* 类型) 的指针接收者，那么栈帧就只会复制一个对象指针，而且通过指针，你可以修改原对象。<br>package main

import "fmt"

type Vertex struct {
	X, Y float64
}
// 值接收者
func (v Vertex) Add(v1 Vertex) Vertex {
	v.X += v1.X
	v.Y += v1.Y
	return v
}

// 指针接收者
func (v *Vertex) AddInPlace(v1 Vertex) {
	v.X += v1.X
	v.Y += v1.Y
	// 不需要 return 了
}
func (v Vertex) Print () {
	fmt.Println(v.X, v.Y)
}

func main() {
	v1 := Vertex{3, 4}
	v2 := Vertex{5, 6}

	v1.AddInPlace(v2)
	v1.Print() // Output: 8 10
}
<br>Go 并不支持像 C++/Java 那样的函数重载/静态多态，所以同一个类型不能同时声明：<br>func (v Vertex) Add(v1 Vertex) Vertex {...}

func (v *Vertex) Add(v1 Vertex) Vertex {...}
<br>即便一个是值接收者，另一个是指针接受者，Go 仍然会认为两个方法命名冲突，进而编译报错。<br>在函数中，当形参是指针参数时，实参也必须是指针类型的。而方法则不太一样，当接收者是指针时，调用方法的对象可以是指针，也可以是值类型。编译器会帮你隐式的取地址，比如：<br>package main

type Vertex struct {
	X, Y float64
}

// 指针接收者
func (v *Vertex) AddInPlace(v1 Vertex) {
	v.X += v1.X
	v.Y += v1.Y
}

func main() {
	var adder_v Vertex = Vertex{1, 2}
	var adder_p *Vertex = &amp;Vertex{3, 4}
	var addee Vertex = Vertex{5, 6}

	adder_v.AddInPlace(addee) // 相当于 (&amp;adder_v).AddInPlace(addee)
	adder_p.AddInPlace(addee) 
}
<br>同样的，如果方法的接收者是值，调用方法时也是可以用值或指针，编译器会帮你解引用。<br><br><br><br>&lt;待完善&gt;<br><br>]]></description><link>https://congzhi.wiki/let's-go/a-tour-of-go.html</link><guid isPermaLink="false">Let's GO/A Tour of Go.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 12 Aug 2025 14:57:21 GMT</pubDate></item><item><title><![CDATA[GPM Model of Go]]></title><description><![CDATA[ 
 <br>Please "Go" read <a data-tooltip-position="top" aria-label="8. Threads and Concurrency" data-href="8. Threads and Concurrency" href="https://congzhi.wiki/congzhi's-os-series/8.-threads-and-concurrency.html" class="internal-link" target="_self" rel="noopener nofollow">this</a> at first, and <a data-tooltip-position="top" aria-label="https://dev.to/aceld/understanding-the-golang-goroutine-scheduler-gpm-model-4l1g" rel="noopener nofollow" class="external-link" href="https://dev.to/aceld/understanding-the-golang-goroutine-scheduler-gpm-model-4l1g" target="_blank">this</a>.<br><br><br>并发是一系列独立执行计算的集合 ，并发通过对 CPU 时间片 (time slice/quantum) 的划分来实现多个进程共享计算资源。无论你的机器时单核心的还是多核心的，对并发的支持会让进程在宏观上有一种自己独占所有计算资源的错觉，结合操作系统对线程的调度策略从而实现对 CPU 的虚拟化。<br>在许多系统级语言中，并发通常依赖开发者手动引入线程并发库来实现。而在 Go 中，并发作为语言特性提供给用户，既然有现成的线程库可以用，为什么 Go 还要设计引入自己的并发模型呢？<br>目前，大多数的线程库都采用 1:1 的线程模型，也就是说，用户使用线程库在用户空间创建的线程会直接映射创建一个内核线程。而这种方式实际上在用户空间并不维护用户线程（也称为协程 co-routine）的数据结构，这就导致了并发完全通过内核线程来实现。看上去不错，但内核线程是很“重”的，操作系统对内核线程的调度切换和上下文切换会花费大量系统资源。这也就是协程诞生的背景。<br>协程运行在用户空间（内核无感知），而且协程的栈通常远小于线程栈，这就使得协程的切换开销非常小：即不需要用户态到内核态到转变；又由于协程栈很小，使用协程实现的内存占用也会小很多；再加上协程又用户态的调度器管理，其调度协程的逻辑也“轻”许多。这样，CPU 就可以在用户内核空间转换和线程调度上浪费更少的时间，即 CPU 的利用率提高了。<br>然而，由于操作系统对硬件资源的封装，CPU 只能“看见”操作系统中的线程（即内核线程），而无法识别协程的存在——对 CPU 来说，协程只是一段指令序列。想要协程真正运行在 CPU 上，就必须将其绑定在线程（即内核线程）上。在这之中，我们可以选择两种不同的线程模型：<br>
<br>N:1 线程模型：这种模型要求一个进程内只能有一个线程（即主线程），之后将 N 个协程绑定在这一个线程上。然而，只有一个线程的话就无法发挥潜在的并发特性了，一时间，最多一个线程在 CPU 上运行，而由于 N:1 的特性，每次也只能有一个协程在 CPU 上运行。
<br>M:N 线程模型：结合了 1:1 和 N:1 线程模型的特点，允许将 M 个协程动态地绑定到 N 个创建的线程上，如此，一时间就可以有 CPU 核心个线程在 CPU 上运行了，每次也可以有 CPU 核心个协程在 CPU 上运行。
<br>在 Go 中，所采用的线程模型就是 M:N 线程模型。为了提供用户友好的并发接口，Go 抽象封装出 Goroutines 和 Channels。<br>Goroutines 就是 Go 中的协程，它们由 Go 运行时 (Go runtime) 调度，并映射到底层线程上运行。每个 Goroutine 启动时只占用几 KB 的栈空间，其栈大小可按需动态扩展。这种设计使得我们可以在内存资源有限的情况下同时运行成千上万个 Goroutine，从而轻松应对高并发场景。<br>而 Channels 是 Go 提供的并发通信原语，用于 Goroutine 之间的传递数据，同时同步 Goroutines 之间的行为。<br><br>最初，Go 使用的调度器逻辑非常简单，调度由一个全局 Goroutine 队列和一个简单的调度循环组成。由于这种调度器的性能太差，可能导致 Goroutine 堆积，形成瓶颈，后来 Go 逐步引入 GPM 三元组模型。我们先来看看原先已弃用的调度模型是怎么样的。<br>我们用 G 来表示 Goroutine，用 M 来表示线程。早期 Go 调度器如下图所示，其顶层设计仍然是 M:N 的线程模型。所有的 M 个 Goroutines 都在一个全局 FIFO 的 Goroutine 队列中，因为所有的线程都盯着同一个 Goroutine 队列，为了保证对 Goroutine 队列的访问不会引起数据竞争，这个队列由一个互斥锁进行保护。<br><img alt="deprecated go scheduler.png" src="https://congzhi.wiki/let's-go/pics/deprecated-go-scheduler.png"><br>这里调度器的结构看起来很简明，但一旦 Goroutines 数量急剧增多，就可能带来一系列的问题：<br>
<br>全局锁竞争严重：创建、调度和销毁 Goroutine 都要求获取全局的互斥锁，就会导致高并发场景下的频繁锁冲突。
<br>无处理器亲和性：由于 Goroutine 随机调度，比如线程 M0 创建的 Goroutine 可能被 M2 所绑定，这就可能造成 poor CPU locality。
<br>这样，一旦 Goroutine 数量极多，这样的全局调度就会变成性能瓶颈。<br><br>为解决之前调度器所存在的问题，Go 引入了新的调度器模型——GPM scheduler。在这个调度器中，除了之前的 G(Goroutines) 和 M(Threads) 外，新增了一个 P(Processer)。这个新引入的 P 就主要用来解决之前可怜的处理器亲和性问题（一般而言，操作系统对线程的处理器亲和性都很不错，所以这种方法是行之有效的）。]]></description><link>https://congzhi.wiki/let's-go/gpm-model-of-go.html</link><guid isPermaLink="false">Let's GO/GPM Model of Go.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 31 Jul 2025 00:25:46 GMT</pubDate><enclosure url="https://congzhi.wiki/let's-go/pics/deprecated-go-scheduler.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/let's-go/pics/deprecated-go-scheduler.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Let's GO]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">🔙Go Back Home</a><br><br><br>Go 是一个现代化的编程语言，以恰到好处的抽象层次和低学习成本而闻名，Go 只有 25 个关键字，与 C++ 相比， Go 少了近 100 个左右的关键字）。Go 活像编程语言中的 Toki Pona，精简而直接。<br>在 Go 中，能用 if 和 for 实现的逻辑，就绝不额外引入其他的语法和关键字。在 Go 里，"Less is more" 并不单单是一种口号，而是一种实践的哲学——“简单，并不代表功能少，而是恰如其分。”<br>Let's Go

<br><a data-href="A Tour of Go" href="https://congzhi.wiki/let's-go/a-tour-of-go.html" class="internal-link" target="_self" rel="noopener nofollow">A Tour of Go</a>
<br><a data-href="GPM Model of Go" href="https://congzhi.wiki/let's-go/gpm-model-of-go.html" class="internal-link" target="_self" rel="noopener nofollow">GPM Model of Go</a>

<br><br><br>任何疑问都欢迎通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/let's-go/let's-go.html</link><guid isPermaLink="false">Let's GO/Let's GO.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 12 Aug 2025 10:43:29 GMT</pubDate></item><item><title><![CDATA[Networks]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br>这个系列用于记录计算机网络协议和一些我感兴趣的一些网络知识。内容如下：<br>Networks

<br><a data-href="CDN - Bringing It Home" href="https://congzhi.wiki/networks/cdn-bringing-it-home.html" class="internal-link" target="_self" rel="noopener nofollow">CDN - Bringing It Home</a>
<br><a data-href="Endianness" href="https://congzhi.wiki/networks/endianness.html" class="internal-link" target="_self" rel="noopener nofollow">Endianness</a>
<br><a data-href="Hyper-Text Transfer Protocol" href="https://congzhi.wiki/networks/hyper-text-transfer-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">Hyper-Text Transfer Protocol</a>
<br><a data-href="ICMP and Ping (NC)" href="https://congzhi.wiki/networks/icmp-and-ping-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">ICMP and Ping (NC)</a>
<br><a data-href="Internet Protocol (NC)" href="https://congzhi.wiki/networks/internet-protocol-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol (NC)</a>
<br><a data-href="Network Attack - DoS Attack" href="https://congzhi.wiki/networks/network-attack-dos-attack.html" class="internal-link" target="_self" rel="noopener nofollow">Network Attack - DoS Attack</a>
<br><a data-href="Network Attack - LAND Attack" href="https://congzhi.wiki/networks/network-attack-land-attack.html" class="internal-link" target="_self" rel="noopener nofollow">Network Attack - LAND Attack</a>
<br><a data-href="Network Attack - Ping of Death (NC)" href="https://congzhi.wiki/networks/network-attack-ping-of-death-(nc).html" class="internal-link" target="_self" rel="noopener nofollow">Network Attack - Ping of Death (NC)</a>
<br><a data-href="Networking Delay" href="https://congzhi.wiki/networks/networking-delay.html" class="internal-link" target="_self" rel="noopener nofollow">Networking Delay</a>
<br><a data-href="Socket API and Transport Layer Protocol" href="https://congzhi.wiki/networks/socket-api-and-transport-layer-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">Socket API and Transport Layer Protocol</a>

<br><br><br>任何疑问都欢迎通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/networks/networks.html</link><guid isPermaLink="false">Networks/Networks.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:25:53 GMT</pubDate></item><item><title><![CDATA[Normal Thread Pool]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/numa-aware-thread-pool/normal-thread-pool.html</link><guid isPermaLink="false">NUMA-Aware Thread Pool/Normal Thread Pool.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 10 Aug 2025 18:54:10 GMT</pubDate></item><item><title><![CDATA[NUMA Wrapper]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/numa-aware-thread-pool/numa-wrapper.html</link><guid isPermaLink="false">NUMA-Aware Thread Pool/NUMA Wrapper.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 10 Aug 2025 18:54:29 GMT</pubDate></item><item><title><![CDATA[NUMA-Aware Thread Pool]]></title><description><![CDATA[ 
 <br>About NUMA: <a data-href="NUMA Systems" href="https://congzhi.wiki/computer-architecture/numa-systems.html" class="internal-link" target="_self" rel="noopener nofollow">NUMA Systems</a>.]]></description><link>https://congzhi.wiki/numa-aware-thread-pool/numa-aware-thread-pool.html</link><guid isPermaLink="false">NUMA-Aware Thread Pool/NUMA-Aware Thread Pool.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 09 Aug 2025 12:50:39 GMT</pubDate></item><item><title><![CDATA[pthread Wrapper]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/numa-aware-thread-pool/pthread-wrapper.html</link><guid isPermaLink="false">NUMA-Aware Thread Pool/pthread Wrapper.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sun, 10 Aug 2025 18:54:49 GMT</pubDate></item><item><title><![CDATA[存储器扩展技术]]></title><description><![CDATA[ 
 <br><br>在数字电路中，二进制译码器可以把  位二进制输入转换成  个输出。译码器的应用非常广泛，地址解码、数据选择等都是常用的译码器应用。<br>为了更清楚地理解 CPU 的  根地址线是如何产生多达  个连续的地址空间的，我们先用简单的3-8译码器来举例。74LS138是最为常见的3线-8线译码器，我们不去关注它的选通使能端，着重观察它的逻辑真值表。在如下表的逻辑真值表中，我们看到，虽然我们的输入位只有 3 位，但是却解码出了 8 种输出状态。即 3 根输入的”地址线“产生了  个连续的”地址空间“。<br><br>在74LS138译码器中，当输出端被选中后会变为低电平（0），其余输出端口保持高电平（1），在设计译码电路时，我们可以人为规定选中后呈现高电平或是低电平。现在，我们就应该理解为什么  根地址线，能过产生  个连续的地址空间了。当寻址方式是按字节寻址时，译码电路的一个输出端就能够对应 1byte 的数据。<br>当地址线条数有 12 根，且按照字节寻址，那么 CPU 直接寻址范围就等于:<br><br>
<br>记忆单元（存储基元/ 存储元/ 位元）(Cell)

<br>具有两种稳态的能够表示二进制数码0和1的物理器件 


<br>存储单元/ 编址单位（Addressing Unit）

<br>具有相同地址的位构成一个存储单元，也称为一个编址单位 


<br>存储体/ 存储矩阵/ 存储阵列（Bank） 

<br>所有存储单元构成一个存储阵列 


<br>编址方式（Addressing Mode）

<br>字节编址、按字编址 


<br><br>位平面(Bit plane) 是存储体的重要组成部分。而每个内存芯片(DRAM chip)中可以包含多个存储体。明晰了它们之间的关系，我们来看下面这张图，假如每个位平面都包含 64*64 的位元，也就是说每个位平面的寻址范围都是 0-4096，每一个可寻址位都包含 1bit 信息。这样，8 个位平面摞在一起就可以每次寻址 8bits（也就是1byte）的数据。<br><img alt="Pasted image 20240812173423.png" src="https://congzhi.wiki/some-notes/pics/pasted-image-20240812173423.png"><br><br><br>在上节中我们看到，寻址空间的范围是会随地址线条数的增加翻倍增长的。地址线每增加一根，可寻址空间增大一倍。这样会导致一个很直接的问题，那就是如果我们只采用一个译码器，译码器每增加一个输入端就会使得输出端翻倍！12 根地址线译码后可是能产生 4096 个输出端口。这样设计的译码电路会使得整个芯片奇长无比，但显然设计人员考虑到了这个问题。因而，我们现在的内存多采用的是二维地址译码方式。<br><br>我们将地址线划分成为不同行(Row)不同列(Column)，当某寻址单元行选中同时列选中时也可以选中这一单元。这样可以避免译码器过于庞大。而且现代计算机多采用地址线分时复用，也就是用 6 根地址线分时复用，由 RAS(Row Address Select) 和 CAS(Column Address Select) 控制分时的时序。这样，原先 12 根地址线就可以减少到 7 根（6根复用线，1根 RAS/CAS）。<br><img alt="Pasted image 20240812172331.png" src="https://congzhi.wiki/some-notes/pics/pasted-image-20240812172331.png"><br>
看上去好像分时复用要花上比原先多一倍的时间去访存。但实际上分时复用其实并不会浪费多少时间，因为行译码后会将这一整行的数据全部放到行缓冲中，这是用 SRAM 做的，速度要比主存快很多，当列选中信号到来的时候，CPU直接从行缓冲中读取数据。而且行缓冲技术遵循了 程序的局部性原理 当CPU后面再需要访存的时候，如果行译码后仍然在这一行，CPU就会直接从 SRAM 中读取数据。这样不但访存时间不会增加，反而因为局部性原理访存时间还减少了不少。<br><br><br>存储器也会出现和一维地址译码方式类似的问题。如果单方面扩大存储芯片的存储容量，就可能导致芯片越做越大，不但可能影响存储性能，也会对物理空间造成影响。因此，我们可以对存储芯片进行编址，这种增加芯片数量增大寻址范围的方式就是我们所说的字扩展。<br>举个例子，两片 4K * 8bits 的芯片可以通过字扩展形成 8K * 8bits 的”组合芯片“。通过字扩展，CPU 可以寻址的范围就从 0-4095 变为 0-8191。<br>芯片进行字扩展时，因为字长不需要改变，所以芯片的字线相连一模一样。片内的地址线也不需要改变，只需要译码芯片选中时将输出端连接响应的芯片片选引脚即可。我们使用3-8译码器，3位地址线可以编址选中 8 片芯片。<br><br>字扩展可以帮我们解决内存寻址大小的问题，但是随着16位机、32位机器甚至64位机的出现，我们还需要对内存芯片进行位扩展来使得我们一次性能读出符合机器的的位数。假如我们的机器是16位机，而恰好我们手头上只有位宽8位的芯片，不论寻址大小是多少，我们总需要两片这样的芯片才能位扩展成16位 I/O 的数据宽度要求。<br>当我们用芯片进行位扩展时，我们想要扩展芯片 I/O 的数据宽度。若 CPU 数据线有16根，我们用两个 4K*8bits 的芯片，片选和地址引脚连接完全相同，芯片1的8位数据线连接系统数据线 D0-D7，芯片2的8位数据线连接系统的 D8-D15即可完成芯片的位扩展。<br><br>字位同时扩展。]]></description><link>https://congzhi.wiki/some-notes/存储器扩展技术.html</link><guid isPermaLink="false">Some Notes/存储器扩展技术.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:45:21 GMT</pubDate><enclosure url="https://congzhi.wiki/some-notes/pics/pasted-image-20240812173423.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/some-notes/pics/pasted-image-20240812173423.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cache Pitfalls]]></title><description><![CDATA[ 
 <br><br>这里，我们说的 cache 不是内存架构 (memory architecture) 里面的 CPU cache，而是软件层面的数据缓存，虽然二者不同，但其核心思想相似——将访问数据缓存到高速存储介质中，减少对较慢存储介质（如数据库）的依赖，从而提升系统的整体性能。<br>服务器中的缓存分多个层面，有 bring it home 的 CDN 缓存，有将浏览器资源缓存到主机内存/磁盘的 HTTP 缓存，还有将磁盘数据库中的数据缓存到内存数据库中的数据库缓存。我们今天要讨论的就是这种数据库缓存这一层面。<br>我们都知道，磁盘 IO 的访问很慢，即使是 SSD ，速度也仍然只有内存的百分之一，所以内存能够承载的 QPS (Queries Per Second) 要远大于 SSD 甚至 HDD 这类二级存储器。所以如果服务器在服务请求时，所有的数据都存放在磁盘上，那就会造成服务器的高负载（因为需要将数据从磁盘拉到内存中才能相应请求）。<br>对此，我们的解决方案是将服务响应可能用到的数据缓存到内存中，也就是把常用数据缓存在内存中的一层临时存储层，以便减少访问更慢的存储介质（数据库）的频率，从而提升系统整体的速度和性能，降低服务器负载。<br><br>虽然缓存技术确实能够降低服务器的负载，提高服务器的响应速度。但是在设计缓存的时候，我们可能会遇到一系列问题，我们称作——缓存陷阱 (Cache Pitfalls)。其指的是缓存技术的使用过程中可能遇到的常见问题，可能会影响系统性能、数据一致性或稳定性。我们下来一个一个地介绍缓存带来的潜在问题。<br><br>缓存带来的首要问题之一是高内存占用。缓存的本质就是将磁盘数据库中的数据存储到内存中以提升访问速度，因此如果不加以限制，缓存数据可能会无限增长，最终导致内存被完全占满。而服务器应用程序运行时需要足够的内存来处理请求，如果缓存占用过多，就会增加系统负载，导致无法分配新进程所需的内存，甚至可能引发系统崩溃 (Out of Memory, OOM)。<br>为了解决这种问题，我们可以限制缓存最大的内存占用，并使用合理的策略（比如 LRU 或 LFU）来淘汰低频次访问的数据。这样我们就可以避免单机缓存可能带来的 OOM 内存崩溃问题。在常见的内存数据库（如 Redis）中，一般也可以通过设置缓存生存时间 (TTL) 避免过高的内存占用。 <br>这样就会带来另外的问题——由于单机缓存容量有限而造成的缓存命中率过低的问题。如果用户量极大，单个缓存节点就无法存储足够的热数据，那么缓存就会形同虚设。每次访问系统都可能需要完成：缓存未命中-&gt;读磁盘-&gt;写缓存-&gt;响应请求。<br>为了解决单机缓存容量过小的问题，一般实践上会采用分布式缓存。也就是将缓存数据分布在多个服务器上，这里就称为 caching server 了，因为这些服务器的功能高度特化。如此一来，缓存横向扩展，避免了单缓存节点过载，提高整体的命中率。<br><br>此外，缓存不一致性也是数据缓存需要考虑的问题。缓存不一致性指的是由于缓存和数据库之间的更新不同步，可能导致数据不一致。例如，数据库更新后，缓存仍然存储旧数据。在操作系统中，由于持久化的需求，一般的系统实现会采用写穿透 (Write-through) 或写回 (Write-back) 策略来确保数据的同步。在实践中，缓存系统通常会使用消息队列（如 Kafka）来触发缓存更新。<br><br>缓存崩溃也是我们要考虑的一个问题。即如果整个缓存系统突然失效，那么所有请求都会直接访问数据库，可能导致数据库过载，系统的负载突然陡增并引发系统奔溃。<br>对于缓存崩溃问题，我们同样可以采用分布式的高可用缓存集群，即使一个缓存服务器节点崩溃了，我们也可以确保大部分缓存仍然可用。<br><br>在高并发环境下，一个关键数据一时间可能被请求成千上万次。一旦该数据从缓存中过期或被删除，那么多个请求将同时尝试重新计算或获取该数据。假设我们有 1000 个请求，每 10 个请求我们用 1 个线程计算并更新缓存，那么一时间就会有 100 个线程计算更新同一个资源的缓存。这就会导致数据库或后端承受巨大压力、系统资源耗尽，最终导致阻塞崩溃 (congestion collapse)。<br>这种多个请求同时尝试获取和计算同一数据的情况，被称为缓存踩踏。为了解决缓存踩踏问题，避免后端服务负载过高的问题，我们可以使用互斥锁机制。在缓存失效时，使用分布式锁来确保只有一个请求负责更新缓存。其他请求等待更新完成后从缓存读取，而不是访问数据库。<br>在某个请求正在更新缓存时，如果有对关键数据的其他请求，那么其他请求就需要进行退避重试 (exponential backoff/backoff retry) ，要求请求方在短暂等待后再次查询缓存。<br>除了锁机制，我们也可以让后台任务提前刷新缓存，避免缓存过期 (cache refresh)。<br><br>上面所述的缓存踩踏发生在某一个热点数据的缓存过期。而我们也会有大量的缓存同时过期的情况，我们称之为缓存雪崩。<br>在缓存服务器刚刚启动的时候，这时服务器的内存中还没有任何对数据的缓存，如果这时启动服务器，就会导致大量的请求同时涌向数据库，造成数据库负载骤增，甚至导致系统崩溃。这时缓存雪崩的一种情况，但是并不直观（体现不出”雪崩“）。<br>为了避免缓存服务器刚启动时可能导致的后端负荷骤增，所以我们通常会进行缓存预热 (Cache Prewarming)，即在缓存服务器服务请求前就加载好关键数据，减少数据库的压力。而这时，如果设置的缓存 TTL 都是一样的，那么就会让预热好的大量数据在某个时间点同时过期，形成”雪崩“。同样的，若是某个缓存服务器宕机，也会导致大量的缓存数据的失效，形成”雪崩“。<br>雪崩导致的结果就是所有请求绕过缓存直接访问数据库，造成数据库负载骤增，导致系统崩溃。为了避免雪崩的发生，在设置 TTL 时可以采用随机过期时间（对分配的 TTL 增加随机的偏移量），避免所有缓存同时失效。<br><br>最后我们要介绍的缓存陷阱叫缓存穿透，指的是请求根本不存在的的数据，即数据既不在缓存中，也不在数据库中。缓存穿透发生时，每次请求都会直接访问数据库，增加数据库压力。如果有大量这样的请求，就会徒增服务器的负载。攻击者可以有意的发生大量随机的查询请求来影响服务器。<br>为了避免这种对根本不存在数据的查询，我们可以给已有数据做哈希，将数据的哈希结果存储在一个位数组中。如果请求数据的哈希结果在位数组为 1，那么就说明数据是存在的，否则说明数据不存在。但如果数据量巨大（如用户名，可能是十亿量级的），一个哈希函数可能让多个数据对应同一个位数组（哈希冲突）。<br>为了避免这种哈希冲突，通常上，我们会使用布隆过滤器 (Bloom Filter)，也就是多个哈希函数来计算多个哈希结果并存储在位数组中，能够有效避免哈希冲突。<br>此外，还可以对不存在的数据存储空值占位符，减少重复查询。]]></description><link>https://congzhi.wiki/some-notes/cache-pitfalls.html</link><guid isPermaLink="false">Some Notes/Cache Pitfalls.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 23 May 2025 03:16:01 GMT</pubDate></item><item><title><![CDATA[Getting Started with Swiftly on Linux]]></title><description><![CDATA[ 
 <br>Quote
This is silly...
<br><br>You need at first download swiftly for Linux, swift has a document for this: <br>curl -O https://download.swift.org/swiftly/linux/swiftly-1.0.0-$(uname -m).tar.gz
<br>Extract the archive:<br>tar -zxf swiftly-1.0.0-$(uname -m).tar.gz
<br>and then initialize it:<br>du@DVM:~$ ./swiftly init
Swiftly will be installed into the following locations:

/home/du/snap/code/184/.local/share/swiftly - Data and configuration files directory including toolchains
/home/du/.local/share/swiftly/bin - Executables installation directory

These locations can be changed with SWIFTLY_HOME_DIR and SWIFTLY_BIN_DIR environment variables and run this again.

Once swiftly is installed it will install the latest available swift toolchain. In the process of installing the new toolchain swiftly will add swift.org GnuPG keys into your keychain to verify the integrity of the downloads.

Proceed? (Y/n): 
y
Installing swiftly in /home/du/.local/share/swiftly/bin/swiftly...
Creating shell environment file for the user...
Updating profile...
Fetching the latest stable Swift release...
Installing Swift 6.1.0
                                                                      Downloading Swift 6.1.0
100% [===============================================================================================================================================]
Downloaded 838.4 MiB of 838.4 MiB

Verifying toolchain signature...
Extracting toolchain...
The global default toolchain has been set to `Swift 6.1.0`
Swift 6.1.0 installed successfully!
To begin using installed swiftly from your current shell, first run the following command:
    
# Added by swiftly
. "/home/du/snap/code/184/.local/share/swiftly/env.sh"

Your shell caches items on your path for better performance. Swiftly has added items to your path that may not get picked up right away. You can run this command to update your shell to get these items.

    hash -r

There are some dependencies that should be installed before using this toolchain.
You can run the following script as the system administrator (e.g. root) to prepare
your system:

    apt-get -y install gnupg2 libpython3-dev libxml2-dev libz3-dev pkg-config python3-lldb-13
<br>在初始化的时候，swiftly 提醒我在使用 swiftly 之前，运行这几条命令：<br># Load env variable at current shell
. "/home/du/snap/code/184/.local/share/swiftly/env.sh"
# Refreshing cache
hash -r
# Download dependencies
apt-get -y install gnupg2 libpython3-dev libxml2-dev libz3-dev pkg-config python3-lldb-13
<br>把 Swift 的路径和环境变量添加到系统的 PATH 中，实现全局使用。<br>du@DVM:~$ echo 'export PATH="$HOME/.local/share/swiftly/bin:$PATH"' &gt;&gt; ~/.bashrc
du@DVM:~$ echo 'source /home/du/snap/code/184/.local/share/swiftly/env.sh' &gt;&gt; ~/.bashrc
du@DVM:~$ source ~/.bashrc
<br>第一行 Swift 代码：<br>print("Hello")
<br>swift myProc.swift
<br>SwiftUI 只能在 Mac 上用。。。]]></description><link>https://congzhi.wiki/some-notes/getting-started-with-swiftly-on-linux.html</link><guid isPermaLink="false">Some Notes/Getting Started with Swiftly on Linux.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:43:49 GMT</pubDate></item><item><title><![CDATA[Intro to Algorithm]]></title><description><![CDATA[ 
 <br><br><br>对于一段好的程序，算法至关重要。算法是解决问题的一系列步骤或规则，是抽象的。而程序则更加具体，当你将算法变为程序时，你需要在特定的环境下用特定的编程语言来进行实现。<br>下面是算法和程序的比较：<br><br>算法是实现前的程序设计，需要领域内的知识，算法可以用任何语言来描述而且通常上无关系硬件和系统。而程序是算法思想的实现，由程序员完成，按照要求用特定的编程语言在特定的硬件设备和系统环境下进行。<br>在算法阶段，我们衡量一个算法好坏的评判标准是分析其时间复杂度和空间复杂度，这通常作为先验分析。在程序实现完成后，我们需要通过实际的运行来评估其性能，这被叫做后验测试。<br><br>算法的特征或properties用以下五方面来表示：<br>
<br>Input: 一个算法应当有0个或多个输入；
<br>Output: 一个算法应当有至少一个输出，不然算法就没有意义了；
<br>Definiteness: 确定性是指算法的每一步操作都应当有明确的定义和说明，确保算法是清晰的、无歧义的。避免出现不确定性。
<br>Finiteness: 要让算法起作用，最起码要保证算法在某些时刻必须终止。这就是有穷性。
<br>Effectiveness: 有效性是指算法的每一步操作都应当是基本的，不应当掺杂多余的部分。
<br><br><br>写一个算法很简单，你可以不需要任何的编程语言知识来描述算法的思想。但是，算法的实现需要你将算法思想转换成一些编程语言。假如我有一个交换A和B值的算法，我可以这样实现：<br>SWAP(A, B):
Begin
	temp = A
	A = B
	B = Temp
End
<br>也可以这样：<br>int SWAP(A, B)
{
	temp := A;
	A := B;
	B := temp;
}
<br>尽管细节可能有所不同，但它们传达的主题思想都是一样的。<br><br>传统上，我们通常使用时间复杂度和空间复杂度来衡量一个算法的好坏。此外，我们还有其他的一些标准来衡量一个算法，如：<br>
<br>Network consumption
<br>Power consumption
<br>CPU registers consumption
<br><br>对于时间复杂度的计算，我们简单地将高级语言程序中的一行代码作为一个基本操作。在上面的SWAP算法中，我们的算法的时间复杂度就为3，因为有三行高级语言代码。虽然高级语言程序最终会变成二进制的机器语言，但我们不用关心对时间复杂度的影响。（对能耗和寄存器的确有影响）<br>由于我们算法的时间复杂度的最高阶是一个常数，因此我们称其为常数时间复杂度，记作&nbsp;。<br><br>同样上述的算法，当我们计算空间复杂度时，我们将一个变量看作一个基本单位。通过分析程序中基本单位的数量和它们的使用频率，我们可以确定整个算法的空间复杂度。在SWAP算法中，我们有三个变量，但由于我们的空间复杂度最高阶是常数阶，我们也可以将其记作&nbsp;。<br><br>     - Constant<br>
 - Logarithmic<br>
  - Square&nbsp;Root<br>
     - Linear<br>
    - Quadratic<br>
    - Cubic<br>
     - Exponential<br>Big-O describes&nbsp;the&nbsp;worst-case&nbsp;scenario,&nbsp;providing&nbsp;an&nbsp;upper&nbsp;bound&nbsp;on&nbsp;the&nbsp;time&nbsp;complexity.<br>Big-Ω describes&nbsp;the&nbsp;best-case&nbsp;scenario,&nbsp;providing&nbsp;a&nbsp;lower&nbsp;bound&nbsp;on&nbsp;the&nbsp;time&nbsp;complexity.<br>Big-Θ describes&nbsp;the&nbsp;average case&nbsp;scenario.]]></description><link>https://congzhi.wiki/some-notes/intro-to-algorithm.html</link><guid isPermaLink="false">Some Notes/Intro to Algorithm.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:43:02 GMT</pubDate></item><item><title><![CDATA[ISA, Instructions and CPU]]></title><description><![CDATA[ 
 <br><br>我们都明白，计算机系统可以分成两类功能上的实体——计算机硬件、计算机软件。但是软件和硬件是两个很宽泛的概念，继续划分，我们可以将整个计算机系统划分成 7 层结构：<br>
<br>应用（问题）
<br>算法
<br>编程（语言）
<br>指令集体系结构(Instruction Set Architecture, ISA)
<br>微体系结构
<br>功能部件的电路设计
<br>器件
<br>在这 7 层结构中软件层面包括应用、算法、编程和ISA，硬件层面包含有ISA、微体系结构、电路设计和器件。你发现了么？软件层面包含ISA，硬件层面也包括了ISA，那ISA到底是何方神圣，脚踏两条船？<br>在我们学习操作系统的时候，可能会听老师说：“操作系统是介于软件和硬件之间最接近硬件的软件，为软件间接提供操作硬件的接口“。通过上面对ISA极其模糊的介绍之后，你会不会有很多关于ISA和操作系统的疑惑？我们下面来看看ISA究竟是什么东西。<br><br>ISA是指令集体系结构，它对于硬件而言是一种设计上的规约，而对于操作系统而言ISA提供API的接口。我们常听说ARM、MIPS、x86等这些就对应着不同的ISA。既然叫指令集体系结构，那它肯定和指令有着密不可分的关系，对于硬件来说，ISA是一种规约，它规定了CPU需要为哪些指令提供接口（例如：加减乘除与或非），而至于具体的实现？那完全是微体系结构的事情。（ISA关心的是有没有乘法器、乘法指令的事情，微体系结构关心的是乘法器如何实现的事情。）<br>我们举例子直观来感受一下ISA对上层操作系统提供的指令集接口，我们用ARM和x86举例子：<br>
<br>
加载寄存器

<br>ARM:&nbsp;LDR R0, [R1]

<br>二进制表示:&nbsp;1110 0101 1000 0000 0000 0000 0000 0000


<br>x86:&nbsp;MOV EAX, [EBX]

<br>二进制表示:&nbsp;1000 1011 0000 0000




<br>
加法操作

<br>ARM:&nbsp;ADD R0, R1, R2

<br>二进制表示:&nbsp;1110 0000 1000 0001 0000 0000 0000 0010


<br>x86:&nbsp;ADD EAX, EBX

<br>二进制表示:&nbsp;0000 0001 1100 0011




<br>
存储寄存器
- **ARM**:&nbsp;`STR R0, [R1]`
    - 二进制表示:&nbsp;`1110 0101 0000 0000 0000 0000 0000 0000`
- **x86**:&nbsp;`MOV [EBX], EAX`
    - 二进制表示:&nbsp;`1000 1001 0000 0000`

发现了么？ISA不同，实现相同功能的指令就是不一样的。这也就是为什么你写的程序不能在其他平台上运行，这里的平台指的就是ISA。

<br><br>如果我们的只有定长的8位指令，我们如何设计自己的ISA？我们下面给出规约：<br><br>系统应当有6个寄存器（REG0-REG5），一个输入端，一个输出端。<br><br>用高两位用来表示指令的类型:<br>
<br>
00表示立即数指令，我们规定立即数指令总是将立即数送给REG0;

<br>
01表示计算指令，这部分指令由ALU实现，我们进行如下规定：

<br>01 000 000表示OR指令
<br>01 000 001表示NAND指令
<br>01 000 010表示NOR指令
<br>01 000 011表示AND指令
<br>01 000 100表示ADD指令
<br>01 000 101表示SUB指令

我们可以向立即数指令一样，设置某个特定的寄存器用于特定的作用，如：算数操作永远是将REG1和REG2中读取数值作为输入的操作数，并将结果写入REG3中。

<br>
10表示复制指令，将源的数据送到目的，我们进行如下规定：

<br>10 000(SRC) 000(DEST)高两位表示类型，中间三位表示源，后三位表示目的。
<br>000-101表示从REG0-REG5的寄存器，中间三位为110时表示从输入端读数据，后三位为110时表示将源数据送到输出端。


<br>
11表示条件判断指令，只有满足条件才会将数据输出，我们给出如下规定：

<br>11 000 000表示永不输出
<br>11 000 001表示输入=0时输出
<br>11 000 010表示输入&lt;0时输出
<br>11 000 011表示输入≤0时输出
<br>11 000 100表示永远输出
<br>11 000 101表示输入≠0时输出
<br>11 000 111表示输入&gt;0时输出


<br><br>汇编，我们可以理解为机器码的别名，每一条汇编指令都可以对于某一个特定的指令。依此，我们可以将机器码通过添加汇编别名方便地为人类理解，如：<br>add ;01 000 100
sub ;01 000 101

mov_reg0_reg5 ;10 000 101
mov_in_out ;10 110 110
<br>等等。看到了么？在我们的指令系统中，操作系统可以用add指令来与底层的裸机进行交互，告诉机器我现在要让你把REG1和REG2的数据相加放到REG3中。<br><br>根据上面我们的指令集体系结构的规约，我们就可以设计自己的CPU了。<br><br>首先，我们需要译码器帮助我们用高两位将不同功能的指令给筛选出来，这里，我们简单用分线器将 8 位指令分成 8 个输入端，取高两位用2-4译码器来完成译码的工作。当高两位为00就使能立即数操作的工作逻辑，为01就使能ALU，以此类推。]]></description><link>https://congzhi.wiki/some-notes/isa,-instructions-and-cpu.html</link><guid isPermaLink="false">Some Notes/ISA, Instructions and CPU.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 23 Jul 2025 09:01:42 GMT</pubDate></item><item><title><![CDATA[Some Notes]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">Go Back Home</a><br><br><br><a data-href="Some Notes" href="https://congzhi.wiki/some-notes/some-notes.html" class="internal-link" target="_self" rel="noopener nofollow">Some Notes</a> 用于保存记录零碎的笔记，或是暂时未开工的新计划。<a data-href="Some Notes" href="https://congzhi.wiki/some-notes/some-notes.html" class="internal-link" target="_self" rel="noopener nofollow">Some Notes</a> 下的笔记有：<br>Some Notes:

<br><a data-href="Three Steps to Have Your Own Obsidian Site" href="https://congzhi.wiki/some-notes/three-steps-to-have-your-own-obsidian-site.html" class="internal-link" target="_self" rel="noopener nofollow">Three Steps to Have Your Own Obsidian Site</a>
<br><a data-href="进程的一生——从出生到死亡 (Abandoned)" href="https://congzhi.wiki/some-notes/进程的一生——从出生到死亡-(abandoned).html" class="internal-link" target="_self" rel="noopener nofollow">进程的一生——从出生到死亡 (Abandoned)</a> --&gt; <a data-href="Building and Version Control" href="https://congzhi.wiki/building-and-version-control/building-and-version-control.html" class="internal-link" target="_self" rel="noopener nofollow">Building and Version Control</a>
<br><a data-href="存储器扩展技术" href="https://congzhi.wiki/some-notes/存储器扩展技术.html" class="internal-link" target="_self" rel="noopener nofollow">存储器扩展技术</a>
<br><a data-href="Cache Pitfalls" href="https://congzhi.wiki/some-notes/cache-pitfalls.html" class="internal-link" target="_self" rel="noopener nofollow">Cache Pitfalls</a>
<br><a data-href="Getting Started with Swiftly on Linux" href="https://congzhi.wiki/some-notes/getting-started-with-swiftly-on-linux.html" class="internal-link" target="_self" rel="noopener nofollow">Getting Started with Swiftly on Linux</a>
<br><a data-href="ISA, Instructions and CPU" href="https://congzhi.wiki/some-notes/isa,-instructions-and-cpu.html" class="internal-link" target="_self" rel="noopener nofollow">ISA, Instructions and CPU</a>
<br><a data-href="printf and Concurrency" href="https://congzhi.wiki/some-notes/printf-and-concurrency.html" class="internal-link" target="_self" rel="noopener nofollow">printf and Concurrency</a>
<br><a data-href="Redis at a Peek" href="https://congzhi.wiki/some-notes/redis-at-a-peek.html" class="internal-link" target="_self" rel="noopener nofollow">Redis at a Peek</a>
<br><a data-href="Intro to Algorithm" href="https://congzhi.wiki/some-notes/intro-to-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Intro to Algorithm</a>
<br><a data-href="What is CUDA" href="https://congzhi.wiki/some-notes/what-is-cuda.html" class="internal-link" target="_self" rel="noopener nofollow">What is CUDA</a>
<br><a data-href="What is Tensors" href="https://congzhi.wiki/some-notes/what-is-tensors.html" class="internal-link" target="_self" rel="noopener nofollow">What is Tensors</a>
<br><a data-href="YOLO Env Config" href="https://congzhi.wiki/some-notes/yolo-env-config.html" class="internal-link" target="_self" rel="noopener nofollow">YOLO Env Config</a>

<br><br><br>任何疑问都欢迎通过 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/some-notes/some-notes.html</link><guid isPermaLink="false">Some Notes/Some Notes.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 20 Jun 2025 20:26:12 GMT</pubDate></item><item><title><![CDATA[What is CUDA]]></title><description><![CDATA[ 
 <br>Inspired by my undergraduate project.<br><br>大多数计院都不会在大学中系统地教授 GPU 这一至关重要的计算机部件。传统的教学内容都把重点放在计算机的五大部件、操作系统（包括网络协议栈）、编程语言等内容，而对 GPU 的关注出奇的少。诚然，学习完这些基本的知识已经足以应对大多数的应用场景，但 GPU 作为现代计算机高性能计算的核心，我们无法忽视这头横冲直撞的大象。<br>GPU 是图形处理器的英文字母缩写，顾名思义，就是一个特化的电子电路，专为辅助 CPU 进行电子图像处理而设计。进入千禧年后，Nvidia 推出了 GeForce 256，标志着 GPU 进入通用并行计算时代 (general-purpose parallel computing, 即 GPGPU)。这使得 GPU 或 GPGPU 不再局限于传统的图像处理，而是在科学计算和人工智能等领域发挥作用。<br><br>有一个很确切的例子来描述 CPU 和 GPU 之间的关系：CPU 是一小撮数学领域的大佬（CPU 一般只有 24/16/8 个核心），能够解出各种复杂的问题，而且单个计算速度极快；而 GPU 更像是一个中学的所有学生（GPU 有上万个核心），虽然水平局限，但胜在人数多。<br>在解决一般性问题上，一个学校上千名学生解题的速度可比这一小撮大佬们要快多了。比方说，现在有  道加法题，一般情况下，我们会采取这种解决方法，时间复杂度为 ：<br>// vec_c[n] = vec_a[n] + vec_b[n]
for(int i = 0; i &lt; n; i++) {
	vec_c[i] = vec_a[i] + vec_b[i];
}
<br>而把这些加法题分给全校的学生，那么问题就会这样被解决，时间复杂度可能为 ，其中 表示学生的数量，也就是 GPU 的核心数：<br>void vecAdd(int* vec_a, int* vec_b, int* vec_c) {
	int i = studentID;
	vec_c[studentID] = vec_a[studentID] + vec_b[studentID];
}
<br>在 AI（如 CNN ）等领域的应用上，GPU 提供的这种高并行计算完美的契合深度学习的计算需求。以卷积操作为例，每一次卷积都涉及到大量的矩阵运算，卷积核会不断地在输入数据上滑动（CPU）并进行点积计算，因为每次滑动的卷积计算都是独立的，所以天然的可以进行并行化处理。诸如此类几乎天然的就能拆分成独立任务的计算就被称为 embarrassingly parallel，也是通用并行计算的核心。<br><img alt="convolution_process.png" src="https://congzhi.wiki/some-notes/pics/convolution_process.png"><br><br>CPU 和 GPU 核心数的差异就使得操作系统和 CPU 的交互方式跟与 GPU 的交互方式大不同。由于较少的核心数量，操作系统对 CPU 核心的调度是精细化的，因此我们有各种调度算法来将线程放在 CPU 核心上运行，进而在系统程序员眼中，CPU 核心是可见的。CPU 计算中，一个任务通常会顺序地在一个核心上执行直到结束。<br>GPU 拥有成千上万个核心，操作系统没有办法逐一调度这么庞大数量的核心，因此会依赖 GPU 进行批量式的任务分配，通常会依赖于 OpenCL、CUDA 等这种 GPU 计算框架。对于操作系统而言，GPU 是不可见的（无从得知使用哪种指令集，有多少个核心等），系统通常只能够看到如 CUDA API 提供的高级抽象 API。<br>在物理结构上，GPU 通常会被分为多个图形处理簇 (Graphics Processing Cluster, GPC)。每个 GPC 又由流多处理器 (Streaming Multiprocessor, SM) 构成，SM 由进而由多个 Warps、一个光追核心 (Ray tracing core) 组成，每个 Warp 由有多个 CUDA 核心和 Tensor 核心构成。<br>早期，CUDA 核心叫做流处理器 (Streaming Processor, SP)，用于描述 GPU 的内部计算单元。在 2006 年， Nvidia 推出了 CUDA 架构，引入了 CUDA 编程模型，因为使用 CUDA API 的每个线程由一个 SP 负责执行，所以 SP 被更名为 CUDA 核心。并行任务的执行调度由 Warp 调度器 (warp scheduler) 负责。<br>
<img alt="GPU_arch.png" src="https://congzhi.wiki/some-notes/pics/gpu_arch.png"><br>通常来说，GPU 有自己的专用显存 (Video RAM)，显存的吞吐量（TB/s）要远远大于内存的吞吐量（GB/s）。上面我们又提到，GPU 的优势在于并行运算，这就意味着在 embrrassingly parallel 的计算任务中，GPU 的数据处理能力要显著优于 CPU。<br><br>GPU 在系统中就像一个外设，因为系统并不能直接访问 GPU 的底层硬件架构。当我们需要使用 GPU 进行并行计算时，通常需要使用驱动程序（如 CUDA driver）与 GPU 进行通信。<br>GPU 有很多核心，运行着与 CPU 截然不同的指令集架构，所以用 CUDA driver 将指令发给 GPU 时，我们需要用特定的编译器（如 NVCC）把高级语言源程序编译为特定 GPU 架构下能够运行的机器指令。所以除了驱动层（CUDA driver），CUDA 还封装了 CUDA runtime，为程序员暴露更高级的接口。对于将 GPU 用于科学计算但不太懂编程的人，CUDA 还提供更高级封装好了的库（如cuDNN），使得通用并行计算更加高效。<br>所有的所有汇总在一起就是 Nvidia 提供 CUDA 计算模型（CUDA Libraries + CUDA Runtime + CUDA Driver），这是一个偏向软件层面的 architecture，屏蔽了 GPU 的底层 infrastructure。程序员可以通过 CUDA 提供的 API 提交并在 GPU 上执行并行计算任务。<br><img alt="CUDA.png" src="https://congzhi.wiki/some-notes/pics/cuda.png"><br>CUDA 计算模型会把用户的计算任务会被分成网格 (Grid)、线程块 (Block)、线程 (Thread)。每个网格包含多个块，每个块包含多个 CUDA 线程。在提交并行任务执行时，每个 CUDA 并行流（也就是 CUDA 线程）都会映射到一个 CUDA 核心上单独并行的执行计算任务，这些任务最终由 GPU 的 SM 进行调度，执行过程对操作系统屏蔽。<br>在 CUDA 编程模型中，一个并行线程由一个 CUDA 核心执行，多个线程（比如 32 个）组成一个 Warp，多个 Warp 组成一个 Block，多个 Block 组成一个 Grid 为 GPU 所调度。一个 Warp 中的所有的 CUDA 核心会执行相同的并行任务但可以处理不同的数据，而同一个 Block 中的不同 Warp 可以执行不同的任务。]]></description><link>https://congzhi.wiki/some-notes/what-is-cuda.html</link><guid isPermaLink="false">Some Notes/What is CUDA.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:06:16 GMT</pubDate><enclosure url="https://congzhi.wiki/some-notes/pics/convolution_process.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/some-notes/pics/convolution_process.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[What is Tensors]]></title><description><![CDATA[ 
 <br><br>张量是神经网络中核心的数据结构，神经网络中的输入、输出和变换都依赖张量进行数据的表示和处理。这与数学（线性代数）有着密切的联系。因为神经网络中的计算都是在计算机中完成的，所以张量成为了打通计算机科学与数学之间的桥梁。<br>在不同的领域中，张量的术语有所不同，在维度上零到二时：<br>
<br>我们用 number, array, 2-D array 等在计算机科学中来表示张量
<br>我们用 scalar, vector, matrix 等在数学领域中来表示张量
<br>它们的核心概念是类似的，而且是一一对应的，毕竟你需要在计算机中编程建模神经网络。在维度超过二维时，我们就会使用更加通用的术语。在数学中，我们会使用 N-D tensor 来表示一个多维的数据结构，N 代表索引数量；在计算机科学中，我们会使用 N-D array。<br><br><br>在深度学习框架中，张量的核心属性有：rank, axes, shape，也就是向量的秩、轴和形状。前面我们看到了不同维度张量在不同领域中的表示术语，不难理解这些术语代表的含义。我们说过神经网络中的输入、输出和变换（矩阵运算）都是以张量的形式表达和处理的，而张量的核心属性就像是一个个的 handler，给我们操纵张量进行运算的工具。<br>张量的秩表示了张量的维度数量，决定访问特定元素所需要的索引数量。张量的轴表示张量的数据方向，决定数据在哪个维度上进行计算和组织，比如在二维张量中的数据方向可能沿 X 轴，也可能沿 Y 轴。形状是张量的具体维度大小，即给出每个维度（轴）的长度。<br>在图像数据的处理中，图像数据通常为 4D 的张量，即有四个轴，分别是长度、宽度、色彩 (color channel) 和批大小 (batch size)，因为处理的图像不止一张。<br><br>在 CNN 卷积神经网络中，我们通常使用卷积操作来提取特征，其本质就是数据的局部变换，也就是一个升阶和降阶的过程。在这个过程中，一个神经元的 tensor 经过卷积或池化操作后，转换成下一个神经元的 tensor，进而逐步提取更高级别的特征。<br>在全连接层，我们通常需要将 2D 或是 3D 的张量展平 (flatten) 成 1D 向量用于最终的分类。这些升阶或降阶的操作被称为 reshaping，它确保数据能够适应不同层的计算需求。]]></description><link>https://congzhi.wiki/some-notes/what-is-tensors.html</link><guid isPermaLink="false">Some Notes/What is Tensors.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:38:35 GMT</pubDate></item><item><title><![CDATA[YOLO Env Config]]></title><description><![CDATA[ 
 <br>Warning
This note is NOT a tutorial.
<br><br>安装conda(运行环境隔离)<br>[Verifying - USTC Mirrors](https://mirrors.ustc.edu.cn/anaconda/miniconda/Miniconda3-py310_24.9.2-0-Windows-x86_64.exe)
<br>输入 conda create -n yolov8 python=3.10 新建环境<br>(base) C:\Users\Chi&gt;conda create -n yolov8 python=3.10
Channels:
 - defaults
Platform: win-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: C:\Users\Chi\miniconda3\envs\yolov8

  added / updated specs:
    - python=3.10


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    ca-certificates-2025.2.25  |       haa95532_0         130 KB
    openssl-3.0.16             |       h3f729d1_0         7.8 MB
    pip-25.0                   |  py310haa95532_0         2.5 MB
    python-3.10.16             |       h4607a30_1        16.3 MB
    setuptools-75.8.0          |  py310haa95532_0         1.7 MB
    tzdata-2025a               |       h04d1e81_0         117 KB
    vc-14.42                   |       haa95532_4          11 KB
    vs2015_runtime-14.42.34433 |       he0abc0d_4         1.2 MB
    wheel-0.45.1               |  py310haa95532_0         145 KB
    xz-5.6.4                   |       h4754444_1         280 KB
    ------------------------------------------------------------
                                           Total:        30.1 MB

The following NEW packages will be INSTALLED:

  bzip2              pkgs/main/win-64::bzip2-1.0.8-h2bbff1b_6
  ca-certificates    pkgs/main/win-64::ca-certificates-2025.2.25-haa95532_0
  libffi             pkgs/main/win-64::libffi-3.4.4-hd77b12b_1
  openssl            pkgs/main/win-64::openssl-3.0.16-h3f729d1_0
  pip                pkgs/main/win-64::pip-25.0-py310haa95532_0
  python             pkgs/main/win-64::python-3.10.16-h4607a30_1
  setuptools         pkgs/main/win-64::setuptools-75.8.0-py310haa95532_0
  sqlite             pkgs/main/win-64::sqlite-3.45.3-h2bbff1b_0
  tk                 pkgs/main/win-64::tk-8.6.14-h0416ee5_0
  tzdata             pkgs/main/noarch::tzdata-2025a-h04d1e81_0
  vc                 pkgs/main/win-64::vc-14.42-haa95532_4
  vs2015_runtime     pkgs/main/win-64::vs2015_runtime-14.42.34433-he0abc0d_4
  wheel              pkgs/main/win-64::wheel-0.45.1-py310haa95532_0
  xz                 pkgs/main/win-64::xz-5.6.4-h4754444_1
  zlib               pkgs/main/win-64::zlib-1.2.13-h8cc25b3_1


Proceed ([y]/n)?
<br>输入 y<br>Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate yolov8
#
# To deactivate an active environment, use
#
#     $ conda deactivate
<br>用 conda env list 查看现有环境<br>(base) C:\Users\Chi&gt;conda env list
# conda environments:
#
base                  *  C:\Users\Chi\miniconda3
yolov8                   C:\Users\Chi\miniconda3\envs\yolov8
<br>然后用 conda activate yolov8 激活环境，我们现在在 yolov8 的环境下<br>(base) C:\Users\Chi&gt;conda activate yolov8

(yolov8) C:\Users\Chi&gt;conda env list
# conda environments:
#
base                     C:\Users\Chi\miniconda3
yolov8                *  C:\Users\Chi\miniconda3\envs\yolov8
<br>配置清华源<br>(yolov8) C:\Users\Chi&gt;pip config set global.index-url https://mirror.tuna.tsinghua.edu.cn/pypi/web/simple
Writing to C:\Users\Chi\AppData\Roaming\pip\pip.ini
<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes
<br>在 cmd 中激活环境：<br>conda init cmd.exe
<br>关闭再打开<br>conda activate yolov8
<br><br>查看显卡支持的 CUDA 版本：12.3<br><img alt="cuda_version.png" src="https://congzhi.wiki/some-notes/pics/cuda_version.png"><br>30系显卡需要安装 CUDA 111 以上的版本<br>conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1  pytorch-cuda=11.8 -c pytorch -c nvidia
<br>测试一切是否正确：<br>(yolov8) C:\Users\Chi&gt;python
Python 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.ones(3).cuda()
tensor([1., 1., 1.], device='cuda:0')
&gt;&gt;&gt; exit()
<br><br><a data-tooltip-position="top" aria-label="https://github.com/ultralytics/ultralytics/releases?page=17" rel="noopener nofollow" class="external-link" href="https://github.com/ultralytics/ultralytics/releases?page=17" target="_blank">Releases · ultralytics/ultralytics</a><br>
下载版本 8.1.0<br>    目录: D:\gesture_recog\ultralytics-8.1.0


Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
d-----         2025/3/16     22:09                .github
d-----         2025/3/16     22:09                docker
d-----         2025/3/16     22:09                docs
d-----         2025/3/16     22:09                examples
d-----         2025/3/16     22:09                tests
d-----         2025/3/16     22:09                ultralytics
-a----         2025/3/16     22:09           2295 .gitignore
-a----         2025/3/16     22:09           2408 .pre-commit-config.yaml
-a----         2025/3/16     22:09            612 CITATION.cff
-a----         2025/3/16     22:09           5585 CONTRIBUTING.md
-a----         2025/3/16     22:09          34523 LICENSE
-a----         2025/3/16     22:09           6728 pyproject.toml
-a----         2025/3/16     22:09          35898 README.md
-a----         2025/3/16     22:09          34774 README.zh-CN.md


(yolov8) PS D:\gesture_recog\ultralytics-8.1.0&gt; pip install -e .
<br>下载<br>pip install jupyterlab tensorboard
<br>环境配置错误<br>pip uninstall pillow # 删除原先的 9.3.0 版本
pip install pillow==9.3.0 # 重新下载一遍，好了
<br>好了<br>(yolov8) PS D:\gesture_recog\ultralytics-8.1.0&gt; yolo
C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\torchvision\io\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

    Arguments received: ['yolo']. Ultralytics 'yolo' commands use the following syntax:

        yolo TASK MODE ARGS

        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose', 'obb')
                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')
                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'

    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01

    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320

    3. Val a pretrained detection model at batch-size 1 and image size 640:
        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640

    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128

    5. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API
        yolo explorer

    7. Run special commands:
        yolo help
        yolo checks
        yolo version
        yolo settings
        yolo copy-cfg
        yolo cfg

    Docs: https://docs.ultralytics.com
    Community: https://community.ultralytics.com
    GitHub: https://github.com/ultralytics/ultralytics
<br>至此，环境配置完成。<br><br>选择下载的 ultralytics 8.1.0 的文件夹。<br>配置终端和解释器。<br>下载 YOLO 模型<br>有警告：<br>(yolov8) D:\gesture_recog\ultralytics-8.1.0&gt;yolo mode='predict' task='detect' model='yolov8m.pt' source='00001.jpeg'
C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\torchvision\io\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
D:\gesture_recog\ultralytics-8.1.0\ultralytics\nn\tasks.py:634: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(file, map_location="cpu"), file  # load

image 1/1 D:\gesture_recog\ultralytics-8.1.0\00001.jpeg: 384x640 19 cars, 4 buss, 70.9ms
Speed: 7.6ms preprocess, 70.9ms inference, 148.1ms postprocess per image at shape (1, 3, 384, 640)
💡 Learn more at https://docs.ultralytics.com/modes/predict
<br>之后重新安装了 TorchVision<br>(yolov8) D:\gesture_recog\ultralytics-8.1.0&gt;conda install torchvision -c pytorch
<br>完成之后测试：发现 Pillow 的 _imaging 模块加载失败<br>(yolov8) D:\gesture_recog\ultralytics-8.1.0&gt;yolo mode='predict' task='detect' model='yolov8m.pt' source='00001.jpeg'
Traceback (most recent call last):
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "C:\Users\Chi\miniconda3\envs\yolov8\Scripts\yolo.exe\__main__.py", line 4, in &lt;module&gt;
  File "D:\gesture_recog\ultralytics-8.1.0\ultralytics\__init__.py", line 5, in &lt;module&gt;
    from ultralytics.data.explorer.explorer import Explorer
  File "D:\gesture_recog\ultralytics-8.1.0\ultralytics\data\__init__.py", line 3, in &lt;module&gt;
    from .base import BaseDataset
  File "D:\gesture_recog\ultralytics-8.1.0\ultralytics\data\base.py", line 17, in &lt;module&gt;
    from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM
  File "D:\gesture_recog\ultralytics-8.1.0\ultralytics\utils\__init__.py", line 19, in &lt;module&gt;
    import matplotlib.pyplot as plt
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\matplotlib\__init__.py", line 161, in &lt;module&gt;
    from . import _api, _version, cbook, _docstring, rcsetup
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\matplotlib\rcsetup.py", line 28, in &lt;module&gt;
    from matplotlib.colors import Colormap, is_color_like
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\matplotlib\colors.py", line 52, in &lt;module&gt;
    from PIL import Image
  File "C:\Users\Chi\miniconda3\envs\yolov8\lib\site-packages\PIL\Image.py", line 100, in &lt;module&gt;
    from . import _imaging as core
ImportError: DLL load failed while importing _imaging: 找不到指定的模块。
<br>随后重装 pillow 测试还是不行<br>然后通过 conda 包管理器从 conda-forge 中安装 pillow 包。<br>conda install -c conda-forge pillow
<br>好了。<br>from ultralytics import YOLO

yolo = YOLO(model='..\Models\yolov8m.pt', task='detect')

result=yolo(source=0, save=True)
<br><br>ultralytics\cfg\default.yaml # 存放参数和一些默认参数形式
<br><br>查看Pytorch是否能检查到GPU<br>import torch
print(torch.cuda.is_available())
<br>训练模型：<br>yolo task=detect mode=train model=..\Model\yolov8n.pt data=D:/gesture_recog/ASL/data.yaml epochs=50 imgsz=640 batch=4 device=0
<br># 验证模型
yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=D:/gesture_recog/ASL/data.yaml
<br>人脸识别测试：<br>(yolov8) D:\facial_recog\ultralytics-8.1.0&gt;yolo task=detect mode=val model=runs/detect/train3/weights/best.pt data=D:\facial_recog\Faces\data.yaml     
<br>(yolov8) D:\gesture_recog\ultralytics-8.1.0&gt;yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=D:/gesture_recog/ASL/data.yaml
D:\gesture_recog\ultralytics-8.1.0\ultralytics\nn\tasks.py:634: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(file, map_location="cpu"), file  # load
Ultralytics YOLOv8.1.0 🚀 Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)
Model summary (fused): 168 layers, 3010718 parameters, 0 gradients, 8.1 GFLOPs
val: Scanning D:\gesture_recog\ASL\valid\labels.cache... 1320 images, 60 backgrounds, 0 corrupt: 100%|██████████| 1320/1320 [00:00&lt;?, ?it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 83/83 [00:13&lt;00:00,  6.27it/s]
                   all       1320       1270      0.921      0.889      0.945      0.785
                     A       1320         41      0.989      0.927      0.963      0.812
                     B       1320         53      0.877          1      0.989      0.839
                     C       1320         51      0.944      0.922       0.98      0.777
                     D       1320         44      0.906      0.876      0.949      0.761
                     E       1320         46          1      0.909      0.949      0.862
                     F       1320         49      0.981      0.898      0.983      0.895
                     G       1320         51      0.843      0.841      0.897      0.761
                     H       1320         46      0.903      0.957      0.956      0.773
                     I       1320         48      0.721      0.729      0.782      0.536
                     J       1320         48      0.954      0.861      0.938      0.697
                     K       1320         51      0.899      0.804      0.893      0.726
                     L       1320         56          1      0.905      0.984      0.826
                     M       1320         50      0.934       0.92      0.982      0.845
                     N       1320         29      0.891       0.85      0.968       0.83
                     O       1320         54      0.958      0.836      0.947      0.701
                     P       1320         44      0.943      0.886      0.939      0.687
                     Q       1320         44      0.831      0.818      0.858      0.744
                     R       1320         52      0.909      0.957      0.974      0.875
                     S       1320         67      0.962      0.881      0.965      0.851
                     T       1320         45      0.926      0.911      0.959       0.89
                     U       1320         53      0.938      0.943      0.977      0.852
                     V       1320         48      0.937      0.929      0.961      0.798
                     W       1320         46      0.977      0.942      0.984      0.842
                     X       1320         57      0.981      0.895       0.96      0.723
                     Y       1320         55      0.928      0.938       0.97      0.802
                     Z       1320         42      0.824      0.786      0.867      0.716
Speed: 0.4ms preprocess, 4.3ms inference, 0.0ms loss, 1.3ms postprocess per image
Results saved to runs\detect\val2
💡 Learn more at https://docs.ultralytics.com/modes/val
<br># 使用模型预测
yolo task=detect mode=predict model=runs/detect/train/weights/best.pt source=path/to/images_or_video
<br><img alt="Pasted image 20250318224527.png" src="https://congzhi.wiki/some-notes/pics/pasted-image-20250318224527.png"><br>安装 Flask 库：<br>(yolov8) D:\gesture_recog\Flask&gt;pip install flask
<br><br><br><br><br><br><br>conda create -n labelimg python=3.8
conda activate labelimg
pip install labelImg
<br>and <br><br>To introduce websockets and onnxruntime packages for the , you need to do this to include them in the environment<br>pip install python-engineio==4.5.1 python-socketio==5.8.0 flask-socketio==5.3.6
pip install eventlet
pip install onnxruntime-gpu
<br>and for the onnx be available, you need to retrain your model follow the steps under below:<br>
<br>
Train Your YOLOv8 Model (If Not Done Yet) If you haven't trained your model yet, first train it using:
from ultralytics import YOLO

model = YOLO('yolov8n.pt')  # Use a pre-trained YOLOv8 model
model.train(data='path/to/dataset.yaml', epochs=50, imgsz=640)

 Once trained, your model will be saved in runs/detect/train/weights/best.pt.

<br>
Export to ONNX Format
from ultralytics import YOLO

model = YOLO('runs/detect/train/weights/best.pt')  # Load trained model
model.export(format="onnx")  # Convert to ONNX format

  This will generate best.onnx in the same directory.

<br>
Verify the Exported ONNX Model After exporting, ensure the model loads correctly:
import onnxruntime as ort

session = ort.InferenceSession("best.onnx")
print(session.get_inputs(), session.get_outputs())


<br>
Optimize the ONNX Model for Faster Inference You can further optimize the model using TensorRT:
trtexec --onnx=best.onnx --saveEngine=best.trt --fp16

  This converts the model into TensorRT format, significantly speeding up inference on NVIDIA GPUs.

<br>yolo task=detect mode=train ^
model=yolov8s.pt ^
data=D:/gesture_recog/MyGestures/data.yaml ^
epochs=200 ^
imgsz=480 ^
batch=8 ^
device=0 ^
optimizer=AdamW ^
lr0=0.002 ^
weight_decay=0.0005 ^
hsv_h=0.01 ^
hsv_s=0.7 ^
hsv_v=0.4 ^
degrees=10 ^
translate=0.1 ^
scale=0.5 ^
fliplr=0.5 ^
mixup=0.1 ^
amp=True ^
cos_lr=True ^
patience=30 ^
save_period=10
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
    173/200      1.27G     0.6958     0.6815      1.064         17        480: 100%|██████████| 623/623 [01:20&lt;00:00,  7.77it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 92/92 [00:12&lt;00:00,  7.25it/s]
                   all       1469       1419      0.933      0.937      0.966      0.806
Stopping training early as no improvement observed in last 30 epochs. Best results observed at epoch 143, best model saved as best.pt.
To update EarlyStopping(patience=30) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.

173 epochs completed in 4.448 hours.
Optimizer stripped from runs\detect\train\weights\last.pt, 22.5MB
Optimizer stripped from runs\detect\train\weights\best.pt, 22.5MB

Validating runs\detect\train\weights\best.pt...
Ultralytics YOLOv8.1.0 🚀 Python-3.10.16 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)
Model summary (fused): 168 layers, 11136807 parameters, 0 gradients, 28.5 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 92/92 [00:12&lt;00:00,  7.54it/s]
                   all       1469       1419      0.933      0.938      0.968      0.808
                     A       1469         41      0.975      0.952      0.977      0.836
                     B       1469         53      0.944          1      0.971      0.841
                     C       1469         51      0.986      0.961      0.992      0.795
                     D       1469         44      0.953      0.926      0.976       0.79
                     E       1469         46      0.912      0.935      0.955      0.841
                     F       1469         49      0.974       0.98      0.984      0.914
                     G       1469         51      0.912      0.941      0.975      0.803
                     H       1469         46      0.942      0.935      0.991      0.807
                     I       1469         48       0.78      0.729      0.822       0.55
                     J       1469         48      0.897      0.958      0.976      0.688
                     K       1469         51      0.917      0.866       0.95      0.773
                     L       1469         56      0.984      0.982      0.988      0.836
                     M       1469         50      0.953       0.92       0.97      0.853
                     N       1469         29      0.863      0.931      0.964      0.823
                     O       1469         54      0.896      0.926      0.943       0.74
                     P       1469         44      0.975       0.88      0.979      0.742
                     Q       1469         44      0.894      0.841       0.92      0.773
                     R       1469         52      0.897          1      0.976       0.87
                     S       1469         67       0.92      0.955      0.984      0.853
                     T       1469         45      0.935      0.952      0.971      0.908
                     U       1469         53      0.945      0.974       0.99      0.852
                     V       1469         48      0.919      0.917      0.975      0.804
                     W       1469         46      0.905      0.978       0.99      0.849
                     X       1469         57      0.963      0.918      0.981      0.778
                     Y       1469         55      0.965      0.992      0.964      0.822
                     Z       1469         42        0.9      0.854      0.916      0.743
             Thumbs-Up       1469         49      0.987          1      0.995      0.862
           Thumbs-Down       1469         50      0.989          1      0.995      0.832
         Middle-Finger       1469         50      0.991          1      0.995      0.866
Speed: 0.2ms preprocess, 2.9ms inference, 0.0ms loss, 1.4ms postprocess per image
Results saved to runs\detect\train
💡 Learn more at https://docs.ultralytics.com/modes/train
<br>安装&nbsp;flask_cors 模块<br># 阶段1 - 基础训练（带层冻结）
yolo train model=yolov8n.pt ^
data=D:/gesture_recog/MyGestures/data.yaml ^
epochs=30 ^
batch=16 ^
freeze=[0,1,2,3,4] ^
name=phase1_frozen

# 阶段2 - 微调训练（带早停）
yolo train model=runs/detect/phase1_frozen/weights/last.pt ^
data=D:/gesture_recog/MyGestures/data.yaml ^
epochs=60 ^
batch=12 ^
freeze=0 ^
name=phase2_unfrozen
]]></description><link>https://congzhi.wiki/some-notes/yolo-env-config.html</link><guid isPermaLink="false">Some Notes/YOLO Env Config.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 29 May 2025 12:40:23 GMT</pubDate><enclosure url="https://congzhi.wiki/some-notes/pics/cuda_version.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://congzhi.wiki/some-notes/pics/cuda_version.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Lecture I & II - Getting Started with SwiftUI]]></title><description><![CDATA[ 
 <br><br><br>Go take an explore, that would be fun. Xcode is Apple's IDE, offers a set of tools for iOS, macOS, watchOS, and tvOS development.<br>There's iPhone simulator, preview pane, navigator, inspector, run mode, select mode. Tons of features you could find on Xcode.<br><br><br>当你创建好一个项目，<br>everything is a struct in swiftUI<br>
class is other whole thing<br>import SwiftUI

struct ContentView: View {
    var body: some View {
        VStack {
            Image(systemName: "globe")
                .imageScale(.large)
                .foregroundStyle(.tint)
            Text("Hello, World!")
        }
        .padding()
    }
}

#Preview {
    ContentView()
}
<br>ContentView:view{}<br>
functional programming<br>
AKA protocol-oriented programming<br>struct ContentView: View {}<br>
Here, :view means the struct ContentView behaves like a View<br>in swift you can a variable like this:<br>var i: Int    // i is an integer
var s: String // s is a string
<br>这其实是一样的，但是又不太一样，View又是一种你可以behave like 的东西，而且你必须按照一定范式来进行，这个范式在 Swift 中叫做协议(protocol)，所以 Swift 也叫 Protocol- Oriented Programming Language<br>OOP 用于封装数据，POP 用于封装行为。所以你想要 ContentView，你就需要 behave like a View。而一旦 ContentView behave like a View 后，你就可以使用 View 定义的所有东西（现成的 built-in）<br>（View 就是屏幕上的一块长方体画布（所以也叫canvas？），涂涂画画然后taking events反馈一个view）<br>那什么叫 behave like a view?<br>View 是一种 protocol，要求你在你的 Struct 内定义 var body: some View {}<br>var body: some View {
        VStack {
            Image(systemName: "globe")
                .imageScale(.large)
                .foregroundStyle(.tint)
            Text("Hello, World!")
        }
        .padding()
    }
<br>var body是一个computed property，即下面的代码。意思是SwiftUI 中，var body: some View 并没有存储具体的值，而是每次访问时动态计算并返回一个视图结构。（每次访问，视图都可能变化，所以用 var）<br>	{
        VStack {
            Image(systemName: "globe")
                .imageScale(.large)
                .foregroundStyle(.tint)
            Text("Hello, World!")
        }
        .padding()
    }
<br>what is some view?<br>
this means that the type of this variable has to be any struct in the whole world as long as it behaves like a view.<br>
this two is the same:<br>struct ContentView: View {
	var body: some View {
		Text("Hello there")
	}
}
struct ContentView: View {
	var body: Text {
		Text("Hello there")
	}
}
<br>which view? some view! and this some tell swift execute this code and see what it returns and use whatever it returns<br>if you want behave like a view, you have to have a vairable that returns some other view<br>也就是说，由于 View 是 computed property，你不能确定每次运行后生成的 view body 都是一样的，所以使用 some view，是这样的么？<br>在 body 中，我们有 image struct 有 text struct，这些都 behave like a view，together，here we have ContentView<br>Image(systemName: "globe) // named parameters<br>VStack {}<br>
的作用就是将 {} 中的 View 组合成一个 TupelView 然后返回结果 所以你不能这样：<br>struct ContentView: View {
	var body: Text {
		VStack(content: {
			Text("Hello")
			Text("World")
		})
	}
}
<br>@ViewBuilder的作用就是确定这个some View 到底是什么View（TupleView）<br>这也是一个Function相当于<br>
VStack(content: {<br>
xxxxx<br>
})<br>.imageScale(.large)等这些函数被称为 View modifier<br><br><br>views are immutable<br>
@State<br>
what does this mean?<br>why cannot use for loop inside a view?<br>
ForEach is a view<br>implicit returns<br>
var cards: some View{<br>
return VStack{<br>
<br>
}<br>
}<br>internal parameter name and external parameter name<br>
编写主题的逻辑的函数 <br>trailing closure syntax只有在最后一个参数是闭包时才可以用<br>.opacity]]></description><link>https://congzhi.wiki/swift-learning-of-swift/lecture-i-&amp;-ii-getting-started-with-swiftui.html</link><guid isPermaLink="false">Swift Learning of Swift/Lecture I &amp; II - Getting Started with SwiftUI.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 01 Jul 2025 15:48:09 GMT</pubDate></item><item><title><![CDATA[Lecture III & IV - MVVM]]></title><description><![CDATA[ 
 <br><br><br>上节课，我们简单地学习了 SwiftUI 并构建了一个 Memorize app<br>这节课，我们将要学习如何构建 App 的逻辑，并将我们之前构建的 Memorize 应用中的逻辑、数据同 UI 分离开。我们要学习的设计范式叫 MVVM，全称 Model-View-ViewModel，是 SwiftUI 推荐的架构模式，旨在提高代码的可读性、可测试性和可维护性。它由三部分核心组件构成，即：<br>
<br>Model：用于描述应用程序的逻辑和数据访问。
<br>View：展示界面，响应用户输入，但不直接处理业务逻辑。
<br>ViewModel：用于绑定连接 View 和 Model，监听 model 的变化，提供状态给 view 使用。
<br>因为 SwiftUI 是声明式 (declared) 的，所以你只需要描述 UI 应该长什么样，而 SwiftUI 会在每次 model 变化后自动更新 view（比如说 Text("Hello") 中的内容改变后，UI 也随之改变，你完全不需要自己 rebuild）。我们关心的，只有如何将这些东西分离开。<br><br>我们现在知道 model 是什么，知道 view UI 是什么，但是将两者联系在一起的中间层是？<br>Swift 是强类型语言，类型安全可以大大减少 Bug<br>本节课重点强调了使用类型约束让代码更可靠且易于重构<br>View 本质上是一个泛型的 Struct，每次 UI 更新其实就是重建这个 View 的 Struct]]></description><link>https://congzhi.wiki/swift-learning-of-swift/lecture-iii-&amp;-iv-mvvm.html</link><guid isPermaLink="false">Swift Learning of Swift/Lecture III &amp; IV - MVVM.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 01 Jul 2025 18:10:53 GMT</pubDate></item><item><title><![CDATA[Protocol in Swift]]></title><description><![CDATA[ 
 ]]></description><link>https://congzhi.wiki/swift-learning-of-swift/protocol-in-swift.html</link><guid isPermaLink="false">Swift Learning of Swift/Protocol in Swift.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 01 Jul 2025 18:44:19 GMT</pubDate></item><item><title><![CDATA[Struct vs. Class in Swift]]></title><description><![CDATA[ 
 <br><br><br>许多编程语言都有 Struct 和 Class 这样类似的关键字用于声明新的类型（对象），Swift 也不例外。<br>但不同于 C++ 那样自由，Swift 语言对于类型有严格的限制。这就使得不同的类型在虚拟空间中存储的位置上不同的。<br>在 Swift 中，我们有值类型和引用类型。<br>在 C++ 中，struct 和 class 唯一的区别就是 域不同 ，它们都是类类型（面向对象）<br>而 Swift 中的 Struct 类型没有继承和派生，所以并不能算是一个类类型。]]></description><link>https://congzhi.wiki/swift-learning-of-swift/struct-vs.-class-in-swift.html</link><guid isPermaLink="false">Swift Learning of Swift/Struct vs. Class in Swift.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 17 Jun 2025 14:28:02 GMT</pubDate></item><item><title><![CDATA[Swift API Design Guidelines]]></title><description><![CDATA[ 
 <br><br>Swift 是]]></description><link>https://congzhi.wiki/swift-learning-of-swift/swift-api-design-guidelines.html</link><guid isPermaLink="false">Swift Learning of Swift/Swift API Design Guidelines.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Thu, 03 Jul 2025 12:24:42 GMT</pubDate></item><item><title><![CDATA[Swift Learning of Swift]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Home Sweet Home" data-href="Home Sweet Home" href="https://congzhi.wiki/home-sweet-home.html" class="internal-link" target="_self" rel="noopener nofollow">🔙Go Back Home</a><br><br><br>This is all about swift. Documenting everything about Swift programming language. <br><br><br>参考的资料有：<br>
<br><a data-tooltip-position="top" aria-label="https://cs193p.stanford.edu/2023" rel="noopener nofollow" class="external-link" href="https://cs193p.stanford.edu/2023" target="_blank">2023 | CS193p - Developing Apps for iOS</a>
<br><a data-tooltip-position="top" aria-label="https://docs.swift.org/swift-book/documentation/the-swift-programming-language" rel="noopener nofollow" class="external-link" href="https://docs.swift.org/swift-book/documentation/the-swift-programming-language" target="_blank">The Swift Programming Language | Documentation</a>
<br><a data-tooltip-position="top" aria-label="https://www.swift.org/documentation/api-design-guidelines/" rel="noopener nofollow" class="external-link" href="https://www.swift.org/documentation/api-design-guidelines/" target="_blank">API Design Guidelines for Swift</a>
<br>......
<br>Swift and SwiftUI
Developing Apps for iOS Using SwiftUI | CS193p 2023

<br><a data-href="Lecture I &amp; II - Getting Started with SwiftUI" href="https://congzhi.wiki/swift-learning-of-swift/lecture-i-&amp;-ii-getting-started-with-swiftui.html" class="internal-link" target="_self" rel="noopener nofollow">Lecture I &amp; II - Getting Started with SwiftUI</a>
<br><a data-href="Lecture III &amp; IV - MVVM" href="https://congzhi.wiki/swift-learning-of-swift/lecture-iii-&amp;-iv-mvvm.html" class="internal-link" target="_self" rel="noopener nofollow">Lecture III &amp; IV - MVVM</a>


Congzhi's Swift Journey

<br><a data-href="Swift API Design Guidelines" href="https://congzhi.wiki/swift-learning-of-swift/swift-api-design-guidelines.html" class="internal-link" target="_self" rel="noopener nofollow">Swift API Design Guidelines</a>
<br><a data-href="The Basics" href="https://congzhi.wiki/swift-learning-of-swift/the-basics.html" class="internal-link" target="_self" rel="noopener nofollow">The Basics</a>
<br><a data-href="Protocol in Swift" href="https://congzhi.wiki/swift-learning-of-swift/protocol-in-swift.html" class="internal-link" target="_self" rel="noopener nofollow">Protocol in Swift</a>
<br><a data-href="Struct vs. Class in Swift" href="https://congzhi.wiki/swift-learning-of-swift/struct-vs.-class-in-swift.html" class="internal-link" target="_self" rel="noopener nofollow">Struct vs. Class in Swift</a>


<br><br><br>如果有发现任何疑问、错误和错别字问题，欢迎在邮箱 <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a> 联系我。]]></description><link>https://congzhi.wiki/swift-learning-of-swift/swift-learning-of-swift.html</link><guid isPermaLink="false">Swift Learning of Swift/Swift Learning of Swift.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Sat, 26 Jul 2025 14:31:18 GMT</pubDate></item><item><title><![CDATA[The Basics]]></title><description><![CDATA[ 
 <br><br><br>Swift 和 C++ 一样，都属于静态类型语言，要求你明确地声明某一个值（变量、常量、属性）的类型后xxxxx，并不再修改（JavaScript就是类型不安全的，即 dynamic typed<br>而 Swift 是一种类型安全的语言<br>这里不对，应该添加swift不支持implicit conversion，而C++支持，swift更佳安全，而C++更自由<br><br>在 C++ 中，常量是变量的一个属性，我们通过给变量添加修饰符 (Specifier) 来实现。因此在 C++ 中，常量是“不可修改的变量”，只具有读属性。在 C++ 中，你会这样声明一个变量或常量：<br>int varInt = 10; // This is a variable.
const int constInt = 10; // This is a constant.
<br>在 Swift 中，一个值要么是常量要么是变量。变量和常量在语义上的关系是完全平等的，彼此独立，而不是一个是另一个的修饰。在 Swift 中，这样的声明方式更为常见：<br>var varInt = 10 // This is a variable.
let constInt = 10 // This is a constant.
<br>这里缺省了值类型名，这是因为 Swift 支持类型推导，虽然在 C++11 后，你也可以使用 auto 关键字来缺省类型实现类型推导，但远没有 Swift 这类语言来得方便。<br><br>由于 C++ 的常量 与 C++ 常量必须在声明时赋值不同，Swift 允许你在运行时赋值，但这时你得显式的给出常量的类型，而且你需要保证不对未初始化的常量进行访问。如：<br>var environment = "DEBUG"
let runtimeConstant: Int
// runtimeConstant not readable
if environment == "DEBUG" {
	runtimeConstant = 100
} else {
	runtimeConstant = 50
}
// Has been initialized, you can read runtimeConstant
<br>如果你在初始化常量时赋值，Swift 还会在编译时对类型进行检查。比如：<br>let myConstant = 20
myConstant = 10 // Error!!
<br><br>虽然你可以在初始化时缺省类型，但是如果你只是想先声明一个变量，你就需要明确值类型（即 type annotation）。值得注意的是，不同与 C++ 中常见的前置类型，在 Swift 中，无论是值类型还是函数的返回值类型，它们都是后置类型，如：<br><br><br><br><br><br><br><br><br><br><br><br>]]></description><link>https://congzhi.wiki/swift-learning-of-swift/the-basics.html</link><guid isPermaLink="false">Swift Learning of Swift/The Basics.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 18 Jul 2025 09:55:19 GMT</pubDate></item><item><title><![CDATA[Home Sweet Home]]></title><description><![CDATA[ 
 <br><br><br>你好呀！欢迎来到我的“家”，我在网络世界的一隅。这里，我将分享一切我认为有意思的事情和知识。虽然大部分内容围绕计算机科学，但我相信后面会有更多其他的更新！<br>Hi there! Welcome to my home site. This site will be my corner of the internet. I will be sharing everything here, including some computer science notes I have written myself, maybe later some life updates, and so much more...<br>这个域名下的所有的内容都是免费且开源的，任何人都可以随心所欲地使用/复制相关内容，但请尊重作者的知识产权，请满足 <a data-tooltip-position="top" aria-label="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" rel="noopener nofollow" class="external-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" target="_blank">CC BY-NC-SA 4.0</a>，提供作者署名信息、禁止任何商业用途、并对于在域名下内容进行修改后使用相同的方式进行共享。<br>The contents are free and open-sourced, and anyone may use/copy it as long as these conditions are met—crediting the author and providing the source link, with no commercial use.<br><br><br>Copyright © 2025 Congzhi. Permission is granted to copy, distribute, and share this work, provided that the following conditions are met:<br>
<br>Proper attribution to the original author, including the author's name and a link to the original source, must be given.
<br>Commercial use is strictly prohibited without explicit written permission.
<br>If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.
<br>All content in this domain is licensed under <a data-tooltip-position="top" aria-label="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" rel="noopener nofollow" class="external-link" href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank">CC BY-NC-SA 4.0</a>.<br><br><br>目前，所有的系列都罗列在下：<br>Congzhi's CS Note Series

<br><a data-href="Building and Version Control" href="https://congzhi.wiki/building-and-version-control/building-and-version-control.html" class="internal-link" target="_self" rel="noopener nofollow">Building and Version Control</a>
<br><a data-href="Computer Architecture" href="https://congzhi.wiki/computer-architecture/computer-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">Computer Architecture</a> &lt;-- Future Plan
<br><a data-href="Congzhi's OS Series" href="https://congzhi.wiki/congzhi's-os-series/congzhi's-os-series.html" class="internal-link" target="_self" rel="noopener nofollow">Congzhi's OS Series</a>
<br><a data-href="Congzhi's C Plus Plus Series" href="https://congzhi.wiki/congzhi's-c-plus-plus-series/congzhi's-c-plus-plus-series.html" class="internal-link" target="_self" rel="noopener nofollow">Congzhi's C Plus Plus Series</a>
<br><a data-href="CS50 SQL" href="https://congzhi.wiki/cs50-sql/cs50-sql.html" class="internal-link" target="_self" rel="noopener nofollow">CS50 SQL</a>
<br><a data-href="Data Structure and Algorithm" href="https://congzhi.wiki/data-structure-and-algorithm/data-structure-and-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Data Structure and Algorithm</a>
<br><a data-href="Networks" href="https://congzhi.wiki/networks/networks.html" class="internal-link" target="_self" rel="noopener nofollow">Networks</a>
<br><a data-href="Parallel Computing" href="https://congzhi.wiki/Parallel Computing" class="internal-link" target="_self" rel="noopener nofollow">Parallel Computing</a> &lt;-- Future Plan
<br><a data-href="Some Notes" href="https://congzhi.wiki/some-notes/some-notes.html" class="internal-link" target="_self" rel="noopener nofollow">Some Notes</a>
<br><a data-href="Swift Learning of Swift" href="https://congzhi.wiki/swift-learning-of-swift/swift-learning-of-swift.html" class="internal-link" target="_self" rel="noopener nofollow">Swift Learning of Swift</a>

<br>开源项目罗列在下：<br>Congzhi's Projects
<a data-href="NUMA-Aware Thread Pool" href="https://congzhi.wiki/numa-aware-thread-pool/numa-aware-thread-pool.html" class="internal-link" target="_self" rel="noopener nofollow">NUMA-Aware Thread Pool</a>
<br>被本人抛弃的系列如下，请仔细甄别内容，我不对任何错误负责。<br>Abandoned Projects

<br><a data-tooltip-position="top" aria-label="Computer Networking A Top-Down Approach" data-href="Computer Networking A Top-Down Approach" href="https://congzhi.wiki/computer-networking-a-top-down-approach/computer-networking-a-top-down-approach.html" class="internal-link" target="_self" rel="noopener nofollow">Computer Networking</a>

<br><br><br>You are welcomed to contact me at <a data-tooltip-position="top" aria-label="mailto:duzhi_02@qq.com" rel="noopener nofollow" class="external-link" href="https://congzhi.wiki/mailto:duzhi_02@qq.com" target="_blank">duzhi_02@qq.com</a>!]]></description><link>https://congzhi.wiki/home-sweet-home.html</link><guid isPermaLink="false">Home Sweet Home.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Tue, 19 Aug 2025 19:04:53 GMT</pubDate></item><item><title><![CDATA[1. Compilation]]></title><description><![CDATA[ 
 <br><br><br>在本节，我们将讨论 GCC 是如何一步一步地将 C/C++ 语言源代码编译成二进制机器语言的。也就是 C/C++ 代码是如何变成 ELF 的。<br><br>在编译 C/C++ 时，我们常常使用 GCC 作为我们的编译器。GCC 并不是 GNU C Compiler， GCC 是一系列编译器的集合。不仅仅支持 C 语言，还支持 C++ 、 D 、 Objective-C  、 Go 等其他的编程语言。你可以用 gcc -v 来查看你的 GCC 支持哪些语言。<br><br>我们把程序从源代码到可执行文件的过程分为两步——编译和链接。在本节，我们将会学习程序是如何编译的。我们把编译的过程共分为 3 步，分别是预处理、编译和汇编。我们先来从机器的视角观察 C/C++ 源代码是怎么样的。<br><br>这是一个最简单的例子。这是一段源代码，我将其存放到 hello.c 文件中。我们在文件中输入的所有指令都是 human-friendly 的，而计算机是没有办法理解这个文本文件到底写了什么的。<br>#include &lt;stdio.h&gt;
int main(){
	printf("hello, world\n"); // This prints "hello, world\n"
}
<br>这段源代码以文本文件的形式存储在磁盘上，它在计算机眼中就是这样的：<br>#include&lt;stdio.h&gt; int main(){printf("hello, world\n");//This prints "hello, world\n"}

<br>而实际上计算机存储的只有 0 和 1 构成的二进制数。所以在计算机眼中，用户实际上输入了一连串的二进制数。如果将这些 ASCII 字符转换成 16 进制的二进制数，就是这样的：<br>23696e636c7564653c737464696f2e683e20696e74206d61696e28297b7072696e7466282268656c6c6f2c20776f726c645c6e22293b2f2f54686973207072696e7473202268656c6c6f2c20776f726c645c6e22227d
<br><br>我们有源程序后，我们需要“翻译”这段程序，让计算机能够理解我们想要表达的意思，这个过程就是编译。而编译的第一步，就是预处理。<br>我们用下面的 shell 指令对程序进行预处理：<br>gcc -E hello.c -o hello.i
cpp hello.c &gt; hello.i
<br>完成后，编译器就会生成一个 .i 文件，即中间文件(intermediate file)。那么预处理的作用是什么呢？我们把这一步骤叫做预处理，把预处理完成所得到的文件叫中间文件。不难想到，预处理阶段是程序正式进行编译的临门一脚。预处理阶段的作用是处理源文件中以 # 开头的语句。即：<br>
<br>删除 #define 并展开其所定义的宏。
<br>处理所有条件预编译指令，如 #if、#ifdef、#endif等。
<br>插入头文件到#include处，可以递归方式进行处理（复制粘贴）。
<br>删除注释（可选择保留）。
<br>添加行标记和文件名标识。
<br>保留#pragma编译指令（编译用）。
<br>如果你打开 .i 文件，你会发现一些函数的声明、一些系统信息......各种乱七八糟的东西，这就是因为我们将头文件插入（粘贴）到 #include 的地方了。但总体上，中间文件还是可读的。只不过是将文件进行了一些加工。在中间文件中，你仍然会看到：<br>int main(){
	printf("hello, world\n"); // Why this are not causing error?
}
<br>此外，你还会看到 printf 的函数声明，这也解释了为什么你的程序不会报错。<br>extern int printf (const char *__restrict __format, ...);
<br>在 C/C++ 中，我们指预处理完成后的 .i （.ii in C++）文件为一个个的翻译单元。<br><br>我们发现，预处理完成后，我们实际上得到的仍然是高级语言源程序。要将它编程机器可读的二进制程序，编译这一步至关重要。编译过程通过把预处理文件中的高级语言代码进行词法分析、语法分析、语义分析和优化后生成汇编代码文件。狭义上，我们把进行编译处理的程序就叫编译器。<br>我们用如下的命令可将程序编译为可读的汇编代码文件。<br>gcc -S hello.i -o hello.s
gcc -S hello.c -o hello.s
/user/lib/gcc/xxxx-linux-gnu/4.1/cc1 hello.c
<br>尽管汇编代码仍是文本，机器无法理解这些代码，但是汇编代码和二进制机器语言代码实际上是一一对应的。你可以把 C/C++ 这种高级语言理解为在汇编语言上的进一步封装/抽象，编译器就是提供这种抽象的核心工具。<br>一般而言，高级语言都是机器无关的(machine-independent)。但是编译器可以将机器无关的高级语言转换成机器相关的汇编。在 x86 的机器上，编译器会将高级语言源程序编译为 x86 汇编，在 ARM 上，编译器会将高级语言程序编译成 ARM 汇编。<br>硬件--&gt;（封装抽象）ISA--&gt;（封装抽象）汇编语言--&gt;（封装抽象）高级语言程序<br>此外，由于高级语言（标准库函数）提供对操作系统的屏蔽和封装，编译器的运行可能还会依赖操作系统。<br><br>机器汇编语言被称为低级语言，相比高级语言，汇编语言和机器的二进制指令之间的对应关系非常紧密。也就是说，不同机器架构上使用的汇编语言是不一样的。你可以用下面的指令得到可重定位目标文件。也就是 .o 文件，即目标文件。<br>gcc –c hello.s –o hello.o
gcc –c hello.c –o hello.o
as hello.s -o hello.o
<br>生成的目标文件中，除了汇编得到的机器码（01数据）之外，还包含元数据。在链接过程中，这些选数据会供链接器使用以完成可执行文件的生成。这些元数据我们在下一节介绍。<br>至此，广义上的编译就算完成了。]]></description><link>https://congzhi.wiki/building-and-version-control/1.-compilation.html</link><guid isPermaLink="false">Building and Version Control/1. Compilation.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Fri, 11 Apr 2025 01:35:13 GMT</pubDate></item><item><title><![CDATA[2. Linking]]></title><description><![CDATA[ 
 <br><br>.o文件中包含一系列的01数据和元数据（链接器用与重定义和。。。）元数据指明了当前的object file定义了那些符号声明了那些符号<br>这些01数据以一定的格式（数据段/bss/代码段)存储，这些格式也作为元数据存储在.o文件中<br>
.o文件是机器指令，实际上可以放到机器上运行了，但是我们上节课也看到，有的符号定义可能在其他的翻译单元（external reference)，也就是为什么在Linux中，我们把.o文件成为.elf，链接后的文件格式仍然是.elf。但是视角不同了。链接器的职责就是将符号的外部引用替换到。。。（place the placeholder of external reference to an address）<br>one definition rule(ODR): ambitious, which one to call<br>生成可执行文件后：<br>
-ldd prog （查看动态链接库）<br>objdump -g 添加debug信息<br>为什么我不需要链接C的动态链接库来运行程序？我直接用 ./proc？甚至用pthread库我也可以./proc而不是 ./proc -lpthread<br>-L来寻找库路径（默认会在 PATH 中去寻找，PATH 有很多（和环境变量的关系呢？<br>
-I 是干嘛的？如果找不到，那么就在这里找<br>当使用&lt;&gt;时，就会去system path中寻找（include path<br>
echo $PATH]]></description><link>https://congzhi.wiki/building-and-version-control/2.-linking.html</link><guid isPermaLink="false">Building and Version Control/2. Linking.md</guid><dc:creator><![CDATA[Congzhi]]></dc:creator><pubDate>Wed, 26 Mar 2025 20:52:34 GMT</pubDate></item><item><title><![CDATA[Chapter 1: Computer Networks and the Internet]]></title><description/></item></channel></rss>